{"db":[{"meta":{"exported_on":1643646974858,"version":"009"},"data":{"posts":[{"id":2,"uuid":"d65b4afa-9611-4e7b-8852-e62a02d05969","title":"Self-hosted HTTP service in C# with Nancy and TopShelf","slug":"self-hosted-http-service-in-c-with-nancy-and-topshelf","markdown":"\nI found myself in need of a standalone, self-hosted HTTP Service for a REST-backend at work the other day. I like my services to be flexible and easy to deploy with a low footprint. Here's the catch: At work we write in .Net and I truly hate IIS. I kinda like C#, but I don't want my webservices to be tightly locked onto the platform-specific overhead hell that is IIS. Thanks to [OWIN](http://owin.org/), [Nancy](http://nancyfx.org/) and [TopShelf](http://topshelf-project.com/) it easy to write a self-hosted HTTP service (Ruby or Node.js style!) in C# and have it run as a standalone application or as a Windows Service. Here is a super duper easy example using Nancys Self-Host and TopShelf.\n\n\n###The Code\nFirst of all we need som packages from NuGet:\n\n\t\tInstall-Package Nancy.Hosting.Self \n\t\tInstall-Package Topshelf \n\t\tInstall-Package Topshelf.Linux\n\t\t\n\nFirst the NancySelfHost class:\n\n\t\tusing System;\n\t\tusing System.Diagnostics;\n\t\tusing Nancy.Hosting.Self;\n\n\t\tnamespace AwesomeNancySelfHost\n\t\t{\n\t\t\tpublic class NancySelfHost\n\t\t\t{\n\t\t\t\tprivate NancyHost m_nancyHost;\n\n\t\t\t\tpublic void Start()\n\t\t\t\t{\n\t\t\t\t\tm_nancyHost = new NancyHost(new Uri(\"http://localhost:5000\"));\n\t\t\t\t\tm_nancyHost.Start();\n\t\t\t\n\t\t\t\t}\n\n\t\t\t\tpublic void Stop()\n\t\t\t\t{\n\t\t\t\t\tm_nancyHost.Stop();\n\t\t\t\t\tConsole.WriteLine(\"Stopped. Good bye!\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\t\t\n\t\t\nNow we need a Nancy module to describe the route for the webservice. Lets make a simple API-example and return some JSON:\n\n\t\tusing System;\n\t\tusing Nancy;\n\n\t\tnamespace AwesomeNancySelfHost\n\t\t{\n\t\t\tpublic class ExampleNancyModule : NancyModule\n\t\t\t{\n\n\t\t\t\tpublic NancyModule() \n\t\t\t\t{\n\n\t\t\t\t\tGet[\"/v1/feeds\"] = parameters =>\n\t\t\t\t\t{\n\t\t\t\t\t\tvar feeds = new string[] {\"foo\", \"bar\"};\n\t\t\t\t\t\treturn Response.AsJson(feeds);\n\t\t\t\t\t};\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\nFinaly we strap the whole thing together and make a service with TopShelf: \n\n\t\tusing System;\n\t\tusing Topshelf;\n\n\t\tnamespace AwesomeNancySelfHost\n\t\t{\n\t\t\tpublic class Program\n\t\t\t{\n\t\t\t\tpublic static void Main()\n\t\t\t\t{\n\t\t\t\t\tHostFactory.Run(x => \n\t\t\t\t\t{\n\t\t\t\t\t\tx.UseLinuxIfAvailable();\n\t\t\t\t\t\tx.Service<NancySelfHost>(s => \n\t\t\t\t\t\t{\n\t\t\t\t\t\t\ts.ConstructUsing(name => new NancySelfHost()); \n\t\t\t\t\t\t\ts.WhenStarted(tc => tc.Start()); \n\t\t\t\t\t\t\ts.WhenStopped(tc => tc.Stop()); \n\t\t\t\t\t\t});\n\n\t\t\t\t\t\tx.RunAsLocalSystem(); \n\t\t\t\t\t\tx.SetDescription(\"Nancy-SelfHost example\"); \n\t\t\t\t\t\tx.SetDisplayName(\"Nancy-SelfHost Service\"); \n\t\t\t\t\t\tx.SetServiceName(\"Nancy-SelfHost\"); \n\t\t\t\t\t}); \n\t\t\t\t}\n\t\t\t}\n\t\t}\n\nThats all the code we need! If we know run the application, a console window will show the following:\n\n\t\tConfiguration Result:\n\t\t[Success] Name Nancy-SelfHost\n\t\t[Success] DisplayName Nancy-SelfHost Service\n\t\t[Success] Description Nancy-SelfHost example\n\t\t[Success] ServiceName Nancy-SelfHost\n\t\tTopshelf v3.1.135.0, .NET Framework v4.0.30319.17020\n\t    The Nancy-SelfHost service is now running, press Control+C to exit.\n\t\t\nNavigate to ``http://localhost:5000/v1/feeds`` with you're favorite browser and get yourself some JSON! \n\n### Install the Windows Service \n\nRunning the console application is no use for us if we want this example API to run on a Windows Server as a Windows Service. To make it so, hit up a CMD (or Powershell) window as administrator. Navigate to the projects bin/debug folder and type \n\t\t\n\t\tAwesomeNancySelfHost.exe install\n\t\tAwesomeNancySelfHost.exe start\nIf you know check the ``services``snap-in (``run => mmc``) you will see the AwesomeNancySelfHost in the list of Windows Services. Again, check ``http://localhost:5000/v1/feeds`` and check out the foobar JSON!\n\n### Bonus Round: Linux hosting\nDid you notice the ``Install-Package Topshelf.Linux`` NuGet-package and the ``x.UseLinuxIfAvailable();``statement in TopShelfs Main method? It is true, both Nancy and TopShelf runs natively in Mono, so our tiny webservice will run under Linux too, making it not only standalone but cross-platform too. If we pack the whole thing together with my [Docker-image for Mono](https://github.com/andmos/Docker-Mono) deployment get realy easy and the service can scale horizontally with ease.\n\n### Wrapping it up\n\nThanks to [OWIN](http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api) and [ASP.NET V-Next](http://www.asp.net/vnext) Microsoft has got its game up on the web front. We now see a decoupling between service and server, which I think is a good thing. Specific platforms and servers should not matter when writing web-services, and it seems like Microsoft finally has realized that fact. With projects like Nancy and TopShelf I no longer fear writing web APIs in .Net, making C# a language that can be used on the entire application stack, as well as cross-platform.\n","mobiledoc":null,"html":"<p>I found myself in need of a standalone, self-hosted HTTP Service for a REST-backend at work the other day. I like my services to be flexible and easy to deploy with a low footprint. Here's the catch: At work we write in .Net and I truly hate IIS. I kinda like C#, but I don't want my webservices to be tightly locked onto the platform-specific overhead hell that is IIS. Thanks to <a href=\"http://owin.org/\">OWIN</a>, <a href=\"http://nancyfx.org/\">Nancy</a> and <a href=\"http://topshelf-project.com/\">TopShelf</a> it easy to write a self-hosted HTTP service (Ruby or Node.js style!) in C# and have it run as a standalone application or as a Windows Service. Here is a super duper easy example using Nancys Self-Host and TopShelf.</p>\n\n<h3 id=\"thecode\">The Code</h3>\n\n<p>First of all we need som packages from NuGet:</p>\n\n<pre><code>    Install-Package Nancy.Hosting.Self \n    Install-Package Topshelf \n    Install-Package Topshelf.Linux\n</code></pre>\n\n<p>First the NancySelfHost class:</p>\n\n<pre><code>    using System;\n    using System.Diagnostics;\n    using Nancy.Hosting.Self;\n\n    namespace AwesomeNancySelfHost\n    {\n        public class NancySelfHost\n        {\n            private NancyHost m_nancyHost;\n\n            public void Start()\n            {\n                m_nancyHost = new NancyHost(new Uri(\"http://localhost:5000\"));\n                m_nancyHost.Start();\n\n            }\n\n            public void Stop()\n            {\n                m_nancyHost.Stop();\n                Console.WriteLine(\"Stopped. Good bye!\");\n            }\n        }\n    }       \n</code></pre>\n\n<p>Now we need a Nancy module to describe the route for the webservice. Lets make a simple API-example and return some JSON:</p>\n\n<pre><code>    using System;\n    using Nancy;\n\n    namespace AwesomeNancySelfHost\n    {\n        public class ExampleNancyModule : NancyModule\n        {\n\n            public NancyModule() \n            {\n\n                Get[\"/v1/feeds\"] = parameters =&gt;\n                {\n                    var feeds = new string[] {\"foo\", \"bar\"};\n                    return Response.AsJson(feeds);\n                };\n            }\n        }\n    }\n</code></pre>\n\n<p>Finaly we strap the whole thing together and make a service with TopShelf: </p>\n\n<pre><code>    using System;\n    using Topshelf;\n\n    namespace AwesomeNancySelfHost\n    {\n        public class Program\n        {\n            public static void Main()\n            {\n                HostFactory.Run(x =&gt; \n                {\n                    x.UseLinuxIfAvailable();\n                    x.Service&lt;NancySelfHost&gt;(s =&gt; \n                    {\n                        s.ConstructUsing(name =&gt; new NancySelfHost()); \n                        s.WhenStarted(tc =&gt; tc.Start()); \n                        s.WhenStopped(tc =&gt; tc.Stop()); \n                    });\n\n                    x.RunAsLocalSystem(); \n                    x.SetDescription(\"Nancy-SelfHost example\"); \n                    x.SetDisplayName(\"Nancy-SelfHost Service\"); \n                    x.SetServiceName(\"Nancy-SelfHost\"); \n                }); \n            }\n        }\n    }\n</code></pre>\n\n<p>Thats all the code we need! If we know run the application, a console window will show the following:</p>\n\n<pre><code>    Configuration Result:\n    [Success] Name Nancy-SelfHost\n    [Success] DisplayName Nancy-SelfHost Service\n    [Success] Description Nancy-SelfHost example\n    [Success] ServiceName Nancy-SelfHost\n    Topshelf v3.1.135.0, .NET Framework v4.0.30319.17020\n    The Nancy-SelfHost service is now running, press Control+C to exit.\n</code></pre>\n\n<p>Navigate to <code>http://localhost:5000/v1/feeds</code> with you're favorite browser and get yourself some JSON! </p>\n\n<h3 id=\"installthewindowsservice\">Install the Windows Service</h3>\n\n<p>Running the console application is no use for us if we want this example API to run on a Windows Server as a Windows Service. To make it so, hit up a CMD (or Powershell) window as administrator. Navigate to the projects bin/debug folder and type </p>\n\n<pre><code>    AwesomeNancySelfHost.exe install\n    AwesomeNancySelfHost.exe start\n</code></pre>\n\n<p>If you know check the <code>services</code>snap-in (<code>run =&gt; mmc</code>) you will see the AwesomeNancySelfHost in the list of Windows Services. Again, check <code>http://localhost:5000/v1/feeds</code> and check out the foobar JSON!</p>\n\n<h3 id=\"bonusroundlinuxhosting\">Bonus Round: Linux hosting</h3>\n\n<p>Did you notice the <code>Install-Package Topshelf.Linux</code> NuGet-package and the <code>x.UseLinuxIfAvailable();</code>statement in TopShelfs Main method? It is true, both Nancy and TopShelf runs natively in Mono, so our tiny webservice will run under Linux too, making it not only standalone but cross-platform too. If we pack the whole thing together with my <a href=\"https://github.com/andmos/Docker-Mono\">Docker-image for Mono</a> deployment get realy easy and the service can scale horizontally with ease.</p>\n\n<h3 id=\"wrappingitup\">Wrapping it up</h3>\n\n<p>Thanks to <a href=\"http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api\">OWIN</a> and <a href=\"http://www.asp.net/vnext\">ASP.NET V-Next</a> Microsoft has got its game up on the web front. We now see a decoupling between service and server, which I think is a good thing. Specific platforms and servers should not matter when writing web-services, and it seems like Microsoft finally has realized that fact. With projects like Nancy and TopShelf I no longer fear writing web APIs in .Net, making C# a language that can be used on the entire application stack, as well as cross-platform.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-03-03 20:20:30","created_by":1,"updated_at":"2015-03-03 20:21:09","updated_by":1,"published_at":"2015-03-03 20:21:09","published_by":1},{"id":3,"uuid":"9a107ef3-b781-4e11-a175-a15e1b3417ca","title":"Build, test and deploy .NET apps with Vagrant and Docker","slug":"build-test-and-deploy-net-apps-with-vagrant-and-docker","markdown":"On a resent project at work we build a cross-platform chat-application with [Xamarin](www.xamarin.com) and [SignalR](http://signalr.net/).\nThe SignalR-hub was to find it's home on a Linux-server, given ASP.NETs new found love for other platforms than Windows and IIS.\nWe had limited time and the back-end guys were developing the hub in Visual Studio. To help them make sure the code they wrote would be Mono-compatible (and easy to deploy for testing), I turned to my two favorite pieces of open source technology: [Vagrant](https://www.vagrantup.com/) and [Docker](https://www.docker.com/).\n\n###The code\nFirst, I wrote a simple Dockerfile based on the latest Mono-baseimage that adds the code and runs xbuild in the build-process. When the container is run without parameters, it deploys the server.\n\n    FROM mono:latest\n    ADD SignalRServer SignalRServer\n    RUN xbuild /SignalRServer/SignalRServer.sln\n\n    EXPOSE 8080\n\n    CMD mono /SignalRServer/SignalRServer.LocalWebServer/bin/Debug/SignalRServer.LocalWebServer.exe\n\nTo automate this process, a simple script...\n\n    # /bin/bash\n    sudo docker build -t dirc/signalrhub /vagrant/\n    sudo docker run -p 8080:8080 -td dirc/signalrhub\n\n ...before we finally spin up a VM via this Vagrantfile and `vagrant up`.\n\n    # -*- mode: ruby -*-\n    # vi: set ft=ruby :\n    Vagrant::Config.run do |config|\n\n      config.vm.box = \"virtualUbuntu64\"\n      config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n\n      config.vm.provision :shell, :inline => \"sudo apt-get update\"\n      config.vm.provision :shell, :inline => \"sudo apt-get install curl -y\"\n      config.vm.provision :shell, :inline => \"curl -s https://get.docker.io/ubuntu/ | sudo sh > /dev/null 2>&1\"\n      config.vm.provision :shell, :inline => \"/vagrant/buildAndDeploySignalRHub\"\n      config.vm.forward_port 8080, 8080\n\n      end\n\n      Vagrant.configure(\"2\") do |config|\n      config.vm.provider :virtualbox do |virtualbox|\n      virtualbox.customize [\"modifyvm\", :id, \"--memory\", \"1024\"]\n      end\n      end\n\nThats it! With a simple `vagrant up` we get an Ubuntu VM, Docker installed, the code compiled and the hub deployed, and is available at `http://localhost:8080`.\nWhen the Hub is finished the Docker-container can easily be moved to the production server. If the Windows-guys uses code that breaks the Mono-compability, it is easily discovered.\n","mobiledoc":null,"html":"<p>On a resent project at work we build a cross-platform chat-application with <a href=\"www.xamarin.com\">Xamarin</a> and <a href=\"http://signalr.net/\">SignalR</a>. <br />\nThe SignalR-hub was to find it's home on a Linux-server, given ASP.NETs new found love for other platforms than Windows and IIS. <br />\nWe had limited time and the back-end guys were developing the hub in Visual Studio. To help them make sure the code they wrote would be Mono-compatible (and easy to deploy for testing), I turned to my two favorite pieces of open source technology: <a href=\"https://www.vagrantup.com/\">Vagrant</a> and <a href=\"https://www.docker.com/\">Docker</a>.</p>\n\n<h3 id=\"thecode\">The code</h3>\n\n<p>First, I wrote a simple Dockerfile based on the latest Mono-baseimage that adds the code and runs xbuild in the build-process. When the container is run without parameters, it deploys the server.</p>\n\n<pre><code>FROM mono:latest\nADD SignalRServer SignalRServer\nRUN xbuild /SignalRServer/SignalRServer.sln\n\nEXPOSE 8080\n\nCMD mono /SignalRServer/SignalRServer.LocalWebServer/bin/Debug/SignalRServer.LocalWebServer.exe\n</code></pre>\n\n<p>To automate this process, a simple script...</p>\n\n<pre><code># /bin/bash\nsudo docker build -t dirc/signalrhub /vagrant/\nsudo docker run -p 8080:8080 -td dirc/signalrhub\n</code></pre>\n\n<p>...before we finally spin up a VM via this Vagrantfile and <code>vagrant up</code>.</p>\n\n<pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\nVagrant::Config.run do |config|\n\n  config.vm.box = \"virtualUbuntu64\"\n  config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n\n  config.vm.provision :shell, :inline =&gt; \"sudo apt-get update\"\n  config.vm.provision :shell, :inline =&gt; \"sudo apt-get install curl -y\"\n  config.vm.provision :shell, :inline =&gt; \"curl -s https://get.docker.io/ubuntu/ | sudo sh &gt; /dev/null 2&gt;&amp;1\"\n  config.vm.provision :shell, :inline =&gt; \"/vagrant/buildAndDeploySignalRHub\"\n  config.vm.forward_port 8080, 8080\n\n  end\n\n  Vagrant.configure(\"2\") do |config|\n  config.vm.provider :virtualbox do |virtualbox|\n  virtualbox.customize [\"modifyvm\", :id, \"--memory\", \"1024\"]\n  end\n  end\n</code></pre>\n\n<p>Thats it! With a simple <code>vagrant up</code> we get an Ubuntu VM, Docker installed, the code compiled and the hub deployed, and is available at <code>http://localhost:8080</code>. <br />\nWhen the Hub is finished the Docker-container can easily be moved to the production server. If the Windows-guys uses code that breaks the Mono-compability, it is easily discovered.</p>","amp":null,"image":null,"featured":1,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"Build, test and deploy .NET apps with Vagrant and Docker","meta_description":null,"author_id":1,"created_at":"2015-03-03 20:26:09","created_by":1,"updated_at":"2015-03-04 17:55:38","updated_by":1,"published_at":"2015-03-03 20:26:57","published_by":1},{"id":4,"uuid":"636eb571-70c2-4ada-9b9a-c5258d8a257d","title":"How I Read","slug":"how-i-read","markdown":"\nDuring my last year of University College, I rediscovered my childhood joy of reading. At the time I was working on my bachelor’s degree (an app for the [company I now work for](www.dips.no)) in a structured way from morning to afternoon, so my evenings were open for the first time in years. At one point, I had watched all seasons of The Wire, all seasons of House MD and Breaking Bad had come to its conclusion. Out of pure boredom, I decided, \"hey, might as well do some research on that bachelor thesis.\" So I went on Amazon and ordered Donald Norman's [The Design of Everyday Things](http://www.amazon.com/The-Design-Everyday-Things-Expanded/dp/0465050654/ref=tmm_pap_title_0?ie=UTF8&qid=1426446749&sr=1-1). \nWhen I got it the book I read it from cover to cover in 4 days. It was THAT good. I ordered Norman's [Living with Complexity](http://www.amazon.com/Living-Complexity-Donald-A-Norman/dp/0262014866/ref=tmm_hrd_title_0?ie=UTF8&qid=1426447113&sr=1-1) next, before Robert C. Martin's [Clean Code](http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_1?s=books&ie=UTF8&qid=1426448269&sr=1-1&keywords=clean+code).\n\nThe list goes on, and now after 15 months, I have read about 60 books, both fictional and non-fictional. How do I get through so many books? A lot has to do with organization.\n###Tips and tricks for reading lots of books\nA while ago I read an excellent [article](http://lifehacker.com/my-secret-to-reading-a-lot-of-books-514189426) on [Lifehacker](http://lifehacker.com) where a guy used [Trello](https://trello.com) (a free service for organizing Kanban-boards) to keep track of his reading lists. I tried it out and found it really useful. In my board I have 4 categories: 'Backlog', 'To Read', 'Reading' and 'Done'. If I stumble upon a book I want to read, I put it in the Backlog. If the 'To Read' list has less than 7 books in it or I want to read it in the near future, I put it in that category. Each book is also color coded, with blue for fiction and green for non-fiction and novels. As a rule of thumb, I try to read at least one non-fictional book each month to keep my brain up to speed and actually learn something.\n\n![](http://i.imgur.com/ycM2jrX.png)\n\nThe Kanban board also helps with motivation: Nothing is as good as dragging a book from 'Reading' to 'Done'.\n\nTo remember as much as possible I always keep a notebook handy when reading non-fiction. Before each reading session, I read the notes I wrote last time I sat down with the book to refresh. This technique has done wonders for my memory. Thanks to this habit, a notebook is now always with me, and it have saves more than one idea from oblivion. For reading academia and non-fiction, I try to set aside 30 minutes each day. A great place to get some reading done is in airports and on planes. Let’s face it, you just sit there and wait to get to where you are going anyway.\n\nFor reading two books simultaneously, audiobooks has revolutionized my life. If I want to read fiction (I find it too hard to concentrate while listening to non-fiction), I always check [Audible](http://www.audible.com/) first.\n\nBy just listening to a audiobook to and from work, I get 40 minutes of book each day. Out for a jog? 30 minutes. Doing laundry? 15 minutes. All this time adds up, so going through a couple of books a month is easy. It is also easier for my brain to distinguishing between two books when one is read on paper and the other is listened to.\n\nFor e-books I use my Kindle and the Kindle-app is installed everywear. I use the excelente application [Calibre](http://calibre-ebook.com/) to mange it all. This library is stored on my Dropbox.\n","mobiledoc":null,"html":"<p>During my last year of University College, I rediscovered my childhood joy of reading. At the time I was working on my bachelor’s degree (an app for the <a href=\"www.dips.no\">company I now work for</a>) in a structured way from morning to afternoon, so my evenings were open for the first time in years. At one point, I had watched all seasons of The Wire, all seasons of House MD and Breaking Bad had come to its conclusion. Out of pure boredom, I decided, \"hey, might as well do some research on that bachelor thesis.\" So I went on Amazon and ordered Donald Norman's <a href=\"http://www.amazon.com/The-Design-Everyday-Things-Expanded/dp/0465050654/ref=tmm_pap_title_0?ie=UTF8&amp;qid=1426446749&amp;sr=1-1\">The Design of Everyday Things</a>. <br />\nWhen I got it the book I read it from cover to cover in 4 days. It was THAT good. I ordered Norman's <a href=\"http://www.amazon.com/Living-Complexity-Donald-A-Norman/dp/0262014866/ref=tmm_hrd_title_0?ie=UTF8&amp;qid=1426447113&amp;sr=1-1\">Living with Complexity</a> next, before Robert C. Martin's <a href=\"http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1426448269&amp;sr=1-1&amp;keywords=clean+code\">Clean Code</a>.</p>\n\n<p>The list goes on, and now after 15 months, I have read about 60 books, both fictional and non-fictional. How do I get through so many books? A lot has to do with organization.  </p>\n\n<h3 id=\"tipsandtricksforreadinglotsofbooks\">Tips and tricks for reading lots of books</h3>\n\n<p>A while ago I read an excellent <a href=\"http://lifehacker.com/my-secret-to-reading-a-lot-of-books-514189426\">article</a> on <a href=\"http://lifehacker.com\">Lifehacker</a> where a guy used <a href=\"https://trello.com\">Trello</a> (a free service for organizing Kanban-boards) to keep track of his reading lists. I tried it out and found it really useful. In my board I have 4 categories: 'Backlog', 'To Read', 'Reading' and 'Done'. If I stumble upon a book I want to read, I put it in the Backlog. If the 'To Read' list has less than 7 books in it or I want to read it in the near future, I put it in that category. Each book is also color coded, with blue for fiction and green for non-fiction and novels. As a rule of thumb, I try to read at least one non-fictional book each month to keep my brain up to speed and actually learn something.</p>\n\n<p><img src=\"http://i.imgur.com/ycM2jrX.png\" alt=\"\" /></p>\n\n<p>The Kanban board also helps with motivation: Nothing is as good as dragging a book from 'Reading' to 'Done'.</p>\n\n<p>To remember as much as possible I always keep a notebook handy when reading non-fiction. Before each reading session, I read the notes I wrote last time I sat down with the book to refresh. This technique has done wonders for my memory. Thanks to this habit, a notebook is now always with me, and it have saves more than one idea from oblivion. For reading academia and non-fiction, I try to set aside 30 minutes each day. A great place to get some reading done is in airports and on planes. Let’s face it, you just sit there and wait to get to where you are going anyway.</p>\n\n<p>For reading two books simultaneously, audiobooks has revolutionized my life. If I want to read fiction (I find it too hard to concentrate while listening to non-fiction), I always check <a href=\"http://www.audible.com/\">Audible</a> first.</p>\n\n<p>By just listening to a audiobook to and from work, I get 40 minutes of book each day. Out for a jog? 30 minutes. Doing laundry? 15 minutes. All this time adds up, so going through a couple of books a month is easy. It is also easier for my brain to distinguishing between two books when one is read on paper and the other is listened to.</p>\n\n<p>For e-books I use my Kindle and the Kindle-app is installed everywear. I use the excelente application <a href=\"http://calibre-ebook.com/\">Calibre</a> to mange it all. This library is stored on my Dropbox.</p>","amp":null,"image":"/content/images/2015/03/2015-03-16-21-41-47-1.jpg","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-03-16 20:32:33","created_by":1,"updated_at":"2015-03-22 21:18:06","updated_by":1,"published_at":"2015-03-16 20:52:47","published_by":1},{"id":5,"uuid":"b53e93f1-1278-444c-81c2-4cbafe5807e7","title":"Zookeeper-aware application server","slug":"zookeeper-aware-application-server","markdown":"\n###The situation\nAt work we use [Apache Solr](http://lucene.apache.org/solr/) from our application server to index and offer search in documents. A LOT of documents. The largest customer of our system has as much as 130 million documents, and the amount keeps growing exponentially. With this in mind, we knew early that we had to shard up and cluster the Solr-collection, this meant using [SolrCloud](https://wiki.apache.org/solr/SolrCloud). SolrCloud uses [Apache ZooKeeper](https://zookeeper.apache.org/) to keep track of config files, live nodes, coordination etc. Zookeeper is a distributed file system that supports putting watchers on its files, thus calling back to the application when a file changes.\n\nAs a abstraction layer we use [SolrNet](https://github.com/mausch/SolrNet) to talk to Solr. The only problem here is that SolrNet has no support for SolrCloud ([yet!](https://github.com/mausch/SolrNet/pull/187)), which means that there is no good way to talk to the cluster, oure system's application server must connect to a single, specified node. If this node goes down, we lose the connection to the cluster. So we have redundancy in SolrCloud, but the consumer can't use it? We can't live with that.\n\n###The solution\nWe decided to write a ZooKeeper-API ourself, letting the application server receive a suitable Solr-node on startup. If this node goes down in production, a watcher on the clusterstate-file kicks inn and changes the active Solr-node, giving us failover without restart.\n\nThere are some ways to talk to Zookeeper from .NET, mainly the [Apache ZooKeeper .NET Client](https://www.nuget.org/packages/ZooKeeper.Net/) package on NuGet. In this case, I decided to exploit the opportunity to test out [IKVM](http://www.ikvm.net/), a (magical) project that let's you convert Java JAR-files to DLLs. Yeah.\n\nHere is some code from the API:\n\n\n    using System;\n    using System.Diagnostics.CodeAnalysis;\n    using Common.Logging;\n    using org.apache.zookeeper;\n    using org.apache.zookeeper.data;\n\n    public class ZookeeperNodeDataChangeWatcher : Watcher, org.apache.zookeeper.AsyncCallback.DataCallback,\n        org.apache.zookeeper.AsyncCallback.StatCallback, IDisposable\n    {\n        private static readonly ILog s_log = LogManager.GetLogger(typeof(ZookeeperNodeDataChangeWatcher));\n\n        private const int SessionTimeout = 2000;\n\n        private static string s_nodePath;\n\n        private readonly ZooKeeper m_zookeeper;\n\n        private readonly IObserver<Maybe<byte[]>> m_observer;\n\n        private object m_nodeExists;\n        private bool IsDisposed { get; set; }\n\n        public ZookeeperNodeDataChangeWatcher(string zookeeperName, IObserver<Maybe<byte[]>> observer, string nodePath)\n        {\n            this.m_observer = observer;\n            this.IsDisposed = false;\n            s_nodePath = nodePath;\n            this.m_zookeeper = new ZooKeeper(zookeeperName, SessionTimeout, this);\n\n            this.GetData();\n\n            s_log.Info(string.Format(\"Zookeeperwatcher: started watcher: {0},{1}\", this.m_zookeeper, s_nodePath));\n        }\n\n        private void GetData()\n        {\n            try\n            {\n                this.m_zookeeper.getData(s_nodePath, this, this, null);\n\n            }\n            catch (Exception e)\n            {\n                this.m_observer.OnError(e);\n                s_log.Info(string.Format(\"Error in getData: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n            }\n        }\n\n        private void WatchAgain()\n        {\n            try\n            {\n                this.m_zookeeper.exists(s_nodePath, this, this, null);\n                s_log.Debug(string.Format(\"Rewatching: {0},{1}\", this.m_zookeeper, s_nodePath));\n            }\n            catch (Exception e)\n            {\n                this.m_observer.OnError(e);\n                s_log.Error(string.Format(\"Error in rewatching: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n            }\n        }\n\n        public void processResult(int i, string str, object obj, byte[] barr, Stat s)\n        {\n            var returnCode = i;\n            var data = barr;\n\n            if (this.IsDisposed)\n            {\n                return;\n            }\n\n            if (returnCode == KeeperException.Code.OK.intValue())\n            {\n                this.m_nodeExists = true;\n                this.m_observer.OnNext(Maybe.Return(data));\n                this.WatchAgain();\n            }\n            else if (returnCode == KeeperException.Code.NONODE.intValue())\n            {\n                this.m_nodeExists = false;\n                this.WatchAgain();\n                this.m_observer.OnNext(Maybe.Empty<byte[]>());\n            }\n            else\n            {\n                this.m_observer.OnError(\n                    KeeperException.create(KeeperException.Code.get(i)));\n            }\n        }\n\n        public void processResult(int i, string str, object obj, Stat s)\n        {\n            if (this.IsDisposed)\n            {\n                return;\n            }\n\n            var returnCode = i;\n            if (returnCode == KeeperException.Code.OK.intValue() ||\n                returnCode == KeeperException.Code.NONODE.intValue() ||\n                returnCode == KeeperException.Code.NODEEXISTS.intValue())\n            {\n                \n                var nodeExistsNow = s != null;\n                var oldExists = (bool?)this.m_nodeExists;\n                this.m_nodeExists = nodeExistsNow;\n                if (nodeExistsNow && nodeExistsNow != oldExists)\n                {\n                    this.GetData();\n                }\n            }\n            else\n            {\n               this.m_observer.OnError(KeeperException.create(\n                    KeeperException.Code.get(returnCode)));\n            }\n        }\n\n        public void process(WatchedEvent e)\n        {\n            if (this.IsDisposed)\n            {\n                return;\n            }\n\n            if (s_nodePath != null && (e == null || s_nodePath != e.getPath()))\n            {\n                this.WatchAgain();\n                return;\n            }\n\n            var type = e.getType();\n            if (type == Watcher.Event.EventType.NodeCreated ||\n                type == Watcher.Event.EventType.NodeDataChanged)\n            {\n                this.GetData();\n            }\n            else if (type == Watcher.Event.EventType.NodeDeleted)\n            {\n                this.m_nodeExists = false;\n                this.m_observer.OnNext(Maybe.Empty<byte[]>());\n                this.WatchAgain();\n            }\n            else\n            {\n                this.WatchAgain();\n            }\n        }\n\n        public void Dispose()\n        {\n            s_log.Debug(string.Format(\"Watcher is disposed: {0},{1}\", this.m_zookeeper, s_nodePath));\n            GC.SuppressFinalize(this);\n        }\n    }\n\nThe implementation of the Clusterstate-watcher:\n\n\n    using System;\n    using System.Collections.Generic;\n    using System.Linq;\n    using System.Reactive.Linq;\n    using System.Text;\n    using Common.Logging;\n    using DIPS.Zookeeper.Model;\n\n    using Newtonsoft.Json;\n    using Newtonsoft.Json.Linq;\n\n    public class ZooKeeperClusterstateWatcher\n    {\n        public event ActiveSolrCollectionChangedDelegate ActiveSolrCollectionChanged;\n\n        private static readonly ILog s_log = LogManager.GetLogger(typeof(ZooKeeperClusterstateWatcher));\n\n        private readonly string m_solrCollectionName;\n\n        private readonly string m_zookeeperConnectionString;\n\n        private const string ClusterState = \"/clusterstate.json\";\n\n        public ZooKeeperClusterstateWatcher(string zookeeperConnectionString, string collectionName)\n        {\n            this.m_solrCollectionName = collectionName;\n            m_zookeeperConnectionString = zookeeperConnectionString;\n            Subscriber();\n        }\n\n        public IObservable<Maybe<byte[]>> WatchData(string zookeeper, string nodePath)\n        {\n            var ob = Observable.Create<Maybe<byte[]>>(\n                    observer =>\n                        new ZookeeperNodeDataChangeWatcher(zookeeper, observer, nodePath));\n            return ob;\n        }\n\n        public void UpdateActiveSolrCollection(SolrCollection collection)\n        {\n            ActiveSolrCollection.SolrCollection = collection;\n            if (ActiveSolrCollectionChanged != null)\n            {\n                ActiveSolrCollectionChanged(this, new ActiveSolrCollectionChangedDelegateArgs() { ActiveCollection = collection });\n            }\n        }\n\n        public void HandleException(Exception ex)\n        {\n            s_log.Error(string.Format(\"Error in clusterstate.json watcher : {0}\", ex));\n\n            if (ex is org.apache.zookeeper.KeeperException.ConnectionLossException)\n            {\n                Subscriber();\n            }\n        }\n\n        public SolrCollection ParseClusterstateJson(string content)\n        {\n            try\n            {\n                var clusterstate = JsonConvert.DeserializeObject<dynamic>(content);\n                var collection = new SolrCollection { Name = this.m_solrCollectionName };\n                var varShards = new List<Shard>();\n\n                foreach (var solrCollection in clusterstate[collection.Name])\n                {\n                    collection.MaxShardsPerNode = int.Parse(clusterstate[collection.Name][\"maxShardsPerNode\"].ToString());\n                    collection.ReplicationFactor = int.Parse(clusterstate[collection.Name][\"replicationFactor\"].ToString());\n                    collection.AutoAddReplicas = bool.Parse(clusterstate[collection.Name][\"autoAddReplicas\"].ToString());\n                    collection.Router = new Router { Name = clusterstate[collection.Name][\"router\"][\"name\"].ToString() };\n                    foreach (JObject shards in solrCollection.Children<JObject>())\n                    {\n                        foreach (var shard in shards.Properties().Where(prop => !prop.Name.Equals(\"name\")))\n                        {\n                            var varShard = new Shard();\n\n                            if (clusterstate[collection.Name][\"shards\"][shard.Name][\"state\"].ToString().Equals(\"active\"))\n                            {\n                                varShard.IsActive = true;\n                            }\n\n                            varShard.Name = shard.Name;\n                            varShard.Range = clusterstate[collection.Name][\"shards\"][shard.Name][\"range\"].ToString();\n                            var coreNodes = new List<CoreNode>();\n\n                            foreach (var replicas in clusterstate[collection.Name][\"shards\"][shard.Name][\"replicas\"])\n                            {\n                                var coreNode = new CoreNode();\n                                foreach (JObject replicass in replicas.Children<JObject>())\n                                {\n                                    foreach (var coreNodeInReplica in replicass.Properties())\n                                    {\n                                        switch (coreNodeInReplica.Name)\n                                        {\n                                            case \"state\":\n                                                coreNode.State = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"node_name\":\n                                                coreNode.NodeName = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"core\":\n                                                coreNode.CoreName = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"base_url\":\n                                                coreNode.BaseUrl = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"leader\":\n                                                coreNode.IsLeader = Convert.ToBoolean(coreNodeInReplica.Value.ToString());\n                                                break;\n                                        }\n                                    }\n\n                                    coreNodes.Add(coreNode);\n                                }\n\n                                varShard.ReplicaCores = coreNodes;\n                            }\n\n                            varShards.Add(varShard);\n                        }\n                    }\n                }\n\n                collection.Shards = varShards;\n                s_log.Debug(string.Format(\"Registered new Solr-collection: {0}. Number of shards is {1}\", collection.Name, collection.Shards.Count()));\n                return collection;\n            }\n            catch (Exception e)\n            {\n                s_log.Error(string.Format(\"Zookeeperwatcher: error in clusterstate.json parser : {0}\", e.Message));\n            }\n\n            return new SolrCollection();\n        }\n\n        private void Subscriber()\n        {\n            var observableClusterState = this.WatchData(m_zookeeperConnectionString, ClusterState);\n            var subscription =\n                observableClusterState.Subscribe(\n                    x => this.UpdateActiveSolrCollection(this.ParseClusterstateJson(Encoding.UTF8.GetString(x.Value))),\n                    this.HandleException);\n        }\n    }\n\n    public delegate void ActiveSolrCollectionChangedDelegate(object sender, ActiveSolrCollectionChangedDelegateArgs args);\n\n\nFrom code we simply read out ``ActiveSolrCollection.SolrCollection.LeaderNode``.\n\n>NOTE: In retrospect, we ended up rewriting and using the [Apache ZooKeeper .NET Client](https://www.nuget.org/packages/ZooKeeper.Net/) package instead, simply because IKVM makes much mess, giving us dependencies to a lot of DLLs and so on.\n\n### Some slides\nI ended up giving a short talk about the work as well as presenting extension that can be made. In the future I will look at ZooKeeper as a place to place distributed config for services, as well as service discovery.\n\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/46646103\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/mostii/zookeeper-aware\" title=\"Zookeeper-aware application server\" target=\"_blank\">Zookeeper-aware application server</a> </strong> from <strong><a href=\"//www.slideshare.net/mostii\" target=\"_blank\">Andreas Mosti</a></strong> </div>\n\nThis work is inspired by the work of the guys over at [Palladium Consulting](http://www.palladiumconsulting.com/2014/06/using-zookeeper-from-net-ikvm-task/).\n","mobiledoc":null,"html":"<h3 id=\"thesituation\">The situation</h3>\n\n<p>At work we use <a href=\"http://lucene.apache.org/solr/\">Apache Solr</a> from our application server to index and offer search in documents. A LOT of documents. The largest customer of our system has as much as 130 million documents, and the amount keeps growing exponentially. With this in mind, we knew early that we had to shard up and cluster the Solr-collection, this meant using <a href=\"https://wiki.apache.org/solr/SolrCloud\">SolrCloud</a>. SolrCloud uses <a href=\"https://zookeeper.apache.org/\">Apache ZooKeeper</a> to keep track of config files, live nodes, coordination etc. Zookeeper is a distributed file system that supports putting watchers on its files, thus calling back to the application when a file changes.</p>\n\n<p>As a abstraction layer we use <a href=\"https://github.com/mausch/SolrNet\">SolrNet</a> to talk to Solr. The only problem here is that SolrNet has no support for SolrCloud (<a href=\"https://github.com/mausch/SolrNet/pull/187\">yet!</a>), which means that there is no good way to talk to the cluster, oure system's application server must connect to a single, specified node. If this node goes down, we lose the connection to the cluster. So we have redundancy in SolrCloud, but the consumer can't use it? We can't live with that.</p>\n\n<h3 id=\"thesolution\">The solution</h3>\n\n<p>We decided to write a ZooKeeper-API ourself, letting the application server receive a suitable Solr-node on startup. If this node goes down in production, a watcher on the clusterstate-file kicks inn and changes the active Solr-node, giving us failover without restart.</p>\n\n<p>There are some ways to talk to Zookeeper from .NET, mainly the <a href=\"https://www.nuget.org/packages/ZooKeeper.Net/\">Apache ZooKeeper .NET Client</a> package on NuGet. In this case, I decided to exploit the opportunity to test out <a href=\"http://www.ikvm.net/\">IKVM</a>, a (magical) project that let's you convert Java JAR-files to DLLs. Yeah.</p>\n\n<p>Here is some code from the API:</p>\n\n<pre><code>using System;\nusing System.Diagnostics.CodeAnalysis;\nusing Common.Logging;\nusing org.apache.zookeeper;\nusing org.apache.zookeeper.data;\n\npublic class ZookeeperNodeDataChangeWatcher : Watcher, org.apache.zookeeper.AsyncCallback.DataCallback,\n    org.apache.zookeeper.AsyncCallback.StatCallback, IDisposable\n{\n    private static readonly ILog s_log = LogManager.GetLogger(typeof(ZookeeperNodeDataChangeWatcher));\n\n    private const int SessionTimeout = 2000;\n\n    private static string s_nodePath;\n\n    private readonly ZooKeeper m_zookeeper;\n\n    private readonly IObserver&lt;Maybe&lt;byte[]&gt;&gt; m_observer;\n\n    private object m_nodeExists;\n    private bool IsDisposed { get; set; }\n\n    public ZookeeperNodeDataChangeWatcher(string zookeeperName, IObserver&lt;Maybe&lt;byte[]&gt;&gt; observer, string nodePath)\n    {\n        this.m_observer = observer;\n        this.IsDisposed = false;\n        s_nodePath = nodePath;\n        this.m_zookeeper = new ZooKeeper(zookeeperName, SessionTimeout, this);\n\n        this.GetData();\n\n        s_log.Info(string.Format(\"Zookeeperwatcher: started watcher: {0},{1}\", this.m_zookeeper, s_nodePath));\n    }\n\n    private void GetData()\n    {\n        try\n        {\n            this.m_zookeeper.getData(s_nodePath, this, this, null);\n\n        }\n        catch (Exception e)\n        {\n            this.m_observer.OnError(e);\n            s_log.Info(string.Format(\"Error in getData: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n        }\n    }\n\n    private void WatchAgain()\n    {\n        try\n        {\n            this.m_zookeeper.exists(s_nodePath, this, this, null);\n            s_log.Debug(string.Format(\"Rewatching: {0},{1}\", this.m_zookeeper, s_nodePath));\n        }\n        catch (Exception e)\n        {\n            this.m_observer.OnError(e);\n            s_log.Error(string.Format(\"Error in rewatching: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n        }\n    }\n\n    public void processResult(int i, string str, object obj, byte[] barr, Stat s)\n    {\n        var returnCode = i;\n        var data = barr;\n\n        if (this.IsDisposed)\n        {\n            return;\n        }\n\n        if (returnCode == KeeperException.Code.OK.intValue())\n        {\n            this.m_nodeExists = true;\n            this.m_observer.OnNext(Maybe.Return(data));\n            this.WatchAgain();\n        }\n        else if (returnCode == KeeperException.Code.NONODE.intValue())\n        {\n            this.m_nodeExists = false;\n            this.WatchAgain();\n            this.m_observer.OnNext(Maybe.Empty&lt;byte[]&gt;());\n        }\n        else\n        {\n            this.m_observer.OnError(\n                KeeperException.create(KeeperException.Code.get(i)));\n        }\n    }\n\n    public void processResult(int i, string str, object obj, Stat s)\n    {\n        if (this.IsDisposed)\n        {\n            return;\n        }\n\n        var returnCode = i;\n        if (returnCode == KeeperException.Code.OK.intValue() ||\n            returnCode == KeeperException.Code.NONODE.intValue() ||\n            returnCode == KeeperException.Code.NODEEXISTS.intValue())\n        {\n\n            var nodeExistsNow = s != null;\n            var oldExists = (bool?)this.m_nodeExists;\n            this.m_nodeExists = nodeExistsNow;\n            if (nodeExistsNow &amp;&amp; nodeExistsNow != oldExists)\n            {\n                this.GetData();\n            }\n        }\n        else\n        {\n           this.m_observer.OnError(KeeperException.create(\n                KeeperException.Code.get(returnCode)));\n        }\n    }\n\n    public void process(WatchedEvent e)\n    {\n        if (this.IsDisposed)\n        {\n            return;\n        }\n\n        if (s_nodePath != null &amp;&amp; (e == null || s_nodePath != e.getPath()))\n        {\n            this.WatchAgain();\n            return;\n        }\n\n        var type = e.getType();\n        if (type == Watcher.Event.EventType.NodeCreated ||\n            type == Watcher.Event.EventType.NodeDataChanged)\n        {\n            this.GetData();\n        }\n        else if (type == Watcher.Event.EventType.NodeDeleted)\n        {\n            this.m_nodeExists = false;\n            this.m_observer.OnNext(Maybe.Empty&lt;byte[]&gt;());\n            this.WatchAgain();\n        }\n        else\n        {\n            this.WatchAgain();\n        }\n    }\n\n    public void Dispose()\n    {\n        s_log.Debug(string.Format(\"Watcher is disposed: {0},{1}\", this.m_zookeeper, s_nodePath));\n        GC.SuppressFinalize(this);\n    }\n}\n</code></pre>\n\n<p>The implementation of the Clusterstate-watcher:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Reactive.Linq;\nusing System.Text;\nusing Common.Logging;\nusing DIPS.Zookeeper.Model;\n\nusing Newtonsoft.Json;\nusing Newtonsoft.Json.Linq;\n\npublic class ZooKeeperClusterstateWatcher\n{\n    public event ActiveSolrCollectionChangedDelegate ActiveSolrCollectionChanged;\n\n    private static readonly ILog s_log = LogManager.GetLogger(typeof(ZooKeeperClusterstateWatcher));\n\n    private readonly string m_solrCollectionName;\n\n    private readonly string m_zookeeperConnectionString;\n\n    private const string ClusterState = \"/clusterstate.json\";\n\n    public ZooKeeperClusterstateWatcher(string zookeeperConnectionString, string collectionName)\n    {\n        this.m_solrCollectionName = collectionName;\n        m_zookeeperConnectionString = zookeeperConnectionString;\n        Subscriber();\n    }\n\n    public IObservable&lt;Maybe&lt;byte[]&gt;&gt; WatchData(string zookeeper, string nodePath)\n    {\n        var ob = Observable.Create&lt;Maybe&lt;byte[]&gt;&gt;(\n                observer =&gt;\n                    new ZookeeperNodeDataChangeWatcher(zookeeper, observer, nodePath));\n        return ob;\n    }\n\n    public void UpdateActiveSolrCollection(SolrCollection collection)\n    {\n        ActiveSolrCollection.SolrCollection = collection;\n        if (ActiveSolrCollectionChanged != null)\n        {\n            ActiveSolrCollectionChanged(this, new ActiveSolrCollectionChangedDelegateArgs() { ActiveCollection = collection });\n        }\n    }\n\n    public void HandleException(Exception ex)\n    {\n        s_log.Error(string.Format(\"Error in clusterstate.json watcher : {0}\", ex));\n\n        if (ex is org.apache.zookeeper.KeeperException.ConnectionLossException)\n        {\n            Subscriber();\n        }\n    }\n\n    public SolrCollection ParseClusterstateJson(string content)\n    {\n        try\n        {\n            var clusterstate = JsonConvert.DeserializeObject&lt;dynamic&gt;(content);\n            var collection = new SolrCollection { Name = this.m_solrCollectionName };\n            var varShards = new List&lt;Shard&gt;();\n\n            foreach (var solrCollection in clusterstate[collection.Name])\n            {\n                collection.MaxShardsPerNode = int.Parse(clusterstate[collection.Name][\"maxShardsPerNode\"].ToString());\n                collection.ReplicationFactor = int.Parse(clusterstate[collection.Name][\"replicationFactor\"].ToString());\n                collection.AutoAddReplicas = bool.Parse(clusterstate[collection.Name][\"autoAddReplicas\"].ToString());\n                collection.Router = new Router { Name = clusterstate[collection.Name][\"router\"][\"name\"].ToString() };\n                foreach (JObject shards in solrCollection.Children&lt;JObject&gt;())\n                {\n                    foreach (var shard in shards.Properties().Where(prop =&gt; !prop.Name.Equals(\"name\")))\n                    {\n                        var varShard = new Shard();\n\n                        if (clusterstate[collection.Name][\"shards\"][shard.Name][\"state\"].ToString().Equals(\"active\"))\n                        {\n                            varShard.IsActive = true;\n                        }\n\n                        varShard.Name = shard.Name;\n                        varShard.Range = clusterstate[collection.Name][\"shards\"][shard.Name][\"range\"].ToString();\n                        var coreNodes = new List&lt;CoreNode&gt;();\n\n                        foreach (var replicas in clusterstate[collection.Name][\"shards\"][shard.Name][\"replicas\"])\n                        {\n                            var coreNode = new CoreNode();\n                            foreach (JObject replicass in replicas.Children&lt;JObject&gt;())\n                            {\n                                foreach (var coreNodeInReplica in replicass.Properties())\n                                {\n                                    switch (coreNodeInReplica.Name)\n                                    {\n                                        case \"state\":\n                                            coreNode.State = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"node_name\":\n                                            coreNode.NodeName = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"core\":\n                                            coreNode.CoreName = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"base_url\":\n                                            coreNode.BaseUrl = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"leader\":\n                                            coreNode.IsLeader = Convert.ToBoolean(coreNodeInReplica.Value.ToString());\n                                            break;\n                                    }\n                                }\n\n                                coreNodes.Add(coreNode);\n                            }\n\n                            varShard.ReplicaCores = coreNodes;\n                        }\n\n                        varShards.Add(varShard);\n                    }\n                }\n            }\n\n            collection.Shards = varShards;\n            s_log.Debug(string.Format(\"Registered new Solr-collection: {0}. Number of shards is {1}\", collection.Name, collection.Shards.Count()));\n            return collection;\n        }\n        catch (Exception e)\n        {\n            s_log.Error(string.Format(\"Zookeeperwatcher: error in clusterstate.json parser : {0}\", e.Message));\n        }\n\n        return new SolrCollection();\n    }\n\n    private void Subscriber()\n    {\n        var observableClusterState = this.WatchData(m_zookeeperConnectionString, ClusterState);\n        var subscription =\n            observableClusterState.Subscribe(\n                x =&gt; this.UpdateActiveSolrCollection(this.ParseClusterstateJson(Encoding.UTF8.GetString(x.Value))),\n                this.HandleException);\n    }\n}\n\npublic delegate void ActiveSolrCollectionChangedDelegate(object sender, ActiveSolrCollectionChangedDelegateArgs args);\n</code></pre>\n\n<p>From code we simply read out <code>ActiveSolrCollection.SolrCollection.LeaderNode</code>.</p>\n\n<blockquote>\n  <p>NOTE: In retrospect, we ended up rewriting and using the <a href=\"https://www.nuget.org/packages/ZooKeeper.Net/\">Apache ZooKeeper .NET Client</a> package instead, simply because IKVM makes much mess, giving us dependencies to a lot of DLLs and so on.</p>\n</blockquote>\n\n<h3 id=\"someslides\">Some slides</h3>\n\n<p>I ended up giving a short talk about the work as well as presenting extension that can be made. In the future I will look at ZooKeeper as a place to place distributed config for services, as well as service discovery.</p>\n\n<p><iframe src=\"//www.slideshare.net/slideshow/embed_code/46646103\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/mostii/zookeeper-aware\" title=\"Zookeeper-aware application server\" target=\"_blank\">Zookeeper-aware application server</a> </strong> from <strong><a href=\"//www.slideshare.net/mostii\" target=\"_blank\">Andreas Mosti</a></strong> </div></p>\n\n<p>This work is inspired by the work of the guys over at <a href=\"http://www.palladiumconsulting.com/2014/06/using-zookeeper-from-net-ikvm-task/\">Palladium Consulting</a>.</p>","amp":null,"image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-04 23:01:13","created_by":1,"updated_at":"2015-04-13 21:07:35","updated_by":1,"published_at":"2015-04-13 20:51:36","published_by":1},{"id":6,"uuid":"4b4ca266-4c08-4e59-b592-8ab2bab8b14f","title":"Updating a SolrCloud schema in a live enviroment","slug":"updating-a-solrcloud-schema-in-a-live-enviroment","markdown":"I can guarantee this will happen to you at some point: You find out that you need to add some more feilds in a Solr schema becouse you want to index some more data from you're documents. Does that mean taking down the live nodes, changing the schema and then start them up again? Thankfully, no. With the help of [Zookeeper](https://zookeeper.apache.org/) and the [Solr schema REST API](https://cwiki.apache.org/confluence/display/solr/Schema+API) we can do this live without any pain. \n\n###The API calls:\n\nFirst of, let's update the new config to the zookeeper: \n\n\t\t./zkcli.sh -zkhost myZookeeper:2181 -cmd upconfig -confname myConfig -confdir /path/to/my/conf/\n\t\t\nThen just reload the core (I do this on the shard leader out of habit): \n\n\t\tcurl http://mySolrNode:8983/solr/admin/cores?action=reload&core=myCore\n\t\t\nThis trick has saved me many times. \n","mobiledoc":null,"html":"<p>I can guarantee this will happen to you at some point: You find out that you need to add some more feilds in a Solr schema becouse you want to index some more data from you're documents. Does that mean taking down the live nodes, changing the schema and then start them up again? Thankfully, no. With the help of <a href=\"https://zookeeper.apache.org/\">Zookeeper</a> and the <a href=\"https://cwiki.apache.org/confluence/display/solr/Schema+API\">Solr schema REST API</a> we can do this live without any pain. </p>\n\n<h3 id=\"theapicalls\">The API calls:</h3>\n\n<p>First of, let's update the new config to the zookeeper: </p>\n\n<pre><code>    ./zkcli.sh -zkhost myZookeeper:2181 -cmd upconfig -confname myConfig -confdir /path/to/my/conf/\n</code></pre>\n\n<p>Then just reload the core (I do this on the shard leader out of habit): </p>\n\n<pre><code>    curl http://mySolrNode:8983/solr/admin/cores?action=reload&amp;core=myCore\n</code></pre>\n\n<p>This trick has saved me many times. </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-25 12:49:12","created_by":1,"updated_at":"2015-04-25 12:52:10","updated_by":1,"published_at":"2015-04-25 12:52:10","published_by":1},{"id":7,"uuid":"be44aad0-60f1-4ec8-9b2f-ab2f6200e9a8","title":"Easy way to upgrade Docker","slug":"easy-way-to-upgrade-docker","markdown":"If you use a system with a strickt package repository (like Debian stable, CentOS, RedHat etc.) chances are new releases of Docker won't be pushed right after release. A sweet trick I use on my CentOS machines is to just wget down the new binary like so: \n\n\t\tservice docker stop\n        wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker\n        service docker start\n        \nSince Docker has released with quite a steady pace (and I like latest and greatest) I have eaven made an alias for the job - \n\n\t\talias getLatestDockerBinary='sudo wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker'\n        \nHope this might help somebody save time in the future.","mobiledoc":null,"html":"<p>If you use a system with a strickt package repository (like Debian stable, CentOS, RedHat etc.) chances are new releases of Docker won't be pushed right after release. A sweet trick I use on my CentOS machines is to just wget down the new binary like so: </p>\n\n<pre><code>    service docker stop\n    wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker\n    service docker start\n</code></pre>\n\n<p>Since Docker has released with quite a steady pace (and I like latest and greatest) I have eaven made an alias for the job - </p>\n\n<pre><code>    alias getLatestDockerBinary='sudo wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker'\n</code></pre>\n\n<p>Hope this might help somebody save time in the future.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-04 20:11:11","created_by":1,"updated_at":"2015-05-04 20:20:21","updated_by":1,"published_at":"2015-05-04 20:20:21","published_by":1},{"id":8,"uuid":"ddd0bd10-7909-41e5-9804-f36bbff96b40","title":"Run Github's Atom editor in Docker (Aka. Containers on the desktop)","slug":"untitlrun-githubs-atom-editor-in-docker-aka-containers-on-the-desktoped","markdown":"\nBy now everybody loves [Docker](https://www.docker.com/). I mean, what is there not to love? Never have it been easier to program, pack and deploy you're applications and getting out of [dependency hell](https://www.youtube.com/watch?v=3N3n9FzebAA) while keeping them isolated from each other.\nDocker has solved the whole \"download and install this on the server, but remember to have the right version of Java (no, not that one!) and Tomcat (7, not 8)\" - problem. All servers are happy.\n\nA couple of months ago I read a [blogpost](https://blog.jessfraz.com/post/docker-containers-on-the-desktop/) by the talented Docker engineer [Jessie Frazelle](https://github.com/jfrazelle) who has made a large collection of Docker images for the [desktop](https://github.com/jfrazelle/dockerfiles).\nThe FREAKING desktop. To [quote](https://twitter.com/frazelledazzell/status/596345044912635904) Jessie:\n\n<blockquote class=\"twitter-tweet\" lang=\"no\"><p lang=\"en\" dir=\"ltr\">The worst thing someone could do if I ever left my computer unlocked (which will never happen) is install something directly on my host</p>&mdash; jessie frazelle (@frazelledazzell) <a href=\"https://twitter.com/frazelledazzell/status/596345044912635904\">7. mai 2015</a></blockquote> <script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nFeeling inspired by this crazy lady I decided to try it out myself. So here is a Dockerfile for the [Atom](https://atom.io/) editor you can run on you're Linux desktop (because, why the hell not?)\n\n### Dockerfile\n\n\n    # Based on work by Jessie Frazelle, https://github.com/jfrazelle/dockerfiles/blob/master/atom/Dockerfile\n    FROM ubuntu:14.04\n    MAINTAINER Andreas Mosti <andreas.mosti@gmail.com>\n\n    RUN apt-get update && apt-get install -y \\\n        build-essential \\\n        ca-certificates \\\n        curl \\\n        git \\\n        libasound2 \\\n        libgconf-2-4 \\\n        libgnome-keyring-dev \\\n        libgtk2.0-0 \\\n        libnss3 \\\n        libxtst6 \\\n        --no-install-recommends\n\n    RUN curl -sL https://deb.nodesource.com/setup | bash -\n    RUN apt-get install -y nodejs\n\n    RUN git clone https://github.com/atom/atom /src\n    WORKDIR /src\n    RUN git fetch && git checkout $(git describe --tags `git rev-list --tags --max-count=1`)\n    RUN script/build && script/grunt install\n\n    # set up user and permission\n    RUN export uid=1000 gid=1000 && \\\n        mkdir -p /home/developer && \\\n        echo \"developer:x:${uid}:${gid}:Developer,,,:/home/developer:/bin/bash\" >> /etc/passwd && \\\n        echo \"developer:x:${uid}:\" >> /etc/group && \\\n        echo \"developer ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/developer && \\\n        chmod 0440 /etc/sudoers.d/developer && \\\n        chown ${uid}:${gid} -R /home/developer\n\n    USER developer\n    ENV HOME /home/developer\n    \n    # get my configfiles from github\n\tRUN mkdir .atom &&  \\ \n    git clone https://github.com/andmos/dotfiles.git && \\ \n    cd dotfiles/atom; ./configureAtom\n\n    CMD /usr/local/bin/atom --foreground --log-file /var/log/atom.log && tail -f /var/log/atom.log\n\n\n### Build and run\n\n    wget https://raw.githubusercontent.com/andmos/Docker-Atom/master/Dockerfile\n    docker build -t atom\n    docker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\n\nAnother cool thing we can do is to attach a [data container](https://docs.docker.com/userguide/dockervolumes/) with the\n``--volumes-from=`` flag. Here is how to build a data container from a git-repo and attach: \n\n    docker run --name coffee -e repo=https://github.com/andmos/coffee.git -t andmos/git\n    docker run --volumes-from:coffee -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\nopen ``/var/workspace/coffee`` and there is the repo! \n\nThis image has lots of possibilities - happy hacking!\n","mobiledoc":null,"html":"<p>By now everybody loves <a href=\"https://www.docker.com/\">Docker</a>. I mean, what is there not to love? Never have it been easier to program, pack and deploy you're applications and getting out of <a href=\"https://www.youtube.com/watch?v=3N3n9FzebAA\">dependency hell</a> while keeping them isolated from each other. <br />\nDocker has solved the whole \"download and install this on the server, but remember to have the right version of Java (no, not that one!) and Tomcat (7, not 8)\" - problem. All servers are happy.</p>\n\n<p>A couple of months ago I read a <a href=\"https://blog.jessfraz.com/post/docker-containers-on-the-desktop/\">blogpost</a> by the talented Docker engineer <a href=\"https://github.com/jfrazelle\">Jessie Frazelle</a> who has made a large collection of Docker images for the <a href=\"https://github.com/jfrazelle/dockerfiles\">desktop</a>. <br />\nThe FREAKING desktop. To <a href=\"https://twitter.com/frazelledazzell/status/596345044912635904\">quote</a> Jessie:</p>\n\n<p><blockquote class=\"twitter-tweet\" lang=\"no\"><p lang=\"en\" dir=\"ltr\">The worst thing someone could do if I ever left my computer unlocked (which will never happen) is install something directly on my host</p>&mdash; jessie frazelle (@frazelledazzell) <a href=\"https://twitter.com/frazelledazzell/status/596345044912635904\">7. mai 2015</a></blockquote> <script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></p>\n\n<p>Feeling inspired by this crazy lady I decided to try it out myself. So here is a Dockerfile for the <a href=\"https://atom.io/\">Atom</a> editor you can run on you're Linux desktop (because, why the hell not?)</p>\n\n<h3 id=\"dockerfile\">Dockerfile</h3>\n\n<pre><code># Based on work by Jessie Frazelle, https://github.com/jfrazelle/dockerfiles/blob/master/atom/Dockerfile\nFROM ubuntu:14.04\nMAINTAINER Andreas Mosti &lt;andreas.mosti@gmail.com&gt;\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    ca-certificates \\\n    curl \\\n    git \\\n    libasound2 \\\n    libgconf-2-4 \\\n    libgnome-keyring-dev \\\n    libgtk2.0-0 \\\n    libnss3 \\\n    libxtst6 \\\n    --no-install-recommends\n\nRUN curl -sL https://deb.nodesource.com/setup | bash -\nRUN apt-get install -y nodejs\n\nRUN git clone https://github.com/atom/atom /src\nWORKDIR /src\nRUN git fetch &amp;&amp; git checkout $(git describe --tags `git rev-list --tags --max-count=1`)\nRUN script/build &amp;&amp; script/grunt install\n\n# set up user and permission\nRUN export uid=1000 gid=1000 &amp;&amp; \\\n    mkdir -p /home/developer &amp;&amp; \\\n    echo \"developer:x:${uid}:${gid}:Developer,,,:/home/developer:/bin/bash\" &gt;&gt; /etc/passwd &amp;&amp; \\\n    echo \"developer:x:${uid}:\" &gt;&gt; /etc/group &amp;&amp; \\\n    echo \"developer ALL=(ALL) NOPASSWD: ALL\" &gt; /etc/sudoers.d/developer &amp;&amp; \\\n    chmod 0440 /etc/sudoers.d/developer &amp;&amp; \\\n    chown ${uid}:${gid} -R /home/developer\n\nUSER developer\nENV HOME /home/developer\n\n# get my configfiles from github\nRUN mkdir .atom &amp;&amp;  \\ \ngit clone https://github.com/andmos/dotfiles.git &amp;&amp; \\ \ncd dotfiles/atom; ./configureAtom\n\nCMD /usr/local/bin/atom --foreground --log-file /var/log/atom.log &amp;&amp; tail -f /var/log/atom.log\n</code></pre>\n\n<h3 id=\"buildandrun\">Build and run</h3>\n\n<pre><code>wget https://raw.githubusercontent.com/andmos/Docker-Atom/master/Dockerfile\ndocker build -t atom\ndocker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\n</code></pre>\n\n<p>Another cool thing we can do is to attach a <a href=\"https://docs.docker.com/userguide/dockervolumes/\">data container</a> with the <br />\n<code>--volumes-from=</code> flag. Here is how to build a data container from a git-repo and attach: </p>\n\n<pre><code>docker run --name coffee -e repo=https://github.com/andmos/coffee.git -t andmos/git\ndocker run --volumes-from:coffee -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\n</code></pre>\n\n<p>open <code>/var/workspace/coffee</code> and there is the repo! </p>\n\n<p>This image has lots of possibilities - happy hacking!</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-23 15:08:24","created_by":1,"updated_at":"2015-07-19 20:21:58","updated_by":1,"published_at":"2015-05-23 15:09:31","published_by":1},{"id":9,"uuid":"c9dc4ef3-ed63-4f2f-a6a4-4ed774e01cf5","title":"On Development Managers","slug":"on-development-managers","markdown":"\nA couple of days ago ex-GitHub employee and brilliant  tech-speaker [Zach Holman](http://zachholman.com/) tweeted the following:\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">There are a lot of ways to become a bad manager, but tapping me on my shoulder when I’m building shit is probably the most heinous.</p>&mdash; Zach Holman (@holman) <a href=\"https://twitter.com/holman/status/620278257506758656\">July 12, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThis statement hits the problem with bad managers spot on. Many companies struggle with this, managers who have to check in on the employees constantly - thus preventing them from being productive. In my opinion, this entire situation show a lack of trust in people and is not only bad for productivity, but also company culture. Why would a developer, a creative human by nature, want to work at a company where his boss has so little trust in him that he constantly checks in on the work he does?\n\nFortunately for me, I have only good experiences with development managers. Here is why I believe the DMs work well at my current company, [DIPS](https://www.dips.no/).\n\n### The DM's are developers themselves\nAll of our DM's have a long history as developers. They are over average interested in software development and tech, thus giving them full understanding of how developers think and how the development process work. Hell, they are one of us. Some of the DM's even rolls up his sleeves and put in some hours of coding if the project allows it. I think this is a great way for the DM's to keep on top of the project and get hands on. [Eliot Horowitz thinks Engineering Managers Should Code 30% of Their Time](http://www.drdobbs.com/architecture-and-design/engineering-managers-should-code-30-of-t/240165174).\n\nIn my opinion, DM's ***should*** have a background as developers. Putting a hot head out of business school in charge might work in the traditional office, but not in a software company.\n\n### The DM's have 100% confidence in their teams\nOn a day to day basis I rarely talk to my DM. Most of my time is spent with my teammates, being creative and building great stuff. Together. I never have my DM hanging over my shoulders asking questions or checking up on me \"to see if I actually do work\". This is because the DM have put together the team consisting of people he or she believes will work good together and get the work done in a brilliant way. The DM have full confidence in the team as a whole. If the developers need something from the DM, they contact him rather than him them. In that way both parties can focus on their work with minimal interruptions.\n\nMy team also work remote, so even if the DM wanted, he could not sneak up behind us to \"check how the work is coming along\". For remote teams trust is even more important.\n### The DM's does hiring\n\nIn the hiring process the DM's interview candidates and put them through the technical interviews. If a DM is unsure of a candidate, the answer is usually no; they only hire people they are 100% sure of. This fits well with the rule of trust. Here we also follow a golden Google rule: [Try to hire people that are smarter than yourself.](http://www.amazon.com/How-Google-Works-Eric-Schmidt/dp/1455582344)\nIt is also important for the DM to hire people who fit well with the rest of the team.\nSome will argue that hiring is the most important job the DM has.  \n","mobiledoc":null,"html":"<p>A couple of days ago ex-GitHub employee and brilliant  tech-speaker <a href=\"http://zachholman.com/\">Zach Holman</a> tweeted the following:</p>\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">There are a lot of ways to become a bad manager, but tapping me on my shoulder when I’m building shit is probably the most heinous.</p>&mdash; Zach Holman (@holman) <a href=\"https://twitter.com/holman/status/620278257506758656\">July 12, 2015</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>This statement hits the problem with bad managers spot on. Many companies struggle with this, managers who have to check in on the employees constantly - thus preventing them from being productive. In my opinion, this entire situation show a lack of trust in people and is not only bad for productivity, but also company culture. Why would a developer, a creative human by nature, want to work at a company where his boss has so little trust in him that he constantly checks in on the work he does?</p>\n\n<p>Fortunately for me, I have only good experiences with development managers. Here is why I believe the DMs work well at my current company, <a href=\"https://www.dips.no/\">DIPS</a>.</p>\n\n<h3 id=\"thedmsaredevelopersthemselves\">The DM's are developers themselves</h3>\n\n<p>All of our DM's have a long history as developers. They are over average interested in software development and tech, thus giving them full understanding of how developers think and how the development process work. Hell, they are one of us. Some of the DM's even rolls up his sleeves and put in some hours of coding if the project allows it. I think this is a great way for the DM's to keep on top of the project and get hands on. <a href=\"http://www.drdobbs.com/architecture-and-design/engineering-managers-should-code-30-of-t/240165174\">Eliot Horowitz thinks Engineering Managers Should Code 30% of Their Time</a>.</p>\n\n<p>In my opinion, DM's <strong><em>should</em></strong> have a background as developers. Putting a hot head out of business school in charge might work in the traditional office, but not in a software company.</p>\n\n<h3 id=\"thedmshave100confidenceintheirteams\">The DM's have 100% confidence in their teams</h3>\n\n<p>On a day to day basis I rarely talk to my DM. Most of my time is spent with my teammates, being creative and building great stuff. Together. I never have my DM hanging over my shoulders asking questions or checking up on me \"to see if I actually do work\". This is because the DM have put together the team consisting of people he or she believes will work good together and get the work done in a brilliant way. The DM have full confidence in the team as a whole. If the developers need something from the DM, they contact him rather than him them. In that way both parties can focus on their work with minimal interruptions.</p>\n\n<p>My team also work remote, so even if the DM wanted, he could not sneak up behind us to \"check how the work is coming along\". For remote teams trust is even more important.  </p>\n\n<h3 id=\"thedmsdoeshiring\">The DM's does hiring</h3>\n\n<p>In the hiring process the DM's interview candidates and put them through the technical interviews. If a DM is unsure of a candidate, the answer is usually no; they only hire people they are 100% sure of. This fits well with the rule of trust. Here we also follow a golden Google rule: <a href=\"http://www.amazon.com/How-Google-Works-Eric-Schmidt/dp/1455582344\">Try to hire people that are smarter than yourself.</a> <br />\nIt is also important for the DM to hire people who fit well with the rest of the team. <br />\nSome will argue that hiring is the most important job the DM has.  </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-07-17 12:49:25","created_by":1,"updated_at":"2015-07-17 12:50:36","updated_by":1,"published_at":"2015-07-17 12:50:36","published_by":1},{"id":10,"uuid":"074efc10-ac4a-442a-b668-f72b6b3ef4ff","title":"Docker Garbage Collection","slug":"docker-garbage-collection","markdown":"\nA mayor issue you might have run into when working on Docker-images is actual disk space. Over time, images and containers can consume a great deal of your precious drive, often when you are developing the actual image and having a test-run after each `ADD` or `RUN` block. The containers then tend to multiply as frequently as rabbits.\n\nTo solve this problem and the lack of Docker \"garbage collection\", the good guys over at [Spotify](https://spotify.com) has created the [docker-gc](https://github.com/spotify/docker-gc) project on GitHub. This project contains a script that removes ALL containers that has been exited over an hour ago, together with their respective images. One hour might sound extreme, but if you think about it, containers that have exited and is not running are mostly (or at least should be!) throw-away containers we have no need for anymore.\n\n### Install and run\nThe script can be run standalone or create a deb-package:\n\n    $ apt-get install git devscripts debhelper\n    $ git clone https://github.com/spotify/docker-gc.git\n    $ cd docker-gc\n    $ debuild -us -uc -b\n\n    # install:\n    $ dpkg -i ../docker-gc_0.0.3_all.deb\n\n    $ sudo docker-gc\n\nThe script is of course best used as a cron.hourly job.\n\nIf you like to run everything in Docker (and hey, why wouldn't you) they have also made a Dockerfile for you:\n\n    docker build -t spotify/docker-gc .\n    docker run --rm -v /var/run/docker.sock:/var/run/docker.sock spotify/docker-gc\n\n### Exclude images\n\nSome images are nice to actually keep on the host (maby for dev or on a production server?)\nIt would have been a real bummer if there were no support for exclusion of images from garbage collection, but thankfully there is:\n\n    touch /etc/docker-gc-exclude\n    echo \"spotify/cassandra:latest\" >> /etc/docker-gc-exclude\n\n\n### Notes\nIf you are running a docker version above 1.6 and experience an error with the date format like so:\n\n      $ sudo docker-gc\n      date: invalid date ‘\"2015-07-25 09:26:5’\nReplace the related lines in the data_parse function with these lines:\n\n    replace_t=\"${without_ms/T/ }\"\n    replace_quote=\"${replace_t#\\\"}\"\n    epoch=$(date_parse \"${replace_quote}\")\nAs of 03.08.2015 this fix has not been merged to master but I guess that will happen at some point soon.\n","mobiledoc":null,"html":"<p>A mayor issue you might have run into when working on Docker-images is actual disk space. Over time, images and containers can consume a great deal of your precious drive, often when you are developing the actual image and having a test-run after each <code>ADD</code> or <code>RUN</code> block. The containers then tend to multiply as frequently as rabbits.</p>\n\n<p>To solve this problem and the lack of Docker \"garbage collection\", the good guys over at <a href=\"https://spotify.com\">Spotify</a> has created the <a href=\"https://github.com/spotify/docker-gc\">docker-gc</a> project on GitHub. This project contains a script that removes ALL containers that has been exited over an hour ago, together with their respective images. One hour might sound extreme, but if you think about it, containers that have exited and is not running are mostly (or at least should be!) throw-away containers we have no need for anymore.</p>\n\n<h3 id=\"installandrun\">Install and run</h3>\n\n<p>The script can be run standalone or create a deb-package:</p>\n\n<pre><code>$ apt-get install git devscripts debhelper\n$ git clone https://github.com/spotify/docker-gc.git\n$ cd docker-gc\n$ debuild -us -uc -b\n\n# install:\n$ dpkg -i ../docker-gc_0.0.3_all.deb\n\n$ sudo docker-gc\n</code></pre>\n\n<p>The script is of course best used as a cron.hourly job.</p>\n\n<p>If you like to run everything in Docker (and hey, why wouldn't you) they have also made a Dockerfile for you:</p>\n\n<pre><code>docker build -t spotify/docker-gc .\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock spotify/docker-gc\n</code></pre>\n\n<h3 id=\"excludeimages\">Exclude images</h3>\n\n<p>Some images are nice to actually keep on the host (maby for dev or on a production server?) <br />\nIt would have been a real bummer if there were no support for exclusion of images from garbage collection, but thankfully there is:</p>\n\n<pre><code>touch /etc/docker-gc-exclude\necho \"spotify/cassandra:latest\" &gt;&gt; /etc/docker-gc-exclude\n</code></pre>\n\n<h3 id=\"notes\">Notes</h3>\n\n<p>If you are running a docker version above 1.6 and experience an error with the date format like so:</p>\n\n<pre><code>  $ sudo docker-gc\n  date: invalid date ‘\"2015-07-25 09:26:5’\n</code></pre>\n\n<p>Replace the related lines in the data_parse function with these lines:</p>\n\n<pre><code>replace_t=\"${without_ms/T/ }\"\nreplace_quote=\"${replace_t#\\\"}\"\nepoch=$(date_parse \"${replace_quote}\")\n</code></pre>\n\n<p>As of 03.08.2015 this fix has not been merged to master but I guess that will happen at some point soon.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-08-03 19:00:11","created_by":1,"updated_at":"2015-08-03 19:00:45","updated_by":1,"published_at":"2015-08-03 19:00:45","published_by":1},{"id":11,"uuid":"9f7b0043-5aae-422c-9ab0-bf29deafcdc4","title":"Managing NuGet dependencies with Paket","slug":"managing-nuget-dependencies-with-paket","markdown":"Since we mainly write .NET code at work, the only real choice for packing dependencies and applications is [NuGet](https://www.nuget.org/) and [Chocolatey](https://chocolatey.org/), everything stored in different feeds on the excellent [ProGet](http://inedo.com/proget/overview) server.  For CI we build our code with [TFS](https://msdn.microsoft.com/en-us/vstudio/ff637362.aspx) and [Team City](https://www.jetbrains.com/teamcity/) using some hand-made build scripts written in [scriptcs](http://scriptcs.net/), while deployment is done with good old [Octopus Deploy](https://octopusdeploy.com/).\n\nAll this works perfectly in our in-house environments, everything is just a `choco install`  away.\nThe only catch her is that we want to ship these packages to our customers as standalone installations. To do this we have  written some custom powershell-scripts that wraps chocolatey and points to a local packages-folder where the correct .nupkg-files for the applications are stored. Since we want to use this exact installation mechanism in our CI pipeline (aka eating your own dog food) we had to be creative.\n\nChocolatey is great for installing software, but the easiest way to actually grab the .nupkg-files without installing them (and get the version number as a part of the .nupkg filename, chocolatey does not include this for some reason) is to use the NuGet CLI itself:\n\n      nuget install mypackage-server -Source http://myproget -OutputDirectory \"C:\\SetupScript\\Packages\"\n\nHere we ran in to a much debated issue: NuGet's way of managing dependencies. By default NuGet resolves the **lowest** available (and legal) version of all dependencies. I understand that this is a safe choice, but for CI we always want to test and deploy the latest version of all packages and its dependencies. The worst part is, after hours of googling and trying we found no way to force NuGet to use the highest available versions of dependencies. To solve this issue I turned to [Paket](http://fsprojects.github.io/Paket/).\n\nPaket is a wonderful project that takes care of dependencies in your project for you in a much more elegant (and Ruby-like) way then what NuGet itself does. Here is an example:\n\n    $ mkdir .paket\n    $ wget https://github.com/fsprojects/Paket/releases/download/1.23.0/paket.exe -p .paket/\n    $ touch paket.dependencies\n\nIn the `paket.dependencies` file, put in your dependencies like this:\n\n    source https://nuget.org/api/v2\n\n    nuget Castle.Windsor-log4net >= 3.2\n    nuget NUnit\n\n    github forki/FsUnit FsUnit.fs\n\nTo install dependencies:\n\n    $ .paket/paket.exe install\n\nAll dependencies (including correct, transitive dependencies in **latest** versions!) are stored in the `paket.lock` file:\n\n    NUGET\n      remote: https://nuget.org/api/v2\n      specs:\n        Castle.Core (3.3.3)\n        Castle.Core-log4net (3.3.3)\n          Castle.Core (>= 3.3.3)\n          log4net (1.2.10)\n        Castle.LoggingFacility (3.3.0)\n          Castle.Core (>= 3.3.0)\n          Castle.Windsor (>= 3.3.0)\n        Castle.Windsor (3.3.0)\n          Castle.Core (>= 3.3.0)\n        Castle.Windsor-log4net (3.3.0)\n          Castle.Core-log4net (>= 3.3.0)\n          Castle.LoggingFacility (>= 3.3.0)\n        log4net (1.2.10)\n        NUnit (2.6.4)\n    GITHUB\n      remote: forki/FsUnit\n      specs:\n        FsUnit.fs (81d27fd09575a32c4ed52eadb2eeac5f365b8348)\n\nThe files end up in a folder called `Packages` with .npkg files and all, just how we like it.\n\nAs a bonus, here is the powershell-script Octopus Deploys runs:\n\n    $SetupRoot = \"C:\\SetupScript\"\n    $SetupPs1 = \"SetupRoot\\Setup.ps1\"\n    $OFS = \"`r`n\"\n\n    Write-Host \"Clearing old cache...\"\n    Remove-Item SetupRoot\\packages\\* -Recurse -Force -ExcludeSetup.psm1,Setup\n\n    if (-not $packageName) {\n        throw \"Please specify the name of a package to install.\"\n    }\n\n    if($version){\n        $versionString = \"-Version $version\"\n    }\n\n    Write-Host \"Fetching packages...\"\n\n    cd $SetupRoot\n\n    Set-Content -Value \"source $sourceFeed $OFS\" -Path $SetupRoot\\paket.dependencies\n    Add-Content -Value \"nuget $packageName\" -Path $SetupRoot\\paket.dependencies\n\n\n    if(Test-Path $SetupRoot\\paket.lock){\n        Write-Host \"Removing paket.lock file\"\n        Remove-Item $DIPSSetupRoot\\paket.lock\n    }\n\n    & .paket/paket.exe install\n\n    Write-Host \"Discovering chocolatey packages...\"\n    Copy-Item $SetupRoot\\packages\\*\\*.nupkg $SetupRoot\\packages\n\n    Write-Host \"Installing chocolatey package...\"\n    if ($myval -eq $null) { \"new value\" } else { $myval }\n\n    if (-not $version){\n        $version = [String]::Join(\".\",(Get-ChildItem $SetupRoot\\packages\\$packageName*.nupkg)[0].Name.Split('.'),1,4)\n    }\n\n    $Config = Get-Item \"$SetupRoot\\Packages.config\"\n    $ConfigContent = [xml]@\"\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    <packages>\n      <package id=\"$packageName\" version=\"$version\" />\n    </packages>\n    \"@\n    $ConfigContent.Save($Config)\n\n    & $SetupRoot\\Setup.ps1\n","mobiledoc":null,"html":"<p>Since we mainly write .NET code at work, the only real choice for packing dependencies and applications is <a href=\"https://www.nuget.org/\">NuGet</a> and <a href=\"https://chocolatey.org/\">Chocolatey</a>, everything stored in different feeds on the excellent <a href=\"http://inedo.com/proget/overview\">ProGet</a> server.  For CI we build our code with <a href=\"https://msdn.microsoft.com/en-us/vstudio/ff637362.aspx\">TFS</a> and <a href=\"https://www.jetbrains.com/teamcity/\">Team City</a> using some hand-made build scripts written in <a href=\"http://scriptcs.net/\">scriptcs</a>, while deployment is done with good old <a href=\"https://octopusdeploy.com/\">Octopus Deploy</a>.</p>\n\n<p>All this works perfectly in our in-house environments, everything is just a <code>choco install</code>  away. <br />\nThe only catch her is that we want to ship these packages to our customers as standalone installations. To do this we have  written some custom powershell-scripts that wraps chocolatey and points to a local packages-folder where the correct .nupkg-files for the applications are stored. Since we want to use this exact installation mechanism in our CI pipeline (aka eating your own dog food) we had to be creative.</p>\n\n<p>Chocolatey is great for installing software, but the easiest way to actually grab the .nupkg-files without installing them (and get the version number as a part of the .nupkg filename, chocolatey does not include this for some reason) is to use the NuGet CLI itself:</p>\n\n<pre><code>  nuget install mypackage-server -Source http://myproget -OutputDirectory \"C:\\SetupScript\\Packages\"\n</code></pre>\n\n<p>Here we ran in to a much debated issue: NuGet's way of managing dependencies. By default NuGet resolves the <strong>lowest</strong> available (and legal) version of all dependencies. I understand that this is a safe choice, but for CI we always want to test and deploy the latest version of all packages and its dependencies. The worst part is, after hours of googling and trying we found no way to force NuGet to use the highest available versions of dependencies. To solve this issue I turned to <a href=\"http://fsprojects.github.io/Paket/\">Paket</a>.</p>\n\n<p>Paket is a wonderful project that takes care of dependencies in your project for you in a much more elegant (and Ruby-like) way then what NuGet itself does. Here is an example:</p>\n\n<pre><code>$ mkdir .paket\n$ wget https://github.com/fsprojects/Paket/releases/download/1.23.0/paket.exe -p .paket/\n$ touch paket.dependencies\n</code></pre>\n\n<p>In the <code>paket.dependencies</code> file, put in your dependencies like this:</p>\n\n<pre><code>source https://nuget.org/api/v2\n\nnuget Castle.Windsor-log4net &gt;= 3.2\nnuget NUnit\n\ngithub forki/FsUnit FsUnit.fs\n</code></pre>\n\n<p>To install dependencies:</p>\n\n<pre><code>$ .paket/paket.exe install\n</code></pre>\n\n<p>All dependencies (including correct, transitive dependencies in <strong>latest</strong> versions!) are stored in the <code>paket.lock</code> file:</p>\n\n<pre><code>NUGET\n  remote: https://nuget.org/api/v2\n  specs:\n    Castle.Core (3.3.3)\n    Castle.Core-log4net (3.3.3)\n      Castle.Core (&gt;= 3.3.3)\n      log4net (1.2.10)\n    Castle.LoggingFacility (3.3.0)\n      Castle.Core (&gt;= 3.3.0)\n      Castle.Windsor (&gt;= 3.3.0)\n    Castle.Windsor (3.3.0)\n      Castle.Core (&gt;= 3.3.0)\n    Castle.Windsor-log4net (3.3.0)\n      Castle.Core-log4net (&gt;= 3.3.0)\n      Castle.LoggingFacility (&gt;= 3.3.0)\n    log4net (1.2.10)\n    NUnit (2.6.4)\nGITHUB\n  remote: forki/FsUnit\n  specs:\n    FsUnit.fs (81d27fd09575a32c4ed52eadb2eeac5f365b8348)\n</code></pre>\n\n<p>The files end up in a folder called <code>Packages</code> with .npkg files and all, just how we like it.</p>\n\n<p>As a bonus, here is the powershell-script Octopus Deploys runs:</p>\n\n<pre><code>$SetupRoot = \"C:\\SetupScript\"\n$SetupPs1 = \"SetupRoot\\Setup.ps1\"\n$OFS = \"`r`n\"\n\nWrite-Host \"Clearing old cache...\"\nRemove-Item SetupRoot\\packages\\* -Recurse -Force -ExcludeSetup.psm1,Setup\n\nif (-not $packageName) {\n    throw \"Please specify the name of a package to install.\"\n}\n\nif($version){\n    $versionString = \"-Version $version\"\n}\n\nWrite-Host \"Fetching packages...\"\n\ncd $SetupRoot\n\nSet-Content -Value \"source $sourceFeed $OFS\" -Path $SetupRoot\\paket.dependencies\nAdd-Content -Value \"nuget $packageName\" -Path $SetupRoot\\paket.dependencies\n\n\nif(Test-Path $SetupRoot\\paket.lock){\n    Write-Host \"Removing paket.lock file\"\n    Remove-Item $DIPSSetupRoot\\paket.lock\n}\n\n&amp; .paket/paket.exe install\n\nWrite-Host \"Discovering chocolatey packages...\"\nCopy-Item $SetupRoot\\packages\\*\\*.nupkg $SetupRoot\\packages\n\nWrite-Host \"Installing chocolatey package...\"\nif ($myval -eq $null) { \"new value\" } else { $myval }\n\nif (-not $version){\n    $version = [String]::Join(\".\",(Get-ChildItem $SetupRoot\\packages\\$packageName*.nupkg)[0].Name.Split('.'),1,4)\n}\n\n$Config = Get-Item \"$SetupRoot\\Packages.config\"\n$ConfigContent = [xml]@\"\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;packages&gt;\n  &lt;package id=\"$packageName\" version=\"$version\" /&gt;\n&lt;/packages&gt;\n\"@\n$ConfigContent.Save($Config)\n\n&amp; $SetupRoot\\Setup.ps1\n</code></pre>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-08-07 16:53:47","created_by":1,"updated_at":"2015-08-07 16:55:05","updated_by":1,"published_at":"2015-08-07 16:54:46","published_by":1},{"id":12,"uuid":"82db10bc-c2ce-4874-963d-f35f8dfed994","title":"Build and publish AsciiDoc with Docker and nginx","slug":"build-and-publish-asciidoc-with-docker-and-nginx","markdown":"One of the teams at work is piloting the usage of [AsciiDoc](http://www.methods.co.nz/asciidoc/) for documentation of their product. AsciiDoc is a markup-format just like Gruber's [Markdown](http://daringfireball.net/projects/markdown/), but is more advanced and offers more possibilities. Since the documentation is located alongside the source code in Git (as it should be!) I created a simple build step for easy build and deploy of the documentation with my favorite tool, [Docker](https://www.docker.com/) with public images directly from the [Docker Hub](https://hub.docker.com/). \n\nThese steps gets triggered on each build:\n\n    if [[ -z \"$(sudo docker ps | grep webserver)\" ]]; then\n        sudo docker run -dt --name webserver -p 80:80 -v /usr/share/nginx/html nginx\n    fi\n\n    sudo docker run -it -v /var/buildDropLocation/build/docs:/documents/ --volumes-from webserver asciidoctor/docker-asciidoctor asciidoctor -a stylesheet=dips.css -a toc-left technical_document.adoc -D /usr/share/nginx/html\n\nIn short, we check if the [nginx](http://nginx.org/)-container is running and starts it if needed. The HTML-folder is exposed.\nNext we pull down and run the [asciidoctor-image](https://github.com/asciidoctor/docker-asciidoctor) from the Docker Hub and link in the docs-folder from the build. We use [TeamCity](https://www.jetbrains.com/teamcity/), so customize these variables to fit your build system. asciidoctor then runs on the files and puts the output HTML in the volume from the nginx-container. Easy as that, live documentation directly from the latest build. ","mobiledoc":null,"html":"<p>One of the teams at work is piloting the usage of <a href=\"http://www.methods.co.nz/asciidoc/\">AsciiDoc</a> for documentation of their product. AsciiDoc is a markup-format just like Gruber's <a href=\"http://daringfireball.net/projects/markdown/\">Markdown</a>, but is more advanced and offers more possibilities. Since the documentation is located alongside the source code in Git (as it should be!) I created a simple build step for easy build and deploy of the documentation with my favorite tool, <a href=\"https://www.docker.com/\">Docker</a> with public images directly from the <a href=\"https://hub.docker.com/\">Docker Hub</a>. </p>\n\n<p>These steps gets triggered on each build:</p>\n\n<pre><code>if [[ -z \"$(sudo docker ps | grep webserver)\" ]]; then\n    sudo docker run -dt --name webserver -p 80:80 -v /usr/share/nginx/html nginx\nfi\n\nsudo docker run -it -v /var/buildDropLocation/build/docs:/documents/ --volumes-from webserver asciidoctor/docker-asciidoctor asciidoctor -a stylesheet=dips.css -a toc-left technical_document.adoc -D /usr/share/nginx/html\n</code></pre>\n\n<p>In short, we check if the <a href=\"http://nginx.org/\">nginx</a>-container is running and starts it if needed. The HTML-folder is exposed. <br />\nNext we pull down and run the <a href=\"https://github.com/asciidoctor/docker-asciidoctor\">asciidoctor-image</a> from the Docker Hub and link in the docs-folder from the build. We use <a href=\"https://www.jetbrains.com/teamcity/\">TeamCity</a>, so customize these variables to fit your build system. asciidoctor then runs on the files and puts the output HTML in the volume from the nginx-container. Easy as that, live documentation directly from the latest build. </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-09-03 19:02:18","created_by":1,"updated_at":"2015-09-03 19:04:43","updated_by":1,"published_at":"2015-09-03 19:02:39","published_by":1},{"id":13,"uuid":"2db8bce6-fc22-462d-8c39-fcc8f665df93","title":"Fun with Travis CI","slug":"fun-with-travis-ci","markdown":"Ok, I need to swallow my own words on this one. Yesterday I stated the following on twitter:\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">I was thinking of writing a blogpost about building with <a href=\"https://twitter.com/travisci\">@travisci</a>, but it is so easy to use that I don&#39;t see the point. <a href=\"https://twitter.com/hashtag/greatProject?src=hash\">#greatProject</a> <a href=\"https://twitter.com/hashtag/CI?src=hash\">#CI</a></p>&mdash; Andreas Mosti (@amostii) <a href=\"https://twitter.com/amostii/status/656205920922390528\">October 19, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nSo, yeah.\n\nRecently I have been playing a lot with [Travis CI](https://travis-ci.com/) for building my open source projects directly from [GitHub](https://github.com/). All you need to get it working is to add a simple ``.travis.yml`` file to the root folder of the project you want to build, where the steps required to build the project is described. Typically it will look something like this:\n\n    language: csharp\n    mono:\n      - latest\n      - 3.12.0\n      - 3.10.0\n    solution: solution-name.sln\n    install:\n      - nuget restore solution-name.sln\n      - nuget install NUnit.Runners -Version 2.6.4 -OutputDirectory testrunner\n    script:\n      - xbuild /p:Configuration=Release solution-name.sln\n      - mono ./testrunner/NUnit.Runners.2.6.4/tools/nunit-console.exe ./MyPoject.Tests/bin/Release/MyProject.Tests.dll\n\nAs my tweet stated, this is really easy to understand just from looking at the file. We select a language, pick out some runtime versions to build on, select a solution, grab some dependencies, run the build and run some tests. commit some code and the build starts. Simple and powerful, as it should be.\n\nPersonally, I prefer to add a `build.sh` file to all my project, which makes it all even easier:  \n\n\n    language: csharp\n    mono:\n      - latest\n      - 3.12.0\n      - 3.10.0\n    before_script:\n        - chmod +x build.sh\n    script:\n      - ./build.sh compile\n      - ./build.sh test\nNormally Travis CI uses [container technology to spawn these builds fast](http://docs.travis-ci.com/user/migrating-from-legacy/#How-can-I-use-container-based-infrastructure%3F), but there is nothing wrong with using containers for yourself in the build:\n\n    language: csharp\n    sudo: required\n    services:\n      - docker\n    before_script:\n      - chmod +x buildServer\n      - ./build.sh build-local\n      - ./build.sh build-docker\n      - ./build.sh run-docker\n    script:\n      - ./build.sh unit-local\n      - ./build.sh integration-local\n\nNow this solution I like a lot. Here we allow the use of [Docker](https://www.docker.com/) thanks to the `sudo: required` and the `-docker` service. Next up we run a regular compile-build both directly on the host and in a Docker-container, before starting this container. The container allows us to run integration-tests against newly built code fast, without the need to deploy it on another server, making the build more efficient and keeps complexity low. The build test the entire solution from compile to deploy, all in a few lines of code.  ","mobiledoc":null,"html":"<p>Ok, I need to swallow my own words on this one. Yesterday I stated the following on twitter:</p>\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">I was thinking of writing a blogpost about building with <a href=\"https://twitter.com/travisci\">@travisci</a>, but it is so easy to use that I don&#39;t see the point. <a href=\"https://twitter.com/hashtag/greatProject?src=hash\">#greatProject</a> <a href=\"https://twitter.com/hashtag/CI?src=hash\">#CI</a></p>&mdash; Andreas Mosti (@amostii) <a href=\"https://twitter.com/amostii/status/656205920922390528\">October 19, 2015</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>So, yeah.</p>\n\n<p>Recently I have been playing a lot with <a href=\"https://travis-ci.com/\">Travis CI</a> for building my open source projects directly from <a href=\"https://github.com/\">GitHub</a>. All you need to get it working is to add a simple <code>.travis.yml</code> file to the root folder of the project you want to build, where the steps required to build the project is described. Typically it will look something like this:</p>\n\n<pre><code>language: csharp\nmono:\n  - latest\n  - 3.12.0\n  - 3.10.0\nsolution: solution-name.sln\ninstall:\n  - nuget restore solution-name.sln\n  - nuget install NUnit.Runners -Version 2.6.4 -OutputDirectory testrunner\nscript:\n  - xbuild /p:Configuration=Release solution-name.sln\n  - mono ./testrunner/NUnit.Runners.2.6.4/tools/nunit-console.exe ./MyPoject.Tests/bin/Release/MyProject.Tests.dll\n</code></pre>\n\n<p>As my tweet stated, this is really easy to understand just from looking at the file. We select a language, pick out some runtime versions to build on, select a solution, grab some dependencies, run the build and run some tests. commit some code and the build starts. Simple and powerful, as it should be.</p>\n\n<p>Personally, I prefer to add a <code>build.sh</code> file to all my project, which makes it all even easier:  </p>\n\n<pre><code>language: csharp\nmono:\n  - latest\n  - 3.12.0\n  - 3.10.0\nbefore_script:\n    - chmod +x build.sh\nscript:\n  - ./build.sh compile\n  - ./build.sh test\n</code></pre>\n\n<p>Normally Travis CI uses <a href=\"http://docs.travis-ci.com/user/migrating-from-legacy/#How-can-I-use-container-based-infrastructure%3F\">container technology to spawn these builds fast</a>, but there is nothing wrong with using containers for yourself in the build:</p>\n\n<pre><code>language: csharp\nsudo: required\nservices:\n  - docker\nbefore_script:\n  - chmod +x buildServer\n  - ./build.sh build-local\n  - ./build.sh build-docker\n  - ./build.sh run-docker\nscript:\n  - ./build.sh unit-local\n  - ./build.sh integration-local\n</code></pre>\n\n<p>Now this solution I like a lot. Here we allow the use of <a href=\"https://www.docker.com/\">Docker</a> thanks to the <code>sudo: required</code> and the <code>-docker</code> service. Next up we run a regular compile-build both directly on the host and in a Docker-container, before starting this container. The container allows us to run integration-tests against newly built code fast, without the need to deploy it on another server, making the build more efficient and keeps complexity low. The build test the entire solution from compile to deploy, all in a few lines of code.  </p>","amp":null,"image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-10-20 20:22:10","created_by":1,"updated_at":"2015-10-20 20:27:27","updated_by":1,"published_at":"2015-10-20 20:23:32","published_by":1},{"id":14,"uuid":"35ef7d68-8c7e-4b3b-9c2e-6d7504274554","title":"Trondheim Developer Conference 2015","slug":"trondheim-developer-conference","markdown":"\nThis year I had the pleasure of being one of the talkers at the fourth annual [TDC](http://2015.trondheimdc.no/) in Trondhiem.\nThe lineup included big international tech-names like [Scott Hanselman](http://www.hanselman.com/), [Sahil Malik](http://blah.winsmarts.com/), [Seb Lee-Delisle](http://seb.ly/) and [Scott Allen](http://odetocode.com/about/scott-allen), and I had a lot of fun making my debut as a conference speaker along side these people.\n\nThe talk i brought was **Simple crossplatform REST-Service with .NET, Vagrant and Docker**, a walkthrough on how to make crossplatform server components in .NET with [Vagrant](https://www.vagrantup.com/) and [Docker](https://www.docker.com/) as key tools, helping us focus on integration testing and production-like deployment from the first lines of code written. This is also a subject I have [blogged](http://blog.amosti.net/build-test-and-deploy-net-apps-with-vagrant-and-docker/) some about before. The essence of the talk is that .NET and C# now is all you need to know to write the entire stack of your applications, including the mobile client code for iOS and Android via [Xamarin](https://xamarin.com/) to the server side part with [Mono](http://www.mono-project.com/), letting you choose what platform to run on.\n\n![](http://i.imgur.com/bJeyynv.jpg)\n\nI gave some tips on frameworks and libraries to use when writing a simple REST-Service, including [NancyFX](http://nancyfx.org/), [Topshelf](http://topshelf-project.com/) and [Dapper](https://github.com/StackExchange/dapper-dot-net).\n\nMy session will be released by the TDC people shortly, until then - here are the slides.\n\n<script async class=\"speakerdeck-embed\" data-id=\"3191aeafb0bf493b8be90abe01639bce\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nEDIT: Here is also the talk (in norwegian): \n\n<iframe src=\"https://player.vimeo.com/video/144964559\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n\nAfter the talk I walked back stage and helped Scott Hanselman out with his upcoming [ASP.NET 5](http://www.asp.net/vnext) talk, where he showed me the future of open source Microsoft, running on all platforms without anything installed thanks to the [CoreCLR](https://github.com/dotnet/coreclr). He told me he planned on asking some random people in the audience for a Mac to show how easy it was to run a sample ASP.NET 5 app with the CoreCLR, so I placed my co-worker [@hjerpbakk](http://hjerpbakk.com/) on the front raw. Sure enough, he had to take the stage with his Mac.\n\n![](http://i.imgur.com/6Ba2BF7.jpg)\n\nThe tricky bit proved to not be getting asp.net to run on a Mac, but tackling the Norwegian keyboard. Hanselman joked as always, and his sessions ended up, not surprisingly, to be some of the best the entire conference had to offer.   ","mobiledoc":null,"html":"<p>This year I had the pleasure of being one of the talkers at the fourth annual <a href=\"http://2015.trondheimdc.no/\">TDC</a> in Trondhiem. <br />\nThe lineup included big international tech-names like <a href=\"http://www.hanselman.com/\">Scott Hanselman</a>, <a href=\"http://blah.winsmarts.com/\">Sahil Malik</a>, <a href=\"http://seb.ly/\">Seb Lee-Delisle</a> and <a href=\"http://odetocode.com/about/scott-allen\">Scott Allen</a>, and I had a lot of fun making my debut as a conference speaker along side these people.</p>\n\n<p>The talk i brought was <strong>Simple crossplatform REST-Service with .NET, Vagrant and Docker</strong>, a walkthrough on how to make crossplatform server components in .NET with <a href=\"https://www.vagrantup.com/\">Vagrant</a> and <a href=\"https://www.docker.com/\">Docker</a> as key tools, helping us focus on integration testing and production-like deployment from the first lines of code written. This is also a subject I have <a href=\"http://blog.amosti.net/build-test-and-deploy-net-apps-with-vagrant-and-docker/\">blogged</a> some about before. The essence of the talk is that .NET and C# now is all you need to know to write the entire stack of your applications, including the mobile client code for iOS and Android via <a href=\"https://xamarin.com/\">Xamarin</a> to the server side part with <a href=\"http://www.mono-project.com/\">Mono</a>, letting you choose what platform to run on.</p>\n\n<p><img src=\"http://i.imgur.com/bJeyynv.jpg\" alt=\"\" /></p>\n\n<p>I gave some tips on frameworks and libraries to use when writing a simple REST-Service, including <a href=\"http://nancyfx.org/\">NancyFX</a>, <a href=\"http://topshelf-project.com/\">Topshelf</a> and <a href=\"https://github.com/StackExchange/dapper-dot-net\">Dapper</a>.</p>\n\n<p>My session will be released by the TDC people shortly, until then - here are the slides.</p>\n\n<script async class=\"speakerdeck-embed\" data-id=\"3191aeafb0bf493b8be90abe01639bce\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n<p>EDIT: Here is also the talk (in norwegian): </p>\n\n<iframe src=\"https://player.vimeo.com/video/144964559\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n<p>After the talk I walked back stage and helped Scott Hanselman out with his upcoming <a href=\"http://www.asp.net/vnext\">ASP.NET 5</a> talk, where he showed me the future of open source Microsoft, running on all platforms without anything installed thanks to the <a href=\"https://github.com/dotnet/coreclr\">CoreCLR</a>. He told me he planned on asking some random people in the audience for a Mac to show how easy it was to run a sample ASP.NET 5 app with the CoreCLR, so I placed my co-worker <a href=\"http://hjerpbakk.com/\">@hjerpbakk</a> on the front raw. Sure enough, he had to take the stage with his Mac.</p>\n\n<p><img src=\"http://i.imgur.com/6Ba2BF7.jpg\" alt=\"\" /></p>\n\n<p>The tricky bit proved to not be getting asp.net to run on a Mac, but tackling the Norwegian keyboard. Hanselman joked as always, and his sessions ended up, not surprisingly, to be some of the best the entire conference had to offer.   </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-10-31 16:00:50","created_by":1,"updated_at":"2015-11-24 14:12:09","updated_by":1,"published_at":"2015-10-31 16:01:59","published_by":1},{"id":15,"uuid":"fb35d433-6955-4fdf-9e1a-a92b161f374f","title":"Dumbing down the build server","slug":"dumbing-down-the-build-server","markdown":"### Motivation\nA common mistake I see many do (yes, i call it *mistake* ) is a build system too tightly linked with the build server they end up choosing. We made that mistake ourself. Being a typical .NET shop, we started out with [Microsoft Team Foundation server](https://www.visualstudio.com/en-us/products/tfs-overview-vs.aspx) back in 2007. Building with TFS in those days relied heavily on workflow scripts and build templates that had to be compiled and could *only* be run on the build server. The result? We could not build our code in exactly the same way locally as we did on the build server. As long as the codebase is small this might be OK, but when you start getting integration tests deep down failing on the server and not locally you are in trouble. Another clear problem we found with the TFS approach was how people treated the build server: like a black box. Of all the developers working in the company, only 4 knew how the build system was working. The whole thing got far too complex and switching over to something like [Team City](https://www.jetbrains.com/teamcity/) would force us to make the entire build system from scratch. We tried this too - and without thinking about it ended up using the [Team City \nenvironment variabes](https://confluence.jetbrains.com/display/TCD9/Predefined+Build+Parameters) far too much. The result? The new build system also got tied strongly against Team City. If you ever find yourself in some of these situation, press the big red button and think.\n\n### What is a build server? \nA build server is nothing more than a *worker*. You send it tasks to do, and it does it. Simple as that. Hell, I don't like the words **build server** or **CI server**, it makes it sound more complex than it really is. The important part is not the build server but the *build system*. With a good build system the server itself can be dumbed down to just running i cron job, nothing more. The key point here is to not use all the fancy configuration options the build server has - that should be a part of the build system itself. With this in mind - let's do it right.\n\n### Do it right \nWhen we started out working with the new build system, we had some clear rules to follow: \n\n* The system must do *exactly*  the same things locally and on the server.\n* The system should be made in an easy to understand scripting language.\n* Each team owns its own build, but common code should be shared.\n* Use as much [Vanilla](https://en.wikipedia.org/wiki/Vanilla_software) methods as possible.\n* The system should be simple without too much lock-in to the build server we choose to use. \n\nBuilding for .NET has become much better the recent years. Where we before called MSBuild directly from PowerShell we now have [psake](https://github.com/psake/psake) inspired by Ruby's  [rake](https://github.com/ruby/rake). We have some mixed experience with Powershell so we early turned this down. If you don't mind Powershell, psake seems to be a good choice.\n\nNext up we looked at [FAKE](https://fsharp.github.io/FAKE/), a popular buildsystem written in F#. NRK has [written about switching to FAKE](https://nrkbeta.no/2015/11/10/how-i-learned-to-stop-worrying-and-love-the-ci-server/). Some teams ended up using FAKE on some independent modules, but we ended up not using it because of the learning curve F# would bring to most of our developers. \n\nThanks to Microsofts [Roslyn compiler](https://roslyn.codeplex.com/) we discovered [ScriptCS](http://scriptcs.net/). ScriptCS let's you use C# as a scripting language, offering REPL and everything else you would expect from a scripting language. Since the language is C# every developer working can read and modify the scripts, no learning needed. Perfect. ScriptCS can also reference DLLs from GAC or a binary folder. \n\nWe ended up creating a `tools\\` folder with a `build.csx` and a `common.csx` file. The `common.csx` file is centrally shared in source control and acts like our library with all methods needed for building tasks, like MSBuild, file management and calls to the NuGet API and triggers for [Octopus Deploy](https://octopus.com/). The `build.csx`  file is the build file itself specified for the solution(s) to build, using methods from `common.csx`. The build system itself is distributed as a NuGet package to be installed installed to the root of the project we want to build. A `build.bat` file wrapps the build system and can be triggered like so:\n\t\t\n\t./build.bat clean init compile unit integration package deploy\n\nThis flow cleans, pulls down dependencies, compiles, runs unit- and integrationtests, packages NuGet or Chocolatey packages and deploys to ProGet and Octopus Deploy. These steps are exactly alike locally and on the build server, just a command line call to build.bat. If ScriptCS is not installed on the system, the bat-file will do it for you. \n\n### Conclusion\n\nIt took some time, but after a while we had a build system we were very satisfied with. The shared code is constantly changing as teams finds new needs. ScriptCS has been a success and have removed a lot of the magic from the build process. The teams have taken a lot more ownership over building than they had before. By pushing logic from the build server and down to the build system itself, everything works perfectly with Team City, TFS2015 and Jenkins, just as it should be. \n\n### Note\n\nIf you don't want to write a ScriptCS-based build system from scratch, take a look at Cake. Cake is pretty much a ScriptCS based framework for building with lots of extension possibilities.","mobiledoc":null,"html":"<h3 id=\"motivation\">Motivation</h3>\n\n<p>A common mistake I see many do (yes, i call it <em>mistake</em> ) is a build system too tightly linked with the build server they end up choosing. We made that mistake ourself. Being a typical .NET shop, we started out with <a href=\"https://www.visualstudio.com/en-us/products/tfs-overview-vs.aspx\">Microsoft Team Foundation server</a> back in 2007. Building with TFS in those days relied heavily on workflow scripts and build templates that had to be compiled and could <em>only</em> be run on the build server. The result? We could not build our code in exactly the same way locally as we did on the build server. As long as the codebase is small this might be OK, but when you start getting integration tests deep down failing on the server and not locally you are in trouble. Another clear problem we found with the TFS approach was how people treated the build server: like a black box. Of all the developers working in the company, only 4 knew how the build system was working. The whole thing got far too complex and switching over to something like <a href=\"https://www.jetbrains.com/teamcity/\">Team City</a> would force us to make the entire build system from scratch. We tried this too - and without thinking about it ended up using the <a href=\"https://confluence.jetbrains.com/display/TCD9/Predefined+Build+Parameters\">Team City <br />\nenvironment variabes</a> far too much. The result? The new build system also got tied strongly against Team City. If you ever find yourself in some of these situation, press the big red button and think.</p>\n\n<h3 id=\"whatisabuildserver\">What is a build server?</h3>\n\n<p>A build server is nothing more than a <em>worker</em>. You send it tasks to do, and it does it. Simple as that. Hell, I don't like the words <strong>build server</strong> or <strong>CI server</strong>, it makes it sound more complex than it really is. The important part is not the build server but the <em>build system</em>. With a good build system the server itself can be dumbed down to just running i cron job, nothing more. The key point here is to not use all the fancy configuration options the build server has - that should be a part of the build system itself. With this in mind - let's do it right.</p>\n\n<h3 id=\"doitright\">Do it right</h3>\n\n<p>When we started out working with the new build system, we had some clear rules to follow: </p>\n\n<ul>\n<li>The system must do <em>exactly</em>  the same things locally and on the server.</li>\n<li>The system should be made in an easy to understand scripting language.</li>\n<li>Each team owns its own build, but common code should be shared.</li>\n<li>Use as much <a href=\"https://en.wikipedia.org/wiki/Vanilla_software\">Vanilla</a> methods as possible.</li>\n<li>The system should be simple without too much lock-in to the build server we choose to use. </li>\n</ul>\n\n<p>Building for .NET has become much better the recent years. Where we before called MSBuild directly from PowerShell we now have <a href=\"https://github.com/psake/psake\">psake</a> inspired by Ruby's  <a href=\"https://github.com/ruby/rake\">rake</a>. We have some mixed experience with Powershell so we early turned this down. If you don't mind Powershell, psake seems to be a good choice.</p>\n\n<p>Next up we looked at <a href=\"https://fsharp.github.io/FAKE/\">FAKE</a>, a popular buildsystem written in F#. NRK has <a href=\"https://nrkbeta.no/2015/11/10/how-i-learned-to-stop-worrying-and-love-the-ci-server/\">written about switching to FAKE</a>. Some teams ended up using FAKE on some independent modules, but we ended up not using it because of the learning curve F# would bring to most of our developers. </p>\n\n<p>Thanks to Microsofts <a href=\"https://roslyn.codeplex.com/\">Roslyn compiler</a> we discovered <a href=\"http://scriptcs.net/\">ScriptCS</a>. ScriptCS let's you use C# as a scripting language, offering REPL and everything else you would expect from a scripting language. Since the language is C# every developer working can read and modify the scripts, no learning needed. Perfect. ScriptCS can also reference DLLs from GAC or a binary folder. </p>\n\n<p>We ended up creating a <code>tools\\</code> folder with a <code>build.csx</code> and a <code>common.csx</code> file. The <code>common.csx</code> file is centrally shared in source control and acts like our library with all methods needed for building tasks, like MSBuild, file management and calls to the NuGet API and triggers for <a href=\"https://octopus.com/\">Octopus Deploy</a>. The <code>build.csx</code>  file is the build file itself specified for the solution(s) to build, using methods from <code>common.csx</code>. The build system itself is distributed as a NuGet package to be installed installed to the root of the project we want to build. A <code>build.bat</code> file wrapps the build system and can be triggered like so:</p>\n\n<pre><code>./build.bat clean init compile unit integration package deploy\n</code></pre>\n\n<p>This flow cleans, pulls down dependencies, compiles, runs unit- and integrationtests, packages NuGet or Chocolatey packages and deploys to ProGet and Octopus Deploy. These steps are exactly alike locally and on the build server, just a command line call to build.bat. If ScriptCS is not installed on the system, the bat-file will do it for you. </p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>It took some time, but after a while we had a build system we were very satisfied with. The shared code is constantly changing as teams finds new needs. ScriptCS has been a success and have removed a lot of the magic from the build process. The teams have taken a lot more ownership over building than they had before. By pushing logic from the build server and down to the build system itself, everything works perfectly with Team City, TFS2015 and Jenkins, just as it should be. </p>\n\n<h3 id=\"note\">Note</h3>\n\n<p>If you don't want to write a ScriptCS-based build system from scratch, take a look at Cake. Cake is pretty much a ScriptCS based framework for building with lots of extension possibilities.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-01-02 12:31:27","created_by":1,"updated_at":"2016-02-14 09:32:48","updated_by":1,"published_at":"2016-01-02 12:32:42","published_by":1},{"id":16,"uuid":"3e7b9961-f2b1-4f09-bba9-17206e7055a4","title":"We created a tech blog!","slug":"we-created-a-tech-blog","markdown":"At [DIPS](https://www.dips.com/) Trondheim, we like to think about new ways to connect with developers and giving back to the community. A lot of us do Open Source development and attend meetups on a regular basis. We talk at [conferences](http://blog.amosti.net/trondheim-developer-conference/) and [universities](http://www.slideshare.net/hjerpbakk/kryssplatform-mobilutvikling-i-c-vha-xamarinforms). As geeks we also read blogs. A lot of blogs. Being Norwegian developers, we found that the amount of people and companies in the tech scene actually *blogging* in Norwegian was rather slim, so we decided to do something about it. DIPS ASA has over 150 developers, making us one of Norway's largest software houses. It's time for us to share some of our knowledge. That's how the [DIPS Tech Blog](http://dipsasa.github.io) was born. On this blog we will share tips and tricks from our every-day work, including solid patterns and practices, libraries and tools we use, how we design products and so on. Everything will be in Norwegian as well. We hope you like it!   \n\nVisit us at [tech.dips.no](http://dipsasa.github.io)! ","mobiledoc":null,"html":"<p>At <a href=\"https://www.dips.com/\">DIPS</a> Trondheim, we like to think about new ways to connect with developers and giving back to the community. A lot of us do Open Source development and attend meetups on a regular basis. We talk at <a href=\"http://blog.amosti.net/trondheim-developer-conference/\">conferences</a> and <a href=\"http://www.slideshare.net/hjerpbakk/kryssplatform-mobilutvikling-i-c-vha-xamarinforms\">universities</a>. As geeks we also read blogs. A lot of blogs. Being Norwegian developers, we found that the amount of people and companies in the tech scene actually <em>blogging</em> in Norwegian was rather slim, so we decided to do something about it. DIPS ASA has over 150 developers, making us one of Norway's largest software houses. It's time for us to share some of our knowledge. That's how the <a href=\"http://dipsasa.github.io\">DIPS Tech Blog</a> was born. On this blog we will share tips and tricks from our every-day work, including solid patterns and practices, libraries and tools we use, how we design products and so on. Everything will be in Norwegian as well. We hope you like it!   </p>\n\n<p>Visit us at <a href=\"http://dipsasa.github.io\">tech.dips.no</a>! </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-24 11:58:30","created_by":1,"updated_at":"2016-03-24 21:25:25","updated_by":1,"published_at":"2016-03-24 12:00:06","published_by":1},{"id":17,"uuid":"eaa33f8c-9176-4d5c-95a9-2e40591f73db","title":"DockerCon 2016: Docker nails iterative open source development","slug":"dockercon-2016-docker-nails-iterative-open-source-development","markdown":"Yesterday [DockerCon 2016](http://2016.dockercon.com/) kicked off from rainy Seattle with a brilliant keynote lead by CEO [Ben Golub](https://twitter.com/golubbe) and CTO [Solomon Hykes](https://twitter.com/solomonstre). Hykes talked about how Docker's goal is to make [tools of mass innovation](https://www.youtube.com/watch?v=apOEYhmskvQ), to remove as much friction as possible from the development workflow. One such example is [Docker for Mac and Windows](https://blog.docker.com/2016/03/docker-for-mac-windows-beta/). [Anand Prasad](https://twitter.com/AanandPrasad) came on stage as an \"First day on the job\" developer to demonstrate how Docker for Mac could help him get up and running, debugging and committing code - 10 minutes in on the new job.\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Live de-bugging inside a container with <a href=\"https://twitter.com/docker\">@docker</a> for Mac and Windows by <a href=\"https://twitter.com/AanandPrasad\">@AanandPrasad</a> <a href=\"https://twitter.com/hashtag/dockercon?src=hash\">#dockercon</a> <a href=\"https://t.co/zAUvVUOEc5\">pic.twitter.com/zAUvVUOEc5</a></p>&mdash; Betty Junod (@BettyJunod) <a href=\"https://twitter.com/BettyJunod/status/744935919673708544\">June 20, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n>As a beta user of Docker for Mac, I am really happy with how simple Docker has made this use case. With bringing Native Docker (or at least [close to native](https://blog.docker.com/2016/03/docker-for-mac-windows-beta/)) I have been able to bypass the Vagrant machine I have been using for running Docker - saving me a lot of overhead. Docker for Mac also gives me the option to run apps and throw them away when done. Great for my laptop. No more clutter with different MySQL databases in the same instance, just run the entire database in it's own container.\n\nThe big \"wow\" experience came with the introduction of the [new built-in orchestration](https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/). It is now possible to add hosts to a cluster and deploy containers to this cluster - with as little as 3 CLI commands, with just the Docker-Engine installed. Switch out the familiar ``run`` command with ``service`` and you are all set. This cluster is secure, self-healing and load balanced out of the box. This is in many ways what [Docker Swarm](https://docs.docker.com/swarm/overview/) should have been from the start. The new orchestration tool is also a great example of the mantra to remove friction from the development process. Docker has become really good at fixing the really hard parts and hiding them behind a simple CLI.\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">on stage: <a href=\"https://twitter.com/mikegoelzer\">@mikegoelzer</a> and <a href=\"https://twitter.com/aluzzardi\">@aluzzardi</a> launching service live with <a href=\"https://twitter.com/docker\">@docker</a> 1.12 <a href=\"https://twitter.com/hashtag/DockerCon?src=hash\">#DockerCon</a> <a href=\"https://t.co/dvItYBDeI0\">pic.twitter.com/dvItYBDeI0</a></p>&mdash; Docker (@docker) <a href=\"https://twitter.com/docker/status/744943959177199616\">June 20, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\nWhat the keynote shows is how good Docker is at doing Iterative Open Source development out in the open. When Docker first released it was a much simpler way to use ``namespacing`` and ``cgroups`` to isolate processes from each other. Great for testing and Linux based development, making it possible to set up a reproducible application. With the containers (or building blocks) came the need for orchestration. What use is a hundred containerized apps if it is a pain to orchestrate them? Since Docker has been Open Source from day one, many problems has been addressed by others. The [Fig Project](http://www.fig.sh/) (now Docker Compose) solved orchestration on the development level. Docker [acquired the team behind Fig](http://www.informationweek.com/cloud/infrastructure-as-a-service/docker-acquires-devops-flavor-with-fig/d/d-id/1297523) and put them in charge for development of Docker Compose.\n\nNext up came the split of the Docker application into the Docker Engine, the Client and the Machine. The Engine is the part that runs Docker, the client talks to it via the CLI. Docker Machine lets you install the Docker-Engine on multiple hosts and controls them with the client, making deployment on remote hosts much easier. The final product to come out of last year's DockerCon was Docker Swarm. Swarm enables multiple Docker Machines to function together as a cluster.\n\nThese products are results of the Docker iteration process. By attacking different challenges one at the time, Docker gives us tools that are adapted to specific needs. These tools can be built upon in the next iteration to tackle another challenge by another group of people. There would not have been a Docker-Engine or Docker-Compose without Docker itself. Docker Machine could not have been made without the Docker Engine. No Machine, no Swarm. Without Swarm? Certainly no [Docker-Cloud](https://www.docker.com/products/docker-cloud).\n","mobiledoc":null,"html":"<p>Yesterday <a href=\"http://2016.dockercon.com/\">DockerCon 2016</a> kicked off from rainy Seattle with a brilliant keynote lead by CEO <a href=\"https://twitter.com/golubbe\">Ben Golub</a> and CTO <a href=\"https://twitter.com/solomonstre\">Solomon Hykes</a>. Hykes talked about how Docker's goal is to make <a href=\"https://www.youtube.com/watch?v=apOEYhmskvQ\">tools of mass innovation</a>, to remove as much friction as possible from the development workflow. One such example is <a href=\"https://blog.docker.com/2016/03/docker-for-mac-windows-beta/\">Docker for Mac and Windows</a>. <a href=\"https://twitter.com/AanandPrasad\">Anand Prasad</a> came on stage as an \"First day on the job\" developer to demonstrate how Docker for Mac could help him get up and running, debugging and committing code - 10 minutes in on the new job.  </p>\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Live de-bugging inside a container with <a href=\"https://twitter.com/docker\">@docker</a> for Mac and Windows by <a href=\"https://twitter.com/AanandPrasad\">@AanandPrasad</a> <a href=\"https://twitter.com/hashtag/dockercon?src=hash\">#dockercon</a> <a href=\"https://t.co/zAUvVUOEc5\">pic.twitter.com/zAUvVUOEc5</a></p>&mdash; Betty Junod (@BettyJunod) <a href=\"https://twitter.com/BettyJunod/status/744935919673708544\">June 20, 2016</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<blockquote>\n  <p>As a beta user of Docker for Mac, I am really happy with how simple Docker has made this use case. With bringing Native Docker (or at least <a href=\"https://blog.docker.com/2016/03/docker-for-mac-windows-beta/\">close to native</a>) I have been able to bypass the Vagrant machine I have been using for running Docker - saving me a lot of overhead. Docker for Mac also gives me the option to run apps and throw them away when done. Great for my laptop. No more clutter with different MySQL databases in the same instance, just run the entire database in it's own container.</p>\n</blockquote>\n\n<p>The big \"wow\" experience came with the introduction of the <a href=\"https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/\">new built-in orchestration</a>. It is now possible to add hosts to a cluster and deploy containers to this cluster - with as little as 3 CLI commands, with just the Docker-Engine installed. Switch out the familiar <code>run</code> command with <code>service</code> and you are all set. This cluster is secure, self-healing and load balanced out of the box. This is in many ways what <a href=\"https://docs.docker.com/swarm/overview/\">Docker Swarm</a> should have been from the start. The new orchestration tool is also a great example of the mantra to remove friction from the development process. Docker has become really good at fixing the really hard parts and hiding them behind a simple CLI.  </p>\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">on stage: <a href=\"https://twitter.com/mikegoelzer\">@mikegoelzer</a> and <a href=\"https://twitter.com/aluzzardi\">@aluzzardi</a> launching service live with <a href=\"https://twitter.com/docker\">@docker</a> 1.12 <a href=\"https://twitter.com/hashtag/DockerCon?src=hash\">#DockerCon</a> <a href=\"https://t.co/dvItYBDeI0\">pic.twitter.com/dvItYBDeI0</a></p>&mdash; Docker (@docker) <a href=\"https://twitter.com/docker/status/744943959177199616\">June 20, 2016</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>What the keynote shows is how good Docker is at doing Iterative Open Source development out in the open. When Docker first released it was a much simpler way to use <code>namespacing</code> and <code>cgroups</code> to isolate processes from each other. Great for testing and Linux based development, making it possible to set up a reproducible application. With the containers (or building blocks) came the need for orchestration. What use is a hundred containerized apps if it is a pain to orchestrate them? Since Docker has been Open Source from day one, many problems has been addressed by others. The <a href=\"http://www.fig.sh/\">Fig Project</a> (now Docker Compose) solved orchestration on the development level. Docker <a href=\"http://www.informationweek.com/cloud/infrastructure-as-a-service/docker-acquires-devops-flavor-with-fig/d/d-id/1297523\">acquired the team behind Fig</a> and put them in charge for development of Docker Compose.</p>\n\n<p>Next up came the split of the Docker application into the Docker Engine, the Client and the Machine. The Engine is the part that runs Docker, the client talks to it via the CLI. Docker Machine lets you install the Docker-Engine on multiple hosts and controls them with the client, making deployment on remote hosts much easier. The final product to come out of last year's DockerCon was Docker Swarm. Swarm enables multiple Docker Machines to function together as a cluster.</p>\n\n<p>These products are results of the Docker iteration process. By attacking different challenges one at the time, Docker gives us tools that are adapted to specific needs. These tools can be built upon in the next iteration to tackle another challenge by another group of people. There would not have been a Docker-Engine or Docker-Compose without Docker itself. Docker Machine could not have been made without the Docker Engine. No Machine, no Swarm. Without Swarm? Certainly no <a href=\"https://www.docker.com/products/docker-cloud\">Docker-Cloud</a>.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-21 10:51:52","created_by":1,"updated_at":"2016-06-21 10:58:28","updated_by":1,"published_at":"2016-06-21 10:52:47","published_by":1},{"id":18,"uuid":"5bbd4d49-2056-44c9-bc35-0700cb1e8dfc","title":"Spawning TFS2015 Build Agents with Docker","slug":"spawning-tfs2015-build-agents-with-docker","markdown":"The Microsoft Open Source train keeps rolling fast forward with more and more projects popping up on GitHub. When we upgraded to TFS2015 late last year, the support for Linux build agents was so-so. \nA couple of weeks ago I decided to check in again. \n\nTo my delight I discovered that the \"old\" [build agent had been depricated](https://github.com/Microsoft/vso-agent) and the [new one](https://github.com/Microsoft/vsts-agent) has been implementet using [dotnet core](https://www.microsoft.com/net/core#macos).\nThat means cross-platform awesomeness!\n\nIt is now easy to install the agent on OSX, Ubuntu, the Red Hat family and so on.\nThat makes TFS much more accessible for people building other stuff than .NET. TFS has become a great product over the years, so this realy neat.\n\nThis also means that Docker can be used to spawn build agents on the fly. Somethimes we don't need to have build agents standing idle hours on end. Docker is great for spawning the agent when it actually is needed. It also allows us to install the relevant build-environment for each case. If you want to build Java, we can create a build agent with Java. Or Node. Or Mono.\n\nThe following ``Dockerfile`` gives you the idea:\n\n```\nFROM java\n\nRUN useradd -ms /bin/bash builder\nWORKDIR /home/builder\n\nRUN apt-get update && apt-get install -y libunwind8 libcurl3 libicu52 && apt-get install wget\n\nRUN mkdir buildAgent && cd buildAgent\nRUN wget https://github.com/Microsoft/vsts-agent/releases/download/v2.107.0/vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz\nRUN tar xzf vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz\n\nUSER builder\nCMD ./config.sh && ./run.sh\n```\n\nThe `Dockerfile` itself installs the build agent and starts an interactive configuration before starting the agent. The neat part is the `FROM` keyword. In this example, I want to build some Java code, so I base the file on the Java image.\n\nAnother tip is the scriptable setup of the agent. The `config.sh` script can be triggered noninteractive:\n\n```\ndocker run -it andmos/vsts-agent ./config.sh --unattended --acceptteeeula --url http://mylocaltfsserver:8080/tfs --auth Negotiate --username DOMAIN\\USER_NAME --password MyPassword --pool default --agent myagent && ./run.sh\n```","mobiledoc":null,"html":"<p>The Microsoft Open Source train keeps rolling fast forward with more and more projects popping up on GitHub. When we upgraded to TFS2015 late last year, the support for Linux build agents was so-so. <br />\nA couple of weeks ago I decided to check in again. </p>\n\n<p>To my delight I discovered that the \"old\" <a href=\"https://github.com/Microsoft/vso-agent\">build agent had been depricated</a> and the <a href=\"https://github.com/Microsoft/vsts-agent\">new one</a> has been implementet using <a href=\"https://www.microsoft.com/net/core#macos\">dotnet core</a>. <br />\nThat means cross-platform awesomeness!</p>\n\n<p>It is now easy to install the agent on OSX, Ubuntu, the Red Hat family and so on. <br />\nThat makes TFS much more accessible for people building other stuff than .NET. TFS has become a great product over the years, so this realy neat.</p>\n\n<p>This also means that Docker can be used to spawn build agents on the fly. Somethimes we don't need to have build agents standing idle hours on end. Docker is great for spawning the agent when it actually is needed. It also allows us to install the relevant build-environment for each case. If you want to build Java, we can create a build agent with Java. Or Node. Or Mono.</p>\n\n<p>The following <code>Dockerfile</code> gives you the idea:</p>\n\n<pre><code>FROM java\n\nRUN useradd -ms /bin/bash builder  \nWORKDIR /home/builder\n\nRUN apt-get update &amp;&amp; apt-get install -y libunwind8 libcurl3 libicu52 &amp;&amp; apt-get install wget\n\nRUN mkdir buildAgent &amp;&amp; cd buildAgent  \nRUN wget https://github.com/Microsoft/vsts-agent/releases/download/v2.107.0/vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz  \nRUN tar xzf vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz\n\nUSER builder  \nCMD ./config.sh &amp;&amp; ./run.sh  \n</code></pre>\n\n<p>The <code>Dockerfile</code> itself installs the build agent and starts an interactive configuration before starting the agent. The neat part is the <code>FROM</code> keyword. In this example, I want to build some Java code, so I base the file on the Java image.</p>\n\n<p>Another tip is the scriptable setup of the agent. The <code>config.sh</code> script can be triggered noninteractive:</p>\n\n<pre><code>docker run -it andmos/vsts-agent ./config.sh --unattended --acceptteeeula --url http://mylocaltfsserver:8080/tfs --auth Negotiate --username DOMAIN\\USER_NAME --password MyPassword --pool default --agent myagent &amp;&amp; ./run.sh  \n</code></pre>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-09-24 12:47:39","created_by":1,"updated_at":"2016-09-24 12:52:40","updated_by":1,"published_at":"2016-09-24 12:48:56","published_by":1},{"id":19,"uuid":"ca71c7aa-a0dc-4cb6-8486-895019f44f2e","title":"Rolling out web services with Topshelf, Chocolatey and Ansible","slug":"rolling-out-web-services-with-topshelf-chocolatey-and-ansible","markdown":"I have been doing a lot of automation on the Windows platform at work lately. That sentence would have been associated with pain some years ago - but things have changed. [Ansible](https://www.ansible.com/) is nothing less than the perfect provisioning tool for both Linux and Windows, providing (in my opinion) the best level of abstraction when managing systems and state. A lot of modules are there also for Windows, so you have to put minimal of effort in the details of the provisioning steps, making it easy for people who are not developers or scripting guys to catch the gist of it. The next tool that has changed the game is [Chocolatey](https://chocolatey.org/). One of the traditional advantages Linux has had  over the Windows platform has been package managers - or a standardized interface for finding and installing software. Chocolatey is is an ambitious attempt at the same thing for Windows. Scott Hanselman [wrote about it ](http://www.hanselman.com/blog/IsTheWindowsUserReadyForAptget.aspx) back in 2013, and since then the project has catched on and kept growing. At DIPS we [package our software with Chocolatey](http://tech.dips.no/2016/08/17/Deployment-av-servere.html), giving us more scriptable flexibility over the old MSI regime. Chocolatey shines on it's own, but combined with Ansible it is _pure_ magic. Ansible provides a [Chocolatey module](https://docs.ansible.com/ansible/win_chocolatey_module.html) that let's us install Chocolatey packages as a part of the provisioning. Take the following examples from our Playbooks:\n\n\n\n``` python\n - name: Install DotNet Framework 4.6.1\n   win_chocolatey: name=dotnet4.6.1\n```\n\n```python\n- name: Install Octopus Tentacle Files\n  win_chocolatey: name=octopusdeploy.tentacle\n```\n\n```python\n- name: Install Java JDK 8\n  win_chocolatey: name=jdk8\n```\n\n\n\nYou catch the point. Almost looks like `apt-get` right there.\n\nNext up I like to show how a HTTP service can be rolled out with Ansible and Chocolatey. Since we roll out our own software as Chocolatey packages in a Continuous Delivery pipeline, the need to monitor exactly which packages are deployed to a given server at _this time_ came up. To deal with it I wrote [Stratos](https://github.com/andmos/Stratos), a simple HTTP API to report what Chocolatey packages are installed on the server. A `GET` on `/api/chocoPackages` will returns some JSON:\n\n```json\n[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 3,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.5.2\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 5,\n        \"build\": 2,\n        \"revision\": 20140902,\n        \"majorRevision\": 307,\n        \"minorRevision\": 21350\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.6.1\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 6,\n        \"build\": 1055,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  }\n]\n```\n\nSimple, but enough to provide data for a simple dashboard.\n\n\n\nStratos is written with [Nancy](http://nancyfx.org/) and [Topshelf](http://topshelf-project.com/). Topshelf is great because it allows you to install Console applications as Windows Services. Together with Nancy's self-hostable package this means that the HTTP service can be deployed without IIS, which is a good thing for simple applications like this one. The main method for Stratos looks like this:\n\n\n\n```c#\nusing Topshelf.Nancy;\nusing Topshelf;\n\nnamespace Stratos\n{\n\tpublic class Program\n\t{\n\t\tstatic void Main(string[] args)\n\t\t{\n\t\t\tvar host = HostFactory.New(x =>\n\t\t\t{\n\t\t\t\tx.UseLinuxIfAvailable();\n\t\t\t\tx.Service<StratosSelfHost>(s =>\n\t\t\t\t{\n\t\t\t\t\ts.ConstructUsing(settings => new StratosSelfHost());\n\t\t\t\t\ts.WhenStarted(service => service.Start());\n\t\t\t\t\ts.WhenStopped(service => service.Stop());\n\t\t\t\t\ts.WithNancyEndpoint(x, c =>\n\t\t\t\t\t{\n\t\t\t\t\t\tc.AddHost(port: 1337);\n\t\t\t\t\t\tc.CreateUrlReservationsOnInstall();\n\t\t\t\t\t\tc.OpenFirewallPortsOnInstall(firewallRuleName: \"StratosService\");\n\t\t\t\t\t});\n\t\t\t\t});\n\n\t\t\t\tx.StartAutomatically();\n\t\t\t\tx.SetServiceName(\"StratosService\");\n\t\t\t\tx.SetDisplayName(\"StratosService\");\n\t\t\t\tx.SetDescription(\"StratosService\");\n\t\t\t\tx.RunAsNetworkService();\n\n\t\t\t});\n\t\t\thost.Run();\n\t\t}\n\t}\n}\n```\n\nAll the configuration in one method. Sweet.\n\n\n\nThe application is packaged up as a Chocolatey package, AKA NuGet with a `ChocolateyInstall.ps1` script for the installation of the service:\n\n\n\n```powershell\nWrite-Host \"Installing Stratos as as windows service...\"\n\ntry {\n\n    $service_name = \"StratosService\"\n    $process_name = \"StratosService\"\n    $serviceFileName = \"Stratos.exe\"\n\n    $PSScriptRoot = Split-Path -parent $MyInvocation.MyCommand.Definition\n    $packageDir = $PSScriptRoot | Split-Path;\n    $srcDir = \"$($PSScriptRoot)\\..\\bin\"\n    $destDir = \"$srcDir\"\n\n    $service = Get-Service | Where-Object {$_.Name -eq $service_name}\n\n    if($service){\n        Stop-Service $service_name\n        $service.WaitForStatus(\"Stopped\")\n        kill -processname $process_name -force -ErrorAction SilentlyContinue\n        Wait-Process -Name $process_name -ErrorAction SilentlyContinue\n        Write-Host \"Uninstalling $service_name...\"\n\n        $fileToUninstall = Join-Path \"$srcDir\\\" $serviceFileName\n        . $fileToUninstall uninstall\n    }\n\n    $fileToInstall = Join-Path \"$destDir\\\" $serviceFileName\n    . $fileToInstall install\n}\ncatch {\n    throw $_.Exception\n}\ntry{\n    . $fileToInstall start\n}\ncatch{\n    Write-Host \"$process_name was successfully installed, but could not be started. This is most likely because of a configuration error. Please check the Windows Event Log.\"\n}\n```\n\nThe next part is to deploy the service. That is the easy part thanks to Ansible:\n\n\n\n```powershell\n- name: Install Stratos Chocolatey service\n  win_chocolatey: name=stratos source=http://dips-nuget/nuget/InternalSoftware state=present upgrade=True\n```\n\nThe `upgrade=True` flag will make sure that any new versions of the service get's rolled out.\n","mobiledoc":null,"html":"<p>I have been doing a lot of automation on the Windows platform at work lately. That sentence would have been associated with pain some years ago - but things have changed. <a href=\"https://www.ansible.com/\">Ansible</a> is nothing less than the perfect provisioning tool for both Linux and Windows, providing (in my opinion) the best level of abstraction when managing systems and state. A lot of modules are there also for Windows, so you have to put minimal of effort in the details of the provisioning steps, making it easy for people who are not developers or scripting guys to catch the gist of it. The next tool that has changed the game is <a href=\"https://chocolatey.org/\">Chocolatey</a>. One of the traditional advantages Linux has had  over the Windows platform has been package managers - or a standardized interface for finding and installing software. Chocolatey is is an ambitious attempt at the same thing for Windows. Scott Hanselman <a href=\"http://www.hanselman.com/blog/IsTheWindowsUserReadyForAptget.aspx\">wrote about it </a> back in 2013, and since then the project has catched on and kept growing. At DIPS we <a href=\"http://tech.dips.no/2016/08/17/Deployment-av-servere.html\">package our software with Chocolatey</a>, giving us more scriptable flexibility over the old MSI regime. Chocolatey shines on it's own, but combined with Ansible it is <em>pure</em> magic. Ansible provides a <a href=\"https://docs.ansible.com/ansible/win_chocolatey_module.html\">Chocolatey module</a> that let's us install Chocolatey packages as a part of the provisioning. Take the following examples from our Playbooks:</p>\n\n<pre><code class=\"language- python\"> - name: Install DotNet Framework 4.6.1\n   win_chocolatey: name=dotnet4.6.1\n</code></pre>\n\n<pre><code class=\"language-python\">- name: Install Octopus Tentacle Files\n  win_chocolatey: name=octopusdeploy.tentacle\n</code></pre>\n\n<pre><code class=\"language-python\">- name: Install Java JDK 8\n  win_chocolatey: name=jdk8\n</code></pre>\n\n<p>You catch the point. Almost looks like <code>apt-get</code> right there.</p>\n\n<p>Next up I like to show how a HTTP service can be rolled out with Ansible and Chocolatey. Since we roll out our own software as Chocolatey packages in a Continuous Delivery pipeline, the need to monitor exactly which packages are deployed to a given server at <em>this time</em> came up. To deal with it I wrote <a href=\"https://github.com/andmos/Stratos\">Stratos</a>, a simple HTTP API to report what Chocolatey packages are installed on the server. A <code>GET</code> on <code>/api/chocoPackages</code> will returns some JSON:</p>\n\n<pre><code class=\"language-json\">[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 3,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.5.2\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 5,\n        \"build\": 2,\n        \"revision\": 20140902,\n        \"majorRevision\": 307,\n        \"minorRevision\": 21350\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.6.1\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 6,\n        \"build\": 1055,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  }\n]\n</code></pre>\n\n<p>Simple, but enough to provide data for a simple dashboard.</p>\n\n<p>Stratos is written with <a href=\"http://nancyfx.org/\">Nancy</a> and <a href=\"http://topshelf-project.com/\">Topshelf</a>. Topshelf is great because it allows you to install Console applications as Windows Services. Together with Nancy's self-hostable package this means that the HTTP service can be deployed without IIS, which is a good thing for simple applications like this one. The main method for Stratos looks like this:</p>\n\n<pre><code class=\"language-c#\">using Topshelf.Nancy;  \nusing Topshelf;\n\nnamespace Stratos  \n{\n    public class Program\n    {\n        static void Main(string[] args)\n        {\n            var host = HostFactory.New(x =&gt;\n            {\n                x.UseLinuxIfAvailable();\n                x.Service&lt;StratosSelfHost&gt;(s =&gt;\n                {\n                    s.ConstructUsing(settings =&gt; new StratosSelfHost());\n                    s.WhenStarted(service =&gt; service.Start());\n                    s.WhenStopped(service =&gt; service.Stop());\n                    s.WithNancyEndpoint(x, c =&gt;\n                    {\n                        c.AddHost(port: 1337);\n                        c.CreateUrlReservationsOnInstall();\n                        c.OpenFirewallPortsOnInstall(firewallRuleName: \"StratosService\");\n                    });\n                });\n\n                x.StartAutomatically();\n                x.SetServiceName(\"StratosService\");\n                x.SetDisplayName(\"StratosService\");\n                x.SetDescription(\"StratosService\");\n                x.RunAsNetworkService();\n\n            });\n            host.Run();\n        }\n    }\n}\n</code></pre>\n\n<p>All the configuration in one method. Sweet.</p>\n\n<p>The application is packaged up as a Chocolatey package, AKA NuGet with a <code>ChocolateyInstall.ps1</code> script for the installation of the service:</p>\n\n<pre><code class=\"language-powershell\">Write-Host \"Installing Stratos as as windows service...\"\n\ntry {\n\n    $service_name = \"StratosService\"\n    $process_name = \"StratosService\"\n    $serviceFileName = \"Stratos.exe\"\n\n    $PSScriptRoot = Split-Path -parent $MyInvocation.MyCommand.Definition\n    $packageDir = $PSScriptRoot | Split-Path;\n    $srcDir = \"$($PSScriptRoot)\\..\\bin\"\n    $destDir = \"$srcDir\"\n\n    $service = Get-Service | Where-Object {$_.Name -eq $service_name}\n\n    if($service){\n        Stop-Service $service_name\n        $service.WaitForStatus(\"Stopped\")\n        kill -processname $process_name -force -ErrorAction SilentlyContinue\n        Wait-Process -Name $process_name -ErrorAction SilentlyContinue\n        Write-Host \"Uninstalling $service_name...\"\n\n        $fileToUninstall = Join-Path \"$srcDir\\\" $serviceFileName\n        . $fileToUninstall uninstall\n    }\n\n    $fileToInstall = Join-Path \"$destDir\\\" $serviceFileName\n    . $fileToInstall install\n}\ncatch {  \n    throw $_.Exception\n}\ntry{  \n    . $fileToInstall start\n}\ncatch{  \n    Write-Host \"$process_name was successfully installed, but could not be started. This is most likely because of a configuration error. Please check the Windows Event Log.\"\n}\n</code></pre>\n\n<p>The next part is to deploy the service. That is the easy part thanks to Ansible:</p>\n\n<pre><code class=\"language-powershell\">- name: Install Stratos Chocolatey service\n  win_chocolatey: name=stratos source=http://dips-nuget/nuget/InternalSoftware state=present upgrade=True\n</code></pre>\n\n<p>The <code>upgrade=True</code> flag will make sure that any new versions of the service get's rolled out.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-11-20 16:34:55","created_by":1,"updated_at":"2016-11-20 16:42:02","updated_by":1,"published_at":"2016-11-20 16:35:37","published_by":1},{"id":20,"uuid":"5aaa023e-2bb0-4824-8ea6-de60a6bd0682","title":"Docker Garbage Collection version 1.13.0 update!","slug":"docker-garbage-collection-version-1-13-0-update","markdown":"Back in 2015 I blogged about how the missing garbage collection in Docker could seriously fill up the drive on the computer you\ndevelop Docker Images on. This was a serioust problem, and the solution (then) was the [Spotify docker-gc\nimage](https://github.com/spotify/docker-gc) image. Now, with the 1.13 release we *finally* got support for just this in the\ndocker-client: `docker system prune`. \n\n`docker system prune` will remove all unused data - that is hanging images (images not used by any existing container) as well as\nold volumes and networks. With the cleanup command in place, what suprises me is how long it took before we finally got this quite\nbasic functionality in the client.  ","mobiledoc":null,"html":"<p>Back in 2015 I blogged about how the missing garbage collection in Docker could seriously fill up the drive on the computer you <br />\ndevelop Docker Images on. This was a serioust problem, and the solution (then) was the <a href=\"https://github.com/spotify/docker-gc\">Spotify docker-gc <br />\nimage</a> image. Now, with the 1.13 release we <em>finally</em> got support for just this in the <br />\ndocker-client: <code>docker system prune</code>. </p>\n\n<p><code>docker system prune</code> will remove all unused data - that is hanging images (images not used by any existing container) as well as\nold volumes and networks. With the cleanup command in place, what suprises me is how long it took before we finally got this quite <br />\nbasic functionality in the client.  </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-15 18:24:40","created_by":1,"updated_at":"2017-06-15 18:25:42","updated_by":1,"published_at":"2017-03-01 18:24:00","published_by":1},{"id":21,"uuid":"2e6c861e-97f2-4a9c-a63b-f84819ed053c","title":"My Favorite Podcasts - 2017 edition","slug":"my-favorite-podcasts-2017-edition-2","markdown":"I am a huge fan of podcasts. They are a great way to learn about new subjects, both from the tech world, science and academia. Here is a list of some of my favorite podcasts so far in 2017.\n\n\n## Tech and Design\n\n### [Arrested DevOps](https://www.arresteddevops.com/)\n\nProbably the leading podcast about the DevOps movement. Matt Stratton (Chef), Trevor Hess (10th Magnitude) and Bridget Kromhout (Pivotal) guides the listener through the ever-changing world of DevOps - including tools, patterns and practices. What makes the show good is the amount of guests that are well known in the DevOps community, including [Julia Evens](https://www.arresteddevops.com/discovery/), [James Turnbull](https://www.arresteddevops.com/art-of-monitoring-james-turnbull/), [Ben Hughes](https://www.arresteddevops.com/containers-security/) and [Jessie Frazelle](https://www.arresteddevops.com/containers-security/) to name a few. You don't have to work with DevOps related subjects to enjoy this show, all software developers or ops people will get something out of it.\n\n\n\n### [Bread Time](https://breadtime.simplecast.fm/)\n\nBread Time is a micro-podcast, which means episodes are between 10 - 30 minutes. Hosted by Gabriel Valdivia (Facebook) and Charlie Deets (WhatsApp), the show touches in to a wide spectrum of topics, ranging from design and software development to soft skills and [how to work well with others](https://breadtime.simplecast.fm/episodes/63809-working-well-with-others), or [Developing an effective workflow](https://breadtime.simplecast.fm/episodes/61202-developing-an-effective-workflow). The podcast is a nice fit for the short commute to work. I would recommend it for software developers, but especially for designers.\n\n\n\n### [No Fluff Just Stuff](https://www.nofluffjuststuff.com/podcast)\n\nOriginally a conference series about Java and software architecture, No Fluff Just Stuff also provides a kick ass podcast about software development, architecture, agile practices and in-depth interviews with developers. NFJS delivers rich content about topics every developer needs to know about, like Git, microservices and REST API's, critical thinking and software evangelism. Hosted by [Michael Carducci](https://www.nofluffjuststuff.com/conference/speaker/michael_carducci).  \n\n\n\n### [99% Invisible](http://99percentinvisible.org/)\n\nOh you already know about this one. 99% Invisible is the holy grail for all things architecture and design. In their own words:\n\n> 99% Invisible is about all the thought that goes into the things we don’t think about — the unnoticed architecture and design that shape our world.\n\nHosted by Roman Mars, the man with the silk voice, 99% Invisible is guaranteed to teach you something new each week, both about design, architecture and history. My favorite episodes include [Ten Letters for the Presiden](http://99percentinvisible.org/episode/ten-letters-president/), [Project Cybersyn](http://99percentinvisible.org/episode/project-cybersyn/) and [Unpleasant Design & Hostile Urban Architecture](http://99percentinvisible.org/episode/unpleasant-design-hostile-urban-architecture/). No matter who you are and what your interests are - you will love 99% Invisible.\n\n\n\n### [Accidental Tech Podcast](http://atp.fm/)\n\nAnother famous one, at least for the Apple fanboys. Marco Arment (Tumblr, Instapaper, Overcast), John Siracusa (Hypercritical, AnandTech) and Casey Liss (Macdown, Analog(ue) and other projects, sorry Casey) were making a car show, but ended up talking about tech - hence the name. These guys are famous in the Apple community, and every week they talk critically about all things Apple. If this sounds boring or you are not an Apple guy, fear not. All of the guys are developers too, and often touches into software development related topics, or random stuff like the [Nintento Switch](http://atp.fm/episodes/215) or [the American school system](the American school system). Also worth listening to just for the ending song.\n\n\n\n## Norwegian\n\n\n\n### [NRK Beta](https://nrkbeta.no/)\n\nNRK Beta is Norsk Rikskringkastings sandbox for technology and the new media. In a world where the traditional media like newspapers and TV struggle, NRK Beta is on a mission to see how modern technology can fit in with how journalists work and how we as users of their service consume content. NRK Beta has worked along side the mothership NRK as advisors on new media experiences, like the popular web-only series [SKAM](http://skam.p3.no/) and the now famous [Slow television](https://en.wikipedia.org/wiki/Slow_television) concept. The podcast talks us through  these solutions and how they came to be.  \n\n\n\n### [BEKK Open Podcast](https://open.bekk.no/)\n\nBEKK consulting is one of Norway's leading consultant houses and strive to be ahead of the technological curve. This means trying out the newest technology in customer projects and talk about the experience afterwards. A typical podcast episode revolves around a specific kind of technology, like [the functional paradigm](https://open.bekk.no/podcast-om-det-funksjonelle-paradigmet), [machine learning](https://open.bekk.no/podcast-om-maskinlering) and [creativity](https://open.bekk.no/podcast-om-kreativitet). The production is top notch and the hosts are developers and scientists working for BEKK. If you understand some Norwegian and love to write code, this show is for you.\n\n\n\n### [Giæver og Joffen](http://www.vg.no/podcast/giaever-og-joffen/)\n\nA political podcast!? Yes. As a matter of fact, it might be Norway's best about domestic and foreign political news. Anders Giæver and Frithjof Jacobsen are some of VG's heaviest political commentators. The podcast is recommended if you want to catch up on this weeks headlines.\n\n\n\n### [Hva er greia med?](http://www.rubicontv.no/radio/13/hva-er-greia-med)\n\nThe last podcast on the list is probably my favorite. In Hva er greia med? (What's The Deal With?) the hosts, Dr. Jonas Bergland and Dr. Ole Elvebakk do homework about a specific topic that interest them and talk about it for about an hour. The result is great entertainment where you also learn something from each episode. Topics so far include the Waldorf school, fingerprints, Ayn Rand, The Golden Ratio, Zombies, hearing and many many more subjects.\n","mobiledoc":null,"html":"<p>I am a huge fan of podcasts. They are a great way to learn about new subjects, both from the tech world, science and academia. Here is a list of some of my favorite podcasts so far in 2017.</p>\n\n<h2 id=\"techanddesign\">Tech and Design</h2>\n\n<h3 id=\"arresteddevopshttpswwwarresteddevopscom\"><a href=\"https://www.arresteddevops.com/\">Arrested DevOps</a></h3>\n\n<p>Probably the leading podcast about the DevOps movement. Matt Stratton (Chef), Trevor Hess (10th Magnitude) and Bridget Kromhout (Pivotal) guides the listener through the ever-changing world of DevOps - including tools, patterns and practices. What makes the show good is the amount of guests that are well known in the DevOps community, including <a href=\"https://www.arresteddevops.com/discovery/\">Julia Evens</a>, <a href=\"https://www.arresteddevops.com/art-of-monitoring-james-turnbull/\">James Turnbull</a>, <a href=\"https://www.arresteddevops.com/containers-security/\">Ben Hughes</a> and <a href=\"https://www.arresteddevops.com/containers-security/\">Jessie Frazelle</a> to name a few. You don't have to work with DevOps related subjects to enjoy this show, all software developers or ops people will get something out of it.</p>\n\n<h3 id=\"breadtimehttpsbreadtimesimplecastfm\"><a href=\"https://breadtime.simplecast.fm/\">Bread Time</a></h3>\n\n<p>Bread Time is a micro-podcast, which means episodes are between 10 - 30 minutes. Hosted by Gabriel Valdivia (Facebook) and Charlie Deets (WhatsApp), the show touches in to a wide spectrum of topics, ranging from design and software development to soft skills and <a href=\"https://breadtime.simplecast.fm/episodes/63809-working-well-with-others\">how to work well with others</a>, or <a href=\"https://breadtime.simplecast.fm/episodes/61202-developing-an-effective-workflow\">Developing an effective workflow</a>. The podcast is a nice fit for the short commute to work. I would recommend it for software developers, but especially for designers.</p>\n\n<h3 id=\"nofluffjuststuffhttpswwwnofluffjuststuffcompodcast\"><a href=\"https://www.nofluffjuststuff.com/podcast\">No Fluff Just Stuff</a></h3>\n\n<p>Originally a conference series about Java and software architecture, No Fluff Just Stuff also provides a kick ass podcast about software development, architecture, agile practices and in-depth interviews with developers. NFJS delivers rich content about topics every developer needs to know about, like Git, microservices and REST API's, critical thinking and software evangelism. Hosted by <a href=\"https://www.nofluffjuststuff.com/conference/speaker/michael_carducci\">Michael Carducci</a>.  </p>\n\n<h3 id=\"99invisiblehttp99percentinvisibleorg\"><a href=\"http://99percentinvisible.org/\">99% Invisible</a></h3>\n\n<p>Oh you already know about this one. 99% Invisible is the holy grail for all things architecture and design. In their own words:</p>\n\n<blockquote>\n  <p>99% Invisible is about all the thought that goes into the things we don’t think about — the unnoticed architecture and design that shape our world.</p>\n</blockquote>\n\n<p>Hosted by Roman Mars, the man with the silk voice, 99% Invisible is guaranteed to teach you something new each week, both about design, architecture and history. My favorite episodes include <a href=\"http://99percentinvisible.org/episode/ten-letters-president/\">Ten Letters for the Presiden</a>, <a href=\"http://99percentinvisible.org/episode/project-cybersyn/\">Project Cybersyn</a> and <a href=\"http://99percentinvisible.org/episode/unpleasant-design-hostile-urban-architecture/\">Unpleasant Design &amp; Hostile Urban Architecture</a>. No matter who you are and what your interests are - you will love 99% Invisible.</p>\n\n<h3 id=\"accidentaltechpodcasthttpatpfm\"><a href=\"http://atp.fm/\">Accidental Tech Podcast</a></h3>\n\n<p>Another famous one, at least for the Apple fanboys. Marco Arment (Tumblr, Instapaper, Overcast), John Siracusa (Hypercritical, AnandTech) and Casey Liss (Macdown, Analog(ue) and other projects, sorry Casey) were making a car show, but ended up talking about tech - hence the name. These guys are famous in the Apple community, and every week they talk critically about all things Apple. If this sounds boring or you are not an Apple guy, fear not. All of the guys are developers too, and often touches into software development related topics, or random stuff like the <a href=\"http://atp.fm/episodes/215\">Nintento Switch</a> or <a href=\"the American school system\">the American school system</a>. Also worth listening to just for the ending song.</p>\n\n<h2 id=\"norwegian\">Norwegian</h2>\n\n<h3 id=\"nrkbetahttpsnrkbetano\"><a href=\"https://nrkbeta.no/\">NRK Beta</a></h3>\n\n<p>NRK Beta is Norsk Rikskringkastings sandbox for technology and the new media. In a world where the traditional media like newspapers and TV struggle, NRK Beta is on a mission to see how modern technology can fit in with how journalists work and how we as users of their service consume content. NRK Beta has worked along side the mothership NRK as advisors on new media experiences, like the popular web-only series <a href=\"http://skam.p3.no/\">SKAM</a> and the now famous <a href=\"https://en.wikipedia.org/wiki/Slow_television\">Slow television</a> concept. The podcast talks us through  these solutions and how they came to be.  </p>\n\n<h3 id=\"bekkopenpodcasthttpsopenbekkno\"><a href=\"https://open.bekk.no/\">BEKK Open Podcast</a></h3>\n\n<p>BEKK consulting is one of Norway's leading consultant houses and strive to be ahead of the technological curve. This means trying out the newest technology in customer projects and talk about the experience afterwards. A typical podcast episode revolves around a specific kind of technology, like <a href=\"https://open.bekk.no/podcast-om-det-funksjonelle-paradigmet\">the functional paradigm</a>, <a href=\"https://open.bekk.no/podcast-om-maskinlering\">machine learning</a> and <a href=\"https://open.bekk.no/podcast-om-kreativitet\">creativity</a>. The production is top notch and the hosts are developers and scientists working for BEKK. If you understand some Norwegian and love to write code, this show is for you.</p>\n\n<h3 id=\"giverogjoffenhttpwwwvgnopodcastgiaeverogjoffen\"><a href=\"http://www.vg.no/podcast/giaever-og-joffen/\">Giæver og Joffen</a></h3>\n\n<p>A political podcast!? Yes. As a matter of fact, it might be Norway's best about domestic and foreign political news. Anders Giæver and Frithjof Jacobsen are some of VG's heaviest political commentators. The podcast is recommended if you want to catch up on this weeks headlines.</p>\n\n<h3 id=\"hvaergreiamedhttpwwwrubicontvnoradio13hvaergreiamed\"><a href=\"http://www.rubicontv.no/radio/13/hva-er-greia-med\">Hva er greia med?</a></h3>\n\n<p>The last podcast on the list is probably my favorite. In Hva er greia med? (What's The Deal With?) the hosts, Dr. Jonas Bergland and Dr. Ole Elvebakk do homework about a specific topic that interest them and talk about it for about an hour. The result is great entertainment where you also learn something from each episode. Topics so far include the Waldorf school, fingerprints, Ayn Rand, The Golden Ratio, Zombies, hearing and many many more subjects.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-15 18:27:17","created_by":1,"updated_at":"2017-06-15 18:28:04","updated_by":1,"published_at":"2017-04-14 18:27:00","published_by":1},{"id":22,"uuid":"f6e2255c-c3b7-49c8-8fa7-f9714612b55f","title":"How I broke an API (and how you can do it too!)","slug":"how-i-broke-an-api-and-how-you-can-do-it-too","markdown":"> Disclaimer: The content of this post is not nearly as dramatic as the title will have it, and for a lot of developers it will sound obvious. Based on code I have seen I nonetheless think it can learn a lot of people a valid lesson as well, and hopefully help other avoid the mistakes laziness can lead to.\n\nIn november last year [I wrote about Stratos,](http://blog.amosti.net/rolling-out-web-services-with-topshelf-chocolatey-and-ansible/) a simple web API based on Nancy with the sole purpose of listing out installed Chocolatey packages on a server. With this convenient web-process installed on our servers, the team figured out that it would be nice if it also supported [plugins](https://github.com/andmos/Stratos/blob/master/doc/Plugin.md).\n\nWith plugin support we could serve other useful info from the servers via the same API host. So far so good. The story begins when one of my colleagues had to serialize some rather complex objects to JSON. Out of the box Nancy comes with it's own JSON serializer - not using [Json.NET](http://www.newtonsoft.com/json) as the rest of the world uses. This serializer did some funny things with his object, so he wanted to use Json.NET. No problem, the Nancy guys offer [Nancy.Serialization.JsonNet](https://github.com/NancyFx/Nancy.Serialization.JsonNet) as a NuGet package.\nJust slide the DLL in and Nancy grabs hold of it. God stuff.\n\nAfter checking in the updated code the CI build failed on the original Stratos API tests. We expect a JSON on the following format when asking for installed Chocolatey packages:\n\n```\n[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 6,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"chocolatey-core.extension\",\n    \"version\": {\n      \"version\": {\n        \"major\": 1,\n        \"minor\": 1,\n        \"build\": 0,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  }]\n```\n\nAnd what we got with Json.Net was\n\n```\n[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": \"0.10.6\\r\"\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": \"1.1.0\\r\"\n  }\n]\n```\n\nThis was quite a surprise. The two JSON serializers were acting quite differently.\n\nThe original object should be quite simple:\n\n```\nusing NuGet;\n\nnamespace Stratos.Model\n{\n    public class NuGetPackage\n    {\n        public string PackageName { get; set;}\n        public SemanticVersion Version { get; set; }\n    }\n}\n```\n\nRight of the bat this should be no problem right?\nTrying to debug this a thought struck me: the only place I need `NuGet.Core` is in this object, to have a `SemanticVersion` object. [If we look at the SemanticVersion class](https://github.com/NuGet/NuGet2/blob/2.13/src/Core/SemanticVersion.cs) there are a lot of stuff there that could mess up the JSON serializer. Not owning the class ourself also prevent us from using things like DataMember attributes to control what parts of the object should be serialized. Another quite lazy choice here is to use the `SemanticVersion` object itself as a model property. There is a lot of dead weight on the object we don't need. A better choice is to wrap the parts we need in it's own object:\n\n```\nnamespace Stratos.Model\n{\n    public class PackageVersion\n    {\n        public Version Version { get; set; }\n        public string SpecialVersion { get; set; }\n\n    }\n}\n```\n\nAnd use it in the original object:\n\n```\nnamespace Stratos.Model\n{\n\tpublic class NuGetPackage\n\t{\n\t\tpublic string PackageName { get; set;}\n\t\tpublic PackageVersion Version { get; set; }\n\t}\n}\n```\n\nTo get rid of the NuGet reference, the `SemanticVersion` class got duplicated in. It is much better to own that logic ourself.\n\nWith this refactoring the JSON response looked a lot better:\n\n```\n[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 0,\n        \"Minor\": 10,\n        \"Build\": 6,\n        \"Revision\": 1,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 1\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 1,\n        \"Minor\": 1,\n        \"Build\": 0,\n        \"Revision\": 0,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 0\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n```\n\nCool, let's ship it!\n\nAn hour later, we had 25 systems that consumed this API die. What had gone wrong? The observant reader have seen it already:\n\n`packageName` vs. `PackageName`. Lowercase, uppercase. The Nancy serializer lowercases the keys by default, while Json.Net don't.\n\nWhy didn't the testes go red you ask? Let's look at the asserts in the test:\n\n```\nvar result = browser.Get(\"/api/chocoPackages\", with =>\n\t\t\t{\n\t\t\t\twith.HttpRequest();\t\t\t\n\t\t\t});\n\n\t\t\tvar resultJson = result.Body.AsString().ToLower();\n\n\t\t\tAssert.Equal(HttpStatusCode.OK, result.StatusCode);\n\t\t\tAssert.True(resultJson.Contains(\"major\"));\nAssert.True(resultJson.Contains(\"minor\"));\n```\n\n`ToLower()`. Yeah. The consumer of the API did *not* use `ToLower()`, obviously.\n\nSo what can we learn from this story?\n\n* Don't take on dependencies for a single and simple usecase\n\nI referenced `Nuget.Core` as a NuGet package to grab hold of the `SemanticVersion` class for parsing semantic version strings to a `Version` object. That usecase is so slim that just duplicating `SemanticVersion` from GitHub to my project is a much better approach - it is just stupid to take on a NuGet dependency.\n\n* Allways unit test as far out as possible\n\nBy having unit tests that call the actual API and assert on the expected response, the chances of breaking the API minimizes substantially. With Nancy [this is no problem](https://github.com/NancyFx/Nancy/wiki/Testing-your-application). Also, *test the types*. A simple `Contains()` on the JSON response is not enough. Always deserialize the object if possible.\n\n* You don't know what consumers have done with your API\n\nThe last and most important lesson is that if you have published your API and you have users on it, you have no idea how the client consume it. Even the smallest changes (like going from lowercase to uppsercase keys in this example) can break the consumer. That is the last thing we want. If you have put the API out there, you should respect the consumer and allways think about what effect your changes can have on them.\n","mobiledoc":null,"html":"<blockquote>\n  <p>Disclaimer: The content of this post is not nearly as dramatic as the title will have it, and for a lot of developers it will sound obvious. Based on code I have seen I nonetheless think it can learn a lot of people a valid lesson as well, and hopefully help other avoid the mistakes laziness can lead to.</p>\n</blockquote>\n\n<p>In november last year <a href=\"http://blog.amosti.net/rolling-out-web-services-with-topshelf-chocolatey-and-ansible/\">I wrote about Stratos,</a> a simple web API based on Nancy with the sole purpose of listing out installed Chocolatey packages on a server. With this convenient web-process installed on our servers, the team figured out that it would be nice if it also supported <a href=\"https://github.com/andmos/Stratos/blob/master/doc/Plugin.md\">plugins</a>.</p>\n\n<p>With plugin support we could serve other useful info from the servers via the same API host. So far so good. The story begins when one of my colleagues had to serialize some rather complex objects to JSON. Out of the box Nancy comes with it's own JSON serializer - not using <a href=\"http://www.newtonsoft.com/json\">Json.NET</a> as the rest of the world uses. This serializer did some funny things with his object, so he wanted to use Json.NET. No problem, the Nancy guys offer <a href=\"https://github.com/NancyFx/Nancy.Serialization.JsonNet\">Nancy.Serialization.JsonNet</a> as a NuGet package. <br />\nJust slide the DLL in and Nancy grabs hold of it. God stuff.</p>\n\n<p>After checking in the updated code the CI build failed on the original Stratos API tests. We expect a JSON on the following format when asking for installed Chocolatey packages:</p>\n\n<pre><code>[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 6,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"chocolatey-core.extension\",\n    \"version\": {\n      \"version\": {\n        \"major\": 1,\n        \"minor\": 1,\n        \"build\": 0,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  }]\n</code></pre>\n\n<p>And what we got with Json.Net was</p>\n\n<pre><code>[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": \"0.10.6\\r\"\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": \"1.1.0\\r\"\n  }\n]\n</code></pre>\n\n<p>This was quite a surprise. The two JSON serializers were acting quite differently.</p>\n\n<p>The original object should be quite simple:</p>\n\n<pre><code>using NuGet;\n\nnamespace Stratos.Model  \n{\n    public class NuGetPackage\n    {\n        public string PackageName { get; set;}\n        public SemanticVersion Version { get; set; }\n    }\n}\n</code></pre>\n\n<p>Right of the bat this should be no problem right? <br />\nTrying to debug this a thought struck me: the only place I need <code>NuGet.Core</code> is in this object, to have a <code>SemanticVersion</code> object. <a href=\"https://github.com/NuGet/NuGet2/blob/2.13/src/Core/SemanticVersion.cs\">If we look at the SemanticVersion class</a> there are a lot of stuff there that could mess up the JSON serializer. Not owning the class ourself also prevent us from using things like DataMember attributes to control what parts of the object should be serialized. Another quite lazy choice here is to use the <code>SemanticVersion</code> object itself as a model property. There is a lot of dead weight on the object we don't need. A better choice is to wrap the parts we need in it's own object:</p>\n\n<pre><code>namespace Stratos.Model  \n{\n    public class PackageVersion\n    {\n        public Version Version { get; set; }\n        public string SpecialVersion { get; set; }\n\n    }\n}\n</code></pre>\n\n<p>And use it in the original object:</p>\n\n<pre><code>namespace Stratos.Model  \n{\n    public class NuGetPackage\n    {\n        public string PackageName { get; set;}\n        public PackageVersion Version { get; set; }\n    }\n}\n</code></pre>\n\n<p>To get rid of the NuGet reference, the <code>SemanticVersion</code> class got duplicated in. It is much better to own that logic ourself.</p>\n\n<p>With this refactoring the JSON response looked a lot better:</p>\n\n<pre><code>[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 0,\n        \"Minor\": 10,\n        \"Build\": 6,\n        \"Revision\": 1,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 1\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 1,\n        \"Minor\": 1,\n        \"Build\": 0,\n        \"Revision\": 0,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 0\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n</code></pre>\n\n<p>Cool, let's ship it!</p>\n\n<p>An hour later, we had 25 systems that consumed this API die. What had gone wrong? The observant reader have seen it already:</p>\n\n<p><code>packageName</code> vs. <code>PackageName</code>. Lowercase, uppercase. The Nancy serializer lowercases the keys by default, while Json.Net don't.</p>\n\n<p>Why didn't the testes go red you ask? Let's look at the asserts in the test:</p>\n\n<pre><code>var result = browser.Get(\"/api/chocoPackages\", with =&gt;  \n            {\n                with.HttpRequest();         \n            });\n\n            var resultJson = result.Body.AsString().ToLower();\n\n            Assert.Equal(HttpStatusCode.OK, result.StatusCode);\n            Assert.True(resultJson.Contains(\"major\"));\nAssert.True(resultJson.Contains(\"minor\"));  \n</code></pre>\n\n<p><code>ToLower()</code>. Yeah. The consumer of the API did <em>not</em> use <code>ToLower()</code>, obviously.</p>\n\n<p>So what can we learn from this story?</p>\n\n<ul>\n<li>Don't take on dependencies for a single and simple usecase</li>\n</ul>\n\n<p>I referenced <code>Nuget.Core</code> as a NuGet package to grab hold of the <code>SemanticVersion</code> class for parsing semantic version strings to a <code>Version</code> object. That usecase is so slim that just duplicating <code>SemanticVersion</code> from GitHub to my project is a much better approach - it is just stupid to take on a NuGet dependency.</p>\n\n<ul>\n<li>Allways unit test as far out as possible</li>\n</ul>\n\n<p>By having unit tests that call the actual API and assert on the expected response, the chances of breaking the API minimizes substantially. With Nancy <a href=\"https://github.com/NancyFx/Nancy/wiki/Testing-your-application\">this is no problem</a>. Also, <em>test the types</em>. A simple <code>Contains()</code> on the JSON response is not enough. Always deserialize the object if possible.</p>\n\n<ul>\n<li>You don't know what consumers have done with your API</li>\n</ul>\n\n<p>The last and most important lesson is that if you have published your API and you have users on it, you have no idea how the client consume it. Even the smallest changes (like going from lowercase to uppsercase keys in this example) can break the consumer. That is the last thing we want. If you have put the API out there, you should respect the consumer and allways think about what effect your changes can have on them.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-15 18:29:41","created_by":1,"updated_at":"2017-06-15 18:55:42","updated_by":1,"published_at":"2017-06-08 18:29:00","published_by":1},{"id":23,"uuid":"3ebb4213-a785-47f5-9b42-3cac79e9eab3","title":"Associate file endings with languages in Atom","slug":"associate-file-endings-with-languages-in-atom","markdown":"A short tip if you use [Atom](https://atom.io/) as your preferred editor: how to associate file endings with a specific language pack for syntax highlighting. Should be easy right? Well, I actually had to do some googling on this.\n\nWhen working with my [Ansible](https://github.com/ansible/ansible) codebase, I like to use Atom. As with most languages out there, the community has created a [language pack for it](https://atom.io/packages/language-ansible) to give you some syntax highlighting goodness. There is just one detail missing here: How do I associate my file endings with the Ansible language pack?\n\nFor my inventory files, I postfix them with `.inventory` for convenience. All the servers for, let's say Team A, goes in `TeamA.inventory`. The language pack for Ansible doesn't know this, so I need to tell it som how. Right out of the box there is actually no easy way to do this, but thankfully the Atom ecosystem also got this covered with the `file-types` package.\n\n`apm install file-types` and then edit the local `config.cson` file like so:\n\n```\n\"*\":\n  \"file-types\":\n      \".*/group_vars/.*\": \"source.ansible\"\n      \".*/host_vars/.*\": \"source.ansible\"\n      inventory: \"source.ansible\"\n```\n\nAnd we are good to go.\n\nUpdate: As of version `1.24.1` of Atom we don't need any extension for setting language-pack to basic file-endings.\nThis can be done with:\n\n```\n\"*\":\n  core:\n    customFileTypes:\n      \"source.ansible\": [\n        \"inventory\"\n      ]\n```","mobiledoc":null,"html":"<p>A short tip if you use <a href=\"https://atom.io/\">Atom</a> as your preferred editor: how to associate file endings with a specific language pack for syntax highlighting. Should be easy right? Well, I actually had to do some googling on this.</p>\n\n<p>When working with my <a href=\"https://github.com/ansible/ansible\">Ansible</a> codebase, I like to use Atom. As with most languages out there, the community has created a <a href=\"https://atom.io/packages/language-ansible\">language pack for it</a> to give you some syntax highlighting goodness. There is just one detail missing here: How do I associate my file endings with the Ansible language pack?</p>\n\n<p>For my inventory files, I postfix them with <code>.inventory</code> for convenience. All the servers for, let's say Team A, goes in <code>TeamA.inventory</code>. The language pack for Ansible doesn't know this, so I need to tell it som how. Right out of the box there is actually no easy way to do this, but thankfully the Atom ecosystem also got this covered with the <code>file-types</code> package.</p>\n\n<p><code>apm install file-types</code> and then edit the local <code>config.cson</code> file like so:</p>\n\n<pre><code>\"*\":\n  \"file-types\":\n      \".*/group_vars/.*\": \"source.ansible\"\n      \".*/host_vars/.*\": \"source.ansible\"\n      inventory: \"source.ansible\"\n</code></pre>\n\n<p>And we are good to go.</p>\n\n<p>Update: As of version <code>1.24.1</code> of Atom we don't need any extension for setting language-pack to basic file-endings. <br />\nThis can be done with:</p>\n\n<pre><code>\"*\":\n  core:\n    customFileTypes:\n      \"source.ansible\": [\n        \"inventory\"\n      ]\n</code></pre>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-07-01 07:12:57","created_by":1,"updated_at":"2018-03-13 12:31:04","updated_by":1,"published_at":"2017-07-01 07:14:06","published_by":1},{"id":24,"uuid":"3970702e-f5ae-474b-8f0d-8ded4bd3a96e","title":"Using Docker for executing ad-hoc scripts and cron-like jobs","slug":"using-docker-for-executing-ad-hoc-scripts-and-cron-like-jobs","markdown":"### Intro\n\nPeople use Docker to package applications and infrastructure dependencies for easy shipment - and it shines bright while doing it.\nA less talked about usecase for Docker is executing simple ad-hoc scripts and cron-type jobs, like nightly jobs on a build server. Since Docker isolates processes, we can be much more free to experiment with different languages and runtimes not only in our applications, but also our scripts without having to think about provisioning software to the target servers. As developers we should be able to use the language we want and abstract it away. The output is what matters.\n\n\n### The Case\n\nFirst, let's look at the case. As readers of my blog know, I work with tools crossing the line between the Linux and Windows environments. That's why my team like Ansible. For provisioning Windows servers we use the `win_chocolatey` [module](http://docs.ansible.com/ansible/latest/win_chocolatey_module.html) Ansible provides.\nWe pull all the Chocolatey packages directly from the central Chocolatey repository. To be able to support offline provisioning of servers (as well as letting me sleep good at night knowing we have a backup) I need to script some syncing of the Chocolatey packages we use to our local ProGet server. Sounds easy, just write up some PowerShell and call it a day right?\nNah. Too boring. Let's see if we can do something cool.\n\n\n### The Code\n\nSo grabbing the package names should be rather easy, just some string searching and digging in the Ansible files.\nThe next step is to download the packages (including the package dependencies) and upload them to our NuGet server. Here we can use the NuGet Command Line tool directly, or Chocolatey. Both are now cross platform and works as well on NIX systems as it does on Windows. Throw in a Docker image and we can mix some scripts.\n\nA colleague of mine is working on a cool project called  [dotnet-script](https://github.com/filipw/dotnet-script). In a nutshell, the project provides C# scripting based on dotnet core, with full debug support in Visual Studio Code (if you haven't checked it out, [you should](https://www.strathweb.com/2017/11/c-script-runner-for-net-core-2-0/)). As we all know, dotnet core means Linux support, so after a quick PR to the project it now have a Dockerfile as well. Parsing some Ansible task files is easy when C# is the weapon.\n\nAll Ansible roles installing Chocolatey packages looks like this:\n\n```\n   - name: Installing dotnet 4.6 Target Pack\n     win_chocolatey: name=dotnet4.6-targetpack source=\"{{ ChocoFeedUrl }}\"\n```\nSo the following parser code returns all the package names:\n\n```\n#! \"netcoreapp1.1\"\n#r \"nuget:NetStandard.Library,1.6.1\"\n\nvar rolesFolder = \"roles/\";\nvar packages = new HashSet<string>();\n\nvar allFiles = Directory.GetFiles(rolesFolder, \"*.yaml\", SearchOption.AllDirectories);\n\nforeach(var yamlFile in allFiles)\n{\n    var textLines = File.ReadAllLines(yamlFile);\n    foreach(var line in textLines)\n    {\n        if(line.Contains(\"win_chocolatey\") && !line.ToLower().Contains(\"internalsoftware\"))\n        {\n            packages.Add(FetchPackageName(line));\n        }\n    }\n}\n\nprivate string FetchPackageName(string line)\n{\n    var words = line.Split(' ');\n    foreach(var word in words)\n    {\n        if(word.Contains(\"name=\"))\n        {\n            return word.Replace(\"name=\", string.Empty);\n        }\n    }\n    return string.Empty;\n}\n\nFile.WriteAllLines(\"scripts/nugetPackages.txt\", packages);\n```\nWriting that without a single `Console.Writeline()` for debug is a pretty nice experience.\n\nTo download the packages and upload them to the local NuGet server a simple shell script does the trick:\n\n```\n#! /bin/bash\n\nfilename=\"nugetPackages.txt\"\n\nwhile read -r line\ndo\n    nuget install -OutputDirectory . $line -Source https://chocolatey.org/api/v2/\n    echo $line\ndone < \"$filename\"\n\npackages=$(find **/*.nupkg)\n\nfor package in $packages\ndo\n    choco push $package --api-key \"SomeAPIKeyYouCantHaveDearBlog\" --Source http://internalNugetServer/nuget/ExternalSoftware/ --force\ndone\n```\n\nNote that we use Nuget Command Line for downloading the package, while Chocolatey puts up less of a fuzz when pushing packages (throw in the `--force` flag and it just pushes, no questions asked).\n\nTo glue it all together without having to install any dotnet tools on the Linux server, enter the power of Docker.\n\n```\n#! /bin/bash\n\ndocker run --name packages -v $(cd ../ && pwd):/scripts:z andmos/dotnet-script scripts/GeneratePackagesFile.csx\ndocker run --name packagesync --volumes-from=packages:z -w=\"/scripts/scripts\" andmos/choco ./DownloadFromChocoPushToProget.sh\n\ndocker rm -v data\ndocker rm -v packagesync\n```\n\nHere we first use the the `andmos/dotnet-script` image to execute the parser script, digging out all Chocolatey packages from the Ansible roles. The Ansible repo is shared from the host to the container via the `-v` flag, mounting it in the workspace folder. Next, the workspace folder is mounted to the `andmos/choco` container, giving it access to the `nugetPackages.txt`. The `andmos/choco` provides access to NuGet Command Line and Chocolatey itself.\n\nLast but not least - we clean up after ourself. When writing self contained ad-hoc Docker pipelines like this it is important to take out the thrash.\n\n### Summary\n\nThis code runs each night via a TeamCity build, syncing packages as we provision out new Chocolatey packages. The agent running it has only Docker installed, and that is enough. Docker provides a nice abstraction when executing simple scripts like this, making it easy to try out new languages without having to install a thousand runtimes on the servers. If I want to try out F# or Python next, it is as simple as switching out an Image. It is also really cool to see traditional Windows tools running smoothly on Linux. \n","mobiledoc":null,"html":"<h3 id=\"intro\">Intro</h3>\n\n<p>People use Docker to package applications and infrastructure dependencies for easy shipment - and it shines bright while doing it. <br />\nA less talked about usecase for Docker is executing simple ad-hoc scripts and cron-type jobs, like nightly jobs on a build server. Since Docker isolates processes, we can be much more free to experiment with different languages and runtimes not only in our applications, but also our scripts without having to think about provisioning software to the target servers. As developers we should be able to use the language we want and abstract it away. The output is what matters.</p>\n\n<h3 id=\"thecase\">The Case</h3>\n\n<p>First, let's look at the case. As readers of my blog know, I work with tools crossing the line between the Linux and Windows environments. That's why my team like Ansible. For provisioning Windows servers we use the <code>win_chocolatey</code> <a href=\"http://docs.ansible.com/ansible/latest/win_chocolatey_module.html\">module</a> Ansible provides. <br />\nWe pull all the Chocolatey packages directly from the central Chocolatey repository. To be able to support offline provisioning of servers (as well as letting me sleep good at night knowing we have a backup) I need to script some syncing of the Chocolatey packages we use to our local ProGet server. Sounds easy, just write up some PowerShell and call it a day right? <br />\nNah. Too boring. Let's see if we can do something cool.</p>\n\n<h3 id=\"thecode\">The Code</h3>\n\n<p>So grabbing the package names should be rather easy, just some string searching and digging in the Ansible files. <br />\nThe next step is to download the packages (including the package dependencies) and upload them to our NuGet server. Here we can use the NuGet Command Line tool directly, or Chocolatey. Both are now cross platform and works as well on NIX systems as it does on Windows. Throw in a Docker image and we can mix some scripts.</p>\n\n<p>A colleague of mine is working on a cool project called  <a href=\"https://github.com/filipw/dotnet-script\">dotnet-script</a>. In a nutshell, the project provides C# scripting based on dotnet core, with full debug support in Visual Studio Code (if you haven't checked it out, <a href=\"https://www.strathweb.com/2017/11/c-script-runner-for-net-core-2-0/\">you should</a>). As we all know, dotnet core means Linux support, so after a quick PR to the project it now have a Dockerfile as well. Parsing some Ansible task files is easy when C# is the weapon.</p>\n\n<p>All Ansible roles installing Chocolatey packages looks like this:</p>\n\n<pre><code>   - name: Installing dotnet 4.6 Target Pack\n     win_chocolatey: name=dotnet4.6-targetpack source=\"{{ ChocoFeedUrl }}\"\n</code></pre>\n\n<p>So the following parser code returns all the package names:</p>\n\n<pre><code>#! \"netcoreapp1.1\"\n#r \"nuget:NetStandard.Library,1.6.1\"\n\nvar rolesFolder = \"roles/\";  \nvar packages = new HashSet&lt;string&gt;();\n\nvar allFiles = Directory.GetFiles(rolesFolder, \"*.yaml\", SearchOption.AllDirectories);\n\nforeach(var yamlFile in allFiles)  \n{\n    var textLines = File.ReadAllLines(yamlFile);\n    foreach(var line in textLines)\n    {\n        if(line.Contains(\"win_chocolatey\") &amp;&amp; !line.ToLower().Contains(\"internalsoftware\"))\n        {\n            packages.Add(FetchPackageName(line));\n        }\n    }\n}\n\nprivate string FetchPackageName(string line)  \n{\n    var words = line.Split(' ');\n    foreach(var word in words)\n    {\n        if(word.Contains(\"name=\"))\n        {\n            return word.Replace(\"name=\", string.Empty);\n        }\n    }\n    return string.Empty;\n}\n\nFile.WriteAllLines(\"scripts/nugetPackages.txt\", packages);  \n</code></pre>\n\n<p>Writing that without a single <code>Console.Writeline()</code> for debug is a pretty nice experience.</p>\n\n<p>To download the packages and upload them to the local NuGet server a simple shell script does the trick:</p>\n\n<pre><code>#! /bin/bash\n\nfilename=\"nugetPackages.txt\"\n\nwhile read -r line  \ndo  \n    nuget install -OutputDirectory . $line -Source https://chocolatey.org/api/v2/\n    echo $line\ndone &lt; \"$filename\"\n\npackages=$(find **/*.nupkg)\n\nfor package in $packages  \ndo  \n    choco push $package --api-key \"SomeAPIKeyYouCantHaveDearBlog\" --Source http://internalNugetServer/nuget/ExternalSoftware/ --force\ndone  \n</code></pre>\n\n<p>Note that we use Nuget Command Line for downloading the package, while Chocolatey puts up less of a fuzz when pushing packages (throw in the <code>--force</code> flag and it just pushes, no questions asked).</p>\n\n<p>To glue it all together without having to install any dotnet tools on the Linux server, enter the power of Docker.</p>\n\n<pre><code>#! /bin/bash\n\ndocker run --name packages -v $(cd ../ &amp;&amp; pwd):/scripts:z andmos/dotnet-script scripts/GeneratePackagesFile.csx  \ndocker run --name packagesync --volumes-from=packages:z -w=\"/scripts/scripts\" andmos/choco ./DownloadFromChocoPushToProget.sh\n\ndocker rm -v data  \ndocker rm -v packagesync  \n</code></pre>\n\n<p>Here we first use the the <code>andmos/dotnet-script</code> image to execute the parser script, digging out all Chocolatey packages from the Ansible roles. The Ansible repo is shared from the host to the container via the <code>-v</code> flag, mounting it in the workspace folder. Next, the workspace folder is mounted to the <code>andmos/choco</code> container, giving it access to the <code>nugetPackages.txt</code>. The <code>andmos/choco</code> provides access to NuGet Command Line and Chocolatey itself.</p>\n\n<p>Last but not least - we clean up after ourself. When writing self contained ad-hoc Docker pipelines like this it is important to take out the thrash.</p>\n\n<h3 id=\"summary\">Summary</h3>\n\n<p>This code runs each night via a TeamCity build, syncing packages as we provision out new Chocolatey packages. The agent running it has only Docker installed, and that is enough. Docker provides a nice abstraction when executing simple scripts like this, making it easy to try out new languages without having to install a thousand runtimes on the servers. If I want to try out F# or Python next, it is as simple as switching out an Image. It is also really cool to see traditional Windows tools running smoothly on Linux. </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-11-13 19:23:07","created_by":1,"updated_at":"2017-11-14 07:27:22","updated_by":1,"published_at":"2017-11-13 19:27:55","published_by":1},{"id":25,"uuid":"677e20a4-62f2-4550-8589-78848d27d485","title":"Bootstrapping Octopus Deploy SSH targets with dotnet-script","slug":"bootstrapping-octopus-deploy-ssh-targets-with-dotnet-script","markdown":"Those who use Octopus Deploy probably know about [Calamari](https://octopus.com/docs/api-and-integration/calamari).\nIn short, Calamari is a console app that functions as the deployment engine on the Octopus Deploy targets machines.  \nCalamari is the app that makes sure your NuGet packages, Powershell, Bash, F# scripts etc. Is being executed on the target machine.\nFor Windows the [Octopus Tentacle](https://octopus.com/docs/infrastructure/windows-targets) is used by the main Octopus Server to communicate with the deployment targets, eventually installing Calamari on the first deploy.\n\nThis is all good, but what about Linux targets? There is a story here as well. Traditionally Calamari uses full .NET Framework, and can be run via Mono on Linux targets.\nThere is no need for a Tentacle process, [plain old SSH](https://octopus.com/docs/infrastructure/ssh-targets) is used to invoke a shell script under the hood that in turn invokes Calamari.\nThe Mono approach works, but is kind of \"old and heavy\" in the new dotnet-core world, so Calamari is now also available as a [stand-alone dotnet-core app!](https://octopus.com/docs/infrastructure/ssh-targets/self-contained-calamari)\nThe cool thing here is that no software has to be installed at all for Calamari to run (hence self-contained) which makes is much more lightweight and easy to provision.\n\nAlas, there are some limitations here. Calamari can't run ScriptCS / F# scripts via the self-contained variant since these projects hasn't been targeted for dotnet-core yet. This is quite a bummer since the deployment pipeline we work with contains about 20.000 lines of `.CSX` code. Quite the investment.\n\nBut, a new hope approaches with the [dotnet-script](https://github.com/filipw/dotnet-script) project. As mentioned in another one of my posts, dotnet-script provides C# scripting based on dotnet-core, with full debug support via Visual Studio Code on _all_ OS'es, with great features like referencing NuGet packages inline from the scripts. No need for that `packages.config` file. The only catch is the requirement of the dotnet-core SDK, since dotnet-script uses NuGet and does some compilation behind the scenes. Still better to have a dependency on dotnet than full Mono.\n\nSo, Linux + Calamari + dotnet-script = awesomeness down the road.\nAs a PoC, I wrote an example Ansible Playbook that [installs dotnet-script](https://github.com/andmos/ansible-role-dotnet-script) on the VM, before it joins the Octopus environment via the SSH target option. To make it all full-circle, The SSH-target bootstrapping script is written in dotnet-script itself, taking advantage of the [octopus.client](https://octopus.com/docs/api-and-integration/octopus.client) NuGet package and the beautiful inline NuGet support.\n\nThe script itself is as simple as:\n\n<script src=\"https://gist.github.com/andmos/b82dd2771c1db5ac3ff233f4305be465.js\"></script>\n\nThis script is called via the wrapper shell script:\n\n<script src=\"https://gist.github.com/andmos/3e21275d1695d4faff617b79adf4bc2a.js\"></script>\n\nAnd at last, completing it with a thrown-together Ansible role:\n\n<script src=\"https://gist.github.com/andmos/6c2d9910c8e89aa95e0331062899469f.js\"></script>\n\nWhen the playbook run completes, the new Linux machine is available in the Octopus environment and dotnet-script is ready to go.\nAs of now there are no official support in Calamari for running dotnet-script directly, but [we are working on that](https://github.com/seesharper/Calamari) and planning to check the possibilities for adding `dotnet-script` support upstream in Calamari. It makes much more sense to use a C# script runner that can be executed on all platforms directly, making it easier to have a homogeneous codebase across Windows and Linux targets.\n\nAnother work-around is to create a NuGet package with a wrapper-script to execute dotnet-script. Example:\n```\nNuGetScriptPackage\n│   ├── contentFiles\n│   │   └── csx\n│   │       └── any\n│   │           └── helloworld.csx       \n│   └── tools\n│       └── RunScript.sh\n```\nWhere the Octopus step points to the `RunScript.sh` script in the package:\n```\n#! /bin/bash\n\ndotnet script ../content/csx/any/helloworld.csx\n```\n\nBut this is no good solutions since it can't handle scripts argument or cross-platform execution in any good manner.\n\nI believe dotnet-script can be a nice way to go for Octopus Deploy in the future, if it is to take cross-platform deployment seriously.\n","mobiledoc":null,"html":"<p>Those who use Octopus Deploy probably know about <a href=\"https://octopus.com/docs/api-and-integration/calamari\">Calamari</a>. <br />\nIn short, Calamari is a console app that functions as the deployment engine on the Octopus Deploy targets machines. <br />\nCalamari is the app that makes sure your NuGet packages, Powershell, Bash, F# scripts etc. Is being executed on the target machine. <br />\nFor Windows the <a href=\"https://octopus.com/docs/infrastructure/windows-targets\">Octopus Tentacle</a> is used by the main Octopus Server to communicate with the deployment targets, eventually installing Calamari on the first deploy.</p>\n\n<p>This is all good, but what about Linux targets? There is a story here as well. Traditionally Calamari uses full .NET Framework, and can be run via Mono on Linux targets. <br />\nThere is no need for a Tentacle process, <a href=\"https://octopus.com/docs/infrastructure/ssh-targets\">plain old SSH</a> is used to invoke a shell script under the hood that in turn invokes Calamari. <br />\nThe Mono approach works, but is kind of \"old and heavy\" in the new dotnet-core world, so Calamari is now also available as a <a href=\"https://octopus.com/docs/infrastructure/ssh-targets/self-contained-calamari\">stand-alone dotnet-core app!</a> <br />\nThe cool thing here is that no software has to be installed at all for Calamari to run (hence self-contained) which makes is much more lightweight and easy to provision.</p>\n\n<p>Alas, there are some limitations here. Calamari can't run ScriptCS / F# scripts via the self-contained variant since these projects hasn't been targeted for dotnet-core yet. This is quite a bummer since the deployment pipeline we work with contains about 20.000 lines of <code>.CSX</code> code. Quite the investment.</p>\n\n<p>But, a new hope approaches with the <a href=\"https://github.com/filipw/dotnet-script\">dotnet-script</a> project. As mentioned in another one of my posts, dotnet-script provides C# scripting based on dotnet-core, with full debug support via Visual Studio Code on <em>all</em> OS'es, with great features like referencing NuGet packages inline from the scripts. No need for that <code>packages.config</code> file. The only catch is the requirement of the dotnet-core SDK, since dotnet-script uses NuGet and does some compilation behind the scenes. Still better to have a dependency on dotnet than full Mono.</p>\n\n<p>So, Linux + Calamari + dotnet-script = awesomeness down the road. <br />\nAs a PoC, I wrote an example Ansible Playbook that <a href=\"https://github.com/andmos/ansible-role-dotnet-script\">installs dotnet-script</a> on the VM, before it joins the Octopus environment via the SSH target option. To make it all full-circle, The SSH-target bootstrapping script is written in dotnet-script itself, taking advantage of the <a href=\"https://octopus.com/docs/api-and-integration/octopus.client\">octopus.client</a> NuGet package and the beautiful inline NuGet support.</p>\n\n<p>The script itself is as simple as:</p>\n\n<script src=\"https://gist.github.com/andmos/b82dd2771c1db5ac3ff233f4305be465.js\"></script>\n\n<p>This script is called via the wrapper shell script:</p>\n\n<script src=\"https://gist.github.com/andmos/3e21275d1695d4faff617b79adf4bc2a.js\"></script>\n\n<p>And at last, completing it with a thrown-together Ansible role:</p>\n\n<script src=\"https://gist.github.com/andmos/6c2d9910c8e89aa95e0331062899469f.js\"></script>\n\n<p>When the playbook run completes, the new Linux machine is available in the Octopus environment and dotnet-script is ready to go. <br />\nAs of now there are no official support in Calamari for running dotnet-script directly, but <a href=\"https://github.com/seesharper/Calamari\">we are working on that</a> and planning to check the possibilities for adding <code>dotnet-script</code> support upstream in Calamari. It makes much more sense to use a C# script runner that can be executed on all platforms directly, making it easier to have a homogeneous codebase across Windows and Linux targets.</p>\n\n<p>Another work-around is to create a NuGet package with a wrapper-script to execute dotnet-script. Example:  </p>\n\n<pre><code>NuGetScriptPackage  \n│   ├── contentFiles\n│   │   └── csx\n│   │       └── any\n│   │           └── helloworld.csx       \n│   └── tools\n│       └── RunScript.sh\n</code></pre>\n\n<p>Where the Octopus step points to the <code>RunScript.sh</code> script in the package:  </p>\n\n<pre><code>#! /bin/bash\n\ndotnet script ../content/csx/any/helloworld.csx  \n</code></pre>\n\n<p>But this is no good solutions since it can't handle scripts argument or cross-platform execution in any good manner.</p>\n\n<p>I believe dotnet-script can be a nice way to go for Octopus Deploy in the future, if it is to take cross-platform deployment seriously.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2018-01-03 19:33:38","created_by":1,"updated_at":"2018-01-03 20:15:27","updated_by":1,"published_at":"2018-01-03 19:43:46","published_by":1},{"id":26,"uuid":"335bc3b4-7fca-447b-bb86-9c62ffcf09aa","title":"Container Structure Tests","slug":"container-structure-tests","markdown":"A cool new project from [Google Cloud Platform](https://cloud.google.com/) is [Container Structure Tests](https://github.com/GoogleCloudPlatform/container-structure-test). When working with containers, running tasks like unit tests as a part of the the container build stage is smart - and with [Docker multi-stage builds](https://docs.docker.com/develop/develop-images/multistage-build/) available there is no excuse not to.\n\nIncluding tests, the Dockerfile instructions itself will throw an exitcode if some of the preconditions change.\n\nLet's say we have the step\n```\nCOPY ReadingList.exe.config /ReadingList/bin/Release/ReadingList.exe.config\n```\n\nand it gets tampered with,\n\n```\nCOPY ReadingList.exe.config.fake /ReadingList/bin/Release/ReadingList.exe.config\n```\nthe daemon will throw an exitcode since `ReadingList.exe.config.fake` don't exist.\n\nSo far so good. But what if the destination name gets tampered with, like this?\n\n```\nCOPY ReadingList.exe.config /ReadingList/bin/Release/ReadingList.exe.config.fake\n```\nIf the file is not touched anymore in the Dockerfile before a container is run from the image, the error won't be discovered before runtime. This is the one of the cases the container structure tests is meant to catch.\n\nThe container structure tests will validate the structure of the container image and provides a good way to do regression tests on the build artifacts themselves, so we can catch errors before the image is pushed to the repository. These tests can be used to check the output of commands in an image, as well as verify metadata and contents of the filesystem.\n\nLet's take the example over. If the container image needs to have a config file at `/ReadingList/bin/Release/`, we can write some tests for it (and more):\n\n<script src=\"https://gist.github.com/andmos/d63f2da228b2da60d0eff5a7d004ebb2.js\"></script>\n\n`fileExistenceTests` can assert files in the container image, `metadataTest` can assert expected exposed ports etc.\nThere are also categories for file content tests, command tests, environment variable tests and more.\n\nTo run the tests on an image, use to official container:\n\n```\ndocker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):/tests gcr.io/gcp-runtimes/container-structure-test:v0.2.1 -image andmos/readinglist -test.v /tests/imageTests/readinglist_container_tests_config.yaml\n```\n\nThe output looks similar to that of any other unit test Framework:\n\n```\nUsing driver docker\n=== RUN   TestAll\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe\n2018/03/05 18:08:31 Running tests for file /tests/imageTests/readinglist_container_tests_config.yaml\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe.config\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe.config.template\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe.config.toml\n=== RUN   TestAll/Metadata_Test\n--- PASS: TestAll (0.78s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe (0.18s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe.config (0.18s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe.config.template (0.19s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe.config.toml (0.22s)\n    --- PASS: TestAll/Metadata_Test (0.00s)\n\tstructure_test.go:49: Total tests run: 5\nPASS\n```\n\nI'm planning to implement container structure tests as a last sanity check before an image is pushed.\nWith [Travis](https://travis-ci.org/) this can be done with ease:\n\n```\nscript:\n  - docker build -t andmos/readinglist .\n  - docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):/tests gcr.io/gcp-runtimes/container-structure-test:v0.2.1 -image andmos/readinglist -test.v  /tests/imageTests/readinglist_container_tests_config.yaml\n```\n\nThe project is rather new, but comming from Google I suspect it will get much more love in the future. It surely fills a hole regarding CI and CD in a container world. ","mobiledoc":null,"html":"<p>A cool new project from <a href=\"https://cloud.google.com/\">Google Cloud Platform</a> is <a href=\"https://github.com/GoogleCloudPlatform/container-structure-test\">Container Structure Tests</a>. When working with containers, running tasks like unit tests as a part of the the container build stage is smart - and with <a href=\"https://docs.docker.com/develop/develop-images/multistage-build/\">Docker multi-stage builds</a> available there is no excuse not to.</p>\n\n<p>Including tests, the Dockerfile instructions itself will throw an exitcode if some of the preconditions change.</p>\n\n<p>Let's say we have the step  </p>\n\n<pre><code>COPY ReadingList.exe.config /ReadingList/bin/Release/ReadingList.exe.config  \n</code></pre>\n\n<p>and it gets tampered with,</p>\n\n<pre><code>COPY ReadingList.exe.config.fake /ReadingList/bin/Release/ReadingList.exe.config  \n</code></pre>\n\n<p>the daemon will throw an exitcode since <code>ReadingList.exe.config.fake</code> don't exist.</p>\n\n<p>So far so good. But what if the destination name gets tampered with, like this?</p>\n\n<pre><code>COPY ReadingList.exe.config /ReadingList/bin/Release/ReadingList.exe.config.fake  \n</code></pre>\n\n<p>If the file is not touched anymore in the Dockerfile before a container is run from the image, the error won't be discovered before runtime. This is the one of the cases the container structure tests is meant to catch.</p>\n\n<p>The container structure tests will validate the structure of the container image and provides a good way to do regression tests on the build artifacts themselves, so we can catch errors before the image is pushed to the repository. These tests can be used to check the output of commands in an image, as well as verify metadata and contents of the filesystem.</p>\n\n<p>Let's take the example over. If the container image needs to have a config file at <code>/ReadingList/bin/Release/</code>, we can write some tests for it (and more):</p>\n\n<script src=\"https://gist.github.com/andmos/d63f2da228b2da60d0eff5a7d004ebb2.js\"></script>\n\n<p><code>fileExistenceTests</code> can assert files in the container image, <code>metadataTest</code> can assert expected exposed ports etc.\nThere are also categories for file content tests, command tests, environment variable tests and more.</p>\n\n<p>To run the tests on an image, use to official container:</p>\n\n<pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):/tests gcr.io/gcp-runtimes/container-structure-test:v0.2.1 -image andmos/readinglist -test.v /tests/imageTests/readinglist_container_tests_config.yaml  \n</code></pre>\n\n<p>The output looks similar to that of any other unit test Framework:</p>\n\n<pre><code>Using driver docker  \n=== RUN   TestAll\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe\n2018/03/05 18:08:31 Running tests for file /tests/imageTests/readinglist_container_tests_config.yaml  \n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe.config\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe.config.template\n=== RUN   TestAll/File_Existence_Test:_ReadingList.exe.config.toml\n=== RUN   TestAll/Metadata_Test\n--- PASS: TestAll (0.78s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe (0.18s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe.config (0.18s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe.config.template (0.19s)\n    --- PASS: TestAll/File_Existence_Test:_ReadingList.exe.config.toml (0.22s)\n    --- PASS: TestAll/Metadata_Test (0.00s)\n    structure_test.go:49: Total tests run: 5\nPASS  \n</code></pre>\n\n<p>I'm planning to implement container structure tests as a last sanity check before an image is pushed. <br />\nWith <a href=\"https://travis-ci.org/\">Travis</a> this can be done with ease:</p>\n\n<pre><code>script:  \n  - docker build -t andmos/readinglist .\n  - docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):/tests gcr.io/gcp-runtimes/container-structure-test:v0.2.1 -image andmos/readinglist -test.v  /tests/imageTests/readinglist_container_tests_config.yaml\n</code></pre>\n\n<p>The project is rather new, but comming from Google I suspect it will get much more love in the future. It surely fills a hole regarding CI and CD in a container world. </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2018-03-05 18:14:02","created_by":1,"updated_at":"2018-03-05 18:18:36","updated_by":1,"published_at":"2018-03-05 18:15:47","published_by":1},{"id":27,"uuid":"b8cedeee-2565-4905-b8b5-d945f5a9f0e3","title":"How the \"sharing economy\" fits into my everyday life","slug":"how-the-sharing-economy-fits-in-to-my-life","markdown":"It's been some time since my last post, so let's shake it off with a rare non-technical one.\n\n###  Sharing whatnow?\nThe term \"sharing economy\", or peer-to-peer services if you like, has been hyped up over the last couple of years. Companies like Uber, Airbnb, Lyft and so on have been both praised and villainize for shaking up old, traditional industries. I will not go in to the political and ethnical debate about this \"brave new world\" transition that this economy is bringing in this post - that is content for another day - but as we say in social democratic Norway:  Hey, [regulate](https://e24.no/digital/delingsoekonomi/norske-delingsoekonomi-gr-ndere-vil-reguleres-mer-loennsomt-naa-aa-ikke-ta-ansvar/24163641) and pay them taxes, yo! Let's instead focus on what services I use regularly and what impact they have had in my day to day life.\n\n### Airbnb\nOf course, like everybody else I use [Airbnb](https://www.airbnb.com/) when traveling privately. The personal experience you get from contacting a host is appealing to me, and something the big hotels can't compete with. This summer I rented a nice \"rorbu\", or fishing shelter up in Lofoten. I became really friendly with the host, a fisherman who rented out his shelter in the summer months when he was not using it himself. Airbnb has opened up a new world for people to earn a bit extra on that spare bedroom, as well as connecting people from all around the world. It also makes a traveling experience much more authentic. \n\nI don't rent out anything on Airbnb, but me and my girlfriend are considering renting out the apartment the next time we go on holiday.\n\n### Nabobil\n[Nabobil](https://nabobil.no/), or \"neighbor's car\" is a Norwegian service that lets people rent out their car to others, like Airbnb is for houses. This service have really changed my life in a big way.\nI live pretty close to the city center in Trondheim, about 15 minutes on foot. The bus stop is 50 meters from my door, and [city bikes](https://trondheimbysykkel.no/) about 100 meters from there again. Work is about 3 kilometers away, and we don't have any parking spots there. What did this mean? My trusty old Honda Civic didn't get driven that much, maybe once a week.\n\nIt is not sustainable to own a car with that user pattern, so I got rid of it.\nIf we need to shop for something big or go on a weekend trip, I simply hit up someone on Nabobil and rent a car from them. In the area I live, that is about 30 cars ready to go right now. Like Airbnb, if you first come in contact with a person and the renting experience is solid for both of you, the relationship is made and it's much easier to rent again the next time. When renting a car, I also like to fill up the gas tank and give it a solid wash, just to be a nice. It pays off in the long term. Getting rid of the car and use Nabobil has also saved a lot of money, since stuff like insurance are included in the total sum.  \n\n### Foodora/Wolt\nSometimes you want restaurant food, but are too damn lazy to walk the 15 minutes in to town. Don't worry, [Foodora](https://www.foodora.no/en/?r=1) or [Wolt](https://wolt.com/) got you.\n\nJust log in to one of them, select what food you want from what restaurant and in about 30 minutes time, a nice guy or girl comes biking home to you with it. The restaurant reaches more customers, doesn't need to invest in cars and drivers, and the student on the bike earns some extra money. I'm a fan.\n\n### Vaskehjelp\n[Vaskehjelp](http://vaskehjelp.no/kunde/), or \"rent a cleaner\" is great for people who needs some extra help cleaning at home.\nSimilar to Airbnb and Uber, you come in contact with a person who names a price, you name a time and place, and you get your house cleaned. If the cleaner was nice and you are happy, give him a 5 star review so he gets more customers. Now I don't use the service, but my girlfriend is registered and cleans for people when she has the time. This is a great way for her to work when she wants to, a nice fit for students.\n\n### Little free library\nThe last one has nothing to do with economy (since it's free) but [Little free library](https://littlefreelibrary.org/) is my favorite sharing initiatives. It's small wooden sheds with a glass door you can put a book in. Or take a book from. Or both. In that way, you can donate that beat up version of The Lord Of The Rings you got lying around, in exchange for something else somebody put there. This is a great way to share knowledge with those around you.  ","mobiledoc":null,"html":"<p>It's been some time since my last post, so let's shake it off with a rare non-technical one.</p>\n\n<h3 id=\"sharingwhatnow\">Sharing whatnow?</h3>\n\n<p>The term \"sharing economy\", or peer-to-peer services if you like, has been hyped up over the last couple of years. Companies like Uber, Airbnb, Lyft and so on have been both praised and villainize for shaking up old, traditional industries. I will not go in to the political and ethnical debate about this \"brave new world\" transition that this economy is bringing in this post - that is content for another day - but as we say in social democratic Norway:  Hey, <a href=\"https://e24.no/digital/delingsoekonomi/norske-delingsoekonomi-gr-ndere-vil-reguleres-mer-loennsomt-naa-aa-ikke-ta-ansvar/24163641\">regulate</a> and pay them taxes, yo! Let's instead focus on what services I use regularly and what impact they have had in my day to day life.</p>\n\n<h3 id=\"airbnb\">Airbnb</h3>\n\n<p>Of course, like everybody else I use <a href=\"https://www.airbnb.com/\">Airbnb</a> when traveling privately. The personal experience you get from contacting a host is appealing to me, and something the big hotels can't compete with. This summer I rented a nice \"rorbu\", or fishing shelter up in Lofoten. I became really friendly with the host, a fisherman who rented out his shelter in the summer months when he was not using it himself. Airbnb has opened up a new world for people to earn a bit extra on that spare bedroom, as well as connecting people from all around the world. It also makes a traveling experience much more authentic. </p>\n\n<p>I don't rent out anything on Airbnb, but me and my girlfriend are considering renting out the apartment the next time we go on holiday.</p>\n\n<h3 id=\"nabobil\">Nabobil</h3>\n\n<p><a href=\"https://nabobil.no/\">Nabobil</a>, or \"neighbor's car\" is a Norwegian service that lets people rent out their car to others, like Airbnb is for houses. This service have really changed my life in a big way.\nI live pretty close to the city center in Trondheim, about 15 minutes on foot. The bus stop is 50 meters from my door, and <a href=\"https://trondheimbysykkel.no/\">city bikes</a> about 100 meters from there again. Work is about 3 kilometers away, and we don't have any parking spots there. What did this mean? My trusty old Honda Civic didn't get driven that much, maybe once a week.</p>\n\n<p>It is not sustainable to own a car with that user pattern, so I got rid of it. <br />\nIf we need to shop for something big or go on a weekend trip, I simply hit up someone on Nabobil and rent a car from them. In the area I live, that is about 30 cars ready to go right now. Like Airbnb, if you first come in contact with a person and the renting experience is solid for both of you, the relationship is made and it's much easier to rent again the next time. When renting a car, I also like to fill up the gas tank and give it a solid wash, just to be a nice. It pays off in the long term. Getting rid of the car and use Nabobil has also saved a lot of money, since stuff like insurance are included in the total sum.  </p>\n\n<h3 id=\"foodorawolt\">Foodora/Wolt</h3>\n\n<p>Sometimes you want restaurant food, but are too damn lazy to walk the 15 minutes in to town. Don't worry, <a href=\"https://www.foodora.no/en/?r=1\">Foodora</a> or <a href=\"https://wolt.com/\">Wolt</a> got you.</p>\n\n<p>Just log in to one of them, select what food you want from what restaurant and in about 30 minutes time, a nice guy or girl comes biking home to you with it. The restaurant reaches more customers, doesn't need to invest in cars and drivers, and the student on the bike earns some extra money. I'm a fan.</p>\n\n<h3 id=\"vaskehjelp\">Vaskehjelp</h3>\n\n<p><a href=\"http://vaskehjelp.no/kunde/\">Vaskehjelp</a>, or \"rent a cleaner\" is great for people who needs some extra help cleaning at home.\nSimilar to Airbnb and Uber, you come in contact with a person who names a price, you name a time and place, and you get your house cleaned. If the cleaner was nice and you are happy, give him a 5 star review so he gets more customers. Now I don't use the service, but my girlfriend is registered and cleans for people when she has the time. This is a great way for her to work when she wants to, a nice fit for students.</p>\n\n<h3 id=\"littlefreelibrary\">Little free library</h3>\n\n<p>The last one has nothing to do with economy (since it's free) but <a href=\"https://littlefreelibrary.org/\">Little free library</a> is my favorite sharing initiatives. It's small wooden sheds with a glass door you can put a book in. Or take a book from. Or both. In that way, you can donate that beat up version of The Lord Of The Rings you got lying around, in exchange for something else somebody put there. This is a great way to share knowledge with those around you.  </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2018-07-23 19:45:28","created_by":1,"updated_at":"2018-07-23 20:04:49","updated_by":1,"published_at":"2018-07-23 20:04:49","published_by":1},{"id":28,"uuid":"23a258fb-cd35-4f41-b511-424e9a818a9c","title":"Regression test Ansible Playbooks with Packer","slug":"regression-test-ansible-playbooks-with-packer","markdown":"Some people still live in a pre-Kubernetes world, installing infrastructure components directly on VMs.\nSome still hold on to the old and boring, but still going, vmware cluster from 5 - 10 years ago. And hey, there is nothing wrong with that.\n\nWe still have this setup at work, but thanks to Ansible, it feels like a private cloud at the IaaS level.\nWhen a development, QA or production environment is created, the CI system runs nightly provisioning against these servers.\n\nThis makes sure that all servers are up to date with the latest roles and fixes added to the playbooks.\nThis is all good, but one thing it doesn't fully protect against is regression in the playbooks. Most tasks (as they should be!) does not download the Java installer if Java is already in place on the server, or do a clean install with configuration of RabbitMQ if RabbitMQ is up and running smoothly. Unless the environments are recycled often, you can never be _quite_ sure that the Ansible playbooks and server setup will still work on the next clean install.\n\nEnter regression tests, and enter Packer.\n\nI'm about 5000 years late to the Packer party, but damn Hashicorp delivers as always.\nIf you haven't heard about Packer, it is a tool for building up and packing artifacts, mostly VMs (or containers) that can be delivered to a cloud provider. Or in this case, work against vsphere.\nThe normal Packer flow is to build up a VM from an ISO file with vmware player or workstation under the hood, run some\nprovisioning and then upload the artifact to vsphere.\nFor my regression tests, I don't want all this overhead. The good guys over at Jetbrains has developed a [plugin](https://github.com/jetbrains-infra/packer-builder-vsphere) for Packer that supports working directly with VMs or templates in the vpshere cluster itself, so the VMs can be created directly in the cluster.\n\nThe following Packer configuration creates a VM from a bare minimum CentOS template, before running the RabbitMQ playbook. If anything goes wrong, Packer throws an exit code and deletes the temporary machine. Throw it all in for a nightly build, and we can now be confident that the playbook will work the next time we need to set up a RabbitMQ server.\n\n```JSON\n{\n  \"variables\": {\n    \"vcenter_host\": \"vcenter01\",\n    \"vcenter_user\": \"admin@domain.local\",\n    \"vcenter_password\": \"{{env `VCENTERPASS`}}\",\n    \"ssh_user\": \"adminuser\",\n    \"ssh_private_key_file\": \"{{env `HOME`}}//.ssh/id_rsa\",\n    \"dc\": \"Datacenter\",\n    \"cluster\": \"MetroCluster\",\n    \"template_dir\": \"Application Servers/Team Optimus\",\n    \"resource_pool\": \"Team Optimus\",\n    \"template\": \"Ansible_CentOS73Packer\",\n    \"datastore\":            \"hus015sec-dev\",\n    \"vm_name\":  \"Ansible_CentOS73PackerRabbitMq\"\n  },\n\n  \"builders\": [\n    {\n      \"name\": \"RabbitMQ template test\",\n      \"type\": \"vsphere-clone\",\n\n      \"vcenter_server\":      \"{{ user `vcenter_host` }}\",\n      \"datacenter\":          \"{{ user `dc` }}\",\n      \"cluster\":             \"{{ user `cluster` }}\",\n      \"username\":            \"{{ user `vcenter_user` }}\",\n      \"password\":            \"{{ user `vcenter_password` }}\",\n      \"insecure_connection\": \"true\",\n      \"ssh_username\":        \"{{ user `ssh_user` }}\",\n      \"ssh_private_key_file\":  \"{{ user `ssh_private_key_file` }}\",\n      \"datastore\":            \"{{ user `datastore` }}\",\n      \"resource_pool\": \"{{ user `resource_pool` }}\",\n      \"folder\": \"{{ user `template_dir` }}\",\n      \"template\": \"{{ user `template` }}\",\n      \"vm_name\":  \"{{ user `vm_name` }}\",\n      \"convert_to_template\": false,\n      \"communicator\": \"ssh\"\n    }\n  ],\n\n  \"provisioners\": [\n    {\n      \"type\": \"ansible\",\n      \"playbook_file\": \"../RabbitMQ.yaml\",\n      \"host_alias\": \"rabbitmq\",\n      \"ansible_env_vars\": [ \"ANSIBLE_HOST_KEY_CHECKING=False\" ],\n      \"extra_arguments\": [ \"--verbose\" ],\n      \"user\": \"{{ user `ssh_user` }}\"\n    }\n  ]\n}\n```\n","mobiledoc":null,"html":"<p>Some people still live in a pre-Kubernetes world, installing infrastructure components directly on VMs. <br />\nSome still hold on to the old and boring, but still going, vmware cluster from 5 - 10 years ago. And hey, there is nothing wrong with that.</p>\n\n<p>We still have this setup at work, but thanks to Ansible, it feels like a private cloud at the IaaS level. <br />\nWhen a development, QA or production environment is created, the CI system runs nightly provisioning against these servers.</p>\n\n<p>This makes sure that all servers are up to date with the latest roles and fixes added to the playbooks. <br />\nThis is all good, but one thing it doesn't fully protect against is regression in the playbooks. Most tasks (as they should be!) does not download the Java installer if Java is already in place on the server, or do a clean install with configuration of RabbitMQ if RabbitMQ is up and running smoothly. Unless the environments are recycled often, you can never be <em>quite</em> sure that the Ansible playbooks and server setup will still work on the next clean install.</p>\n\n<p>Enter regression tests, and enter Packer.</p>\n\n<p>I'm about 5000 years late to the Packer party, but damn Hashicorp delivers as always. <br />\nIf you haven't heard about Packer, it is a tool for building up and packing artifacts, mostly VMs (or containers) that can be delivered to a cloud provider. Or in this case, work against vsphere. <br />\nThe normal Packer flow is to build up a VM from an ISO file with vmware player or workstation under the hood, run some <br />\nprovisioning and then upload the artifact to vsphere. <br />\nFor my regression tests, I don't want all this overhead. The good guys over at Jetbrains has developed a <a href=\"https://github.com/jetbrains-infra/packer-builder-vsphere\">plugin</a> for Packer that supports working directly with VMs or templates in the vpshere cluster itself, so the VMs can be created directly in the cluster.</p>\n\n<p>The following Packer configuration creates a VM from a bare minimum CentOS template, before running the RabbitMQ playbook. If anything goes wrong, Packer throws an exit code and deletes the temporary machine. Throw it all in for a nightly build, and we can now be confident that the playbook will work the next time we need to set up a RabbitMQ server.</p>\n\n<pre><code class=\"language-JSON\">{\n  \"variables\": {\n    \"vcenter_host\": \"vcenter01\",\n    \"vcenter_user\": \"admin@domain.local\",\n    \"vcenter_password\": \"{{env `VCENTERPASS`}}\",\n    \"ssh_user\": \"adminuser\",\n    \"ssh_private_key_file\": \"{{env `HOME`}}//.ssh/id_rsa\",\n    \"dc\": \"Datacenter\",\n    \"cluster\": \"MetroCluster\",\n    \"template_dir\": \"Application Servers/Team Optimus\",\n    \"resource_pool\": \"Team Optimus\",\n    \"template\": \"Ansible_CentOS73Packer\",\n    \"datastore\":            \"hus015sec-dev\",\n    \"vm_name\":  \"Ansible_CentOS73PackerRabbitMq\"\n  },\n\n  \"builders\": [\n    {\n      \"name\": \"RabbitMQ template test\",\n      \"type\": \"vsphere-clone\",\n\n      \"vcenter_server\":      \"{{ user `vcenter_host` }}\",\n      \"datacenter\":          \"{{ user `dc` }}\",\n      \"cluster\":             \"{{ user `cluster` }}\",\n      \"username\":            \"{{ user `vcenter_user` }}\",\n      \"password\":            \"{{ user `vcenter_password` }}\",\n      \"insecure_connection\": \"true\",\n      \"ssh_username\":        \"{{ user `ssh_user` }}\",\n      \"ssh_private_key_file\":  \"{{ user `ssh_private_key_file` }}\",\n      \"datastore\":            \"{{ user `datastore` }}\",\n      \"resource_pool\": \"{{ user `resource_pool` }}\",\n      \"folder\": \"{{ user `template_dir` }}\",\n      \"template\": \"{{ user `template` }}\",\n      \"vm_name\":  \"{{ user `vm_name` }}\",\n      \"convert_to_template\": false,\n      \"communicator\": \"ssh\"\n    }\n  ],\n\n  \"provisioners\": [\n    {\n      \"type\": \"ansible\",\n      \"playbook_file\": \"../RabbitMQ.yaml\",\n      \"host_alias\": \"rabbitmq\",\n      \"ansible_env_vars\": [ \"ANSIBLE_HOST_KEY_CHECKING=False\" ],\n      \"extra_arguments\": [ \"--verbose\" ],\n      \"user\": \"{{ user `ssh_user` }}\"\n    }\n  ]\n}\n</code></pre>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2018-08-03 11:34:53","created_by":1,"updated_at":"2018-08-03 11:37:08","updated_by":1,"published_at":"2018-08-03 11:37:08","published_by":1},{"id":29,"uuid":"dfab6aab-8c64-42a1-ad6e-88b132b0fb4c","title":"Trondheim Developer Conference 2018","slug":"trondheim-developer-conference-2018","markdown":"Three years since my last trip to [TDC](https://2018.trondheimdc.no/), I again got a talk approved.\nThis time I submitted the talk _Ansible: Configuration Management for Windows og Linux i skjønn forening_ (translates to something like _Ansible: Configuration Management for Windows and Linux in great harmony_)\n\nAs readers of my blog know, I use Ansible a lot when working on both Linux and Windows, so that topic should not come as a suprise.\n\n![](https://i.imgur.com/eZOT4GY.jpg)\n\nIn abstract, I talked about the journey DIPS has had in to the microservice pattern, and how that quickly revealed the need for configuration management and infrastructure as code, leading to the choice of Ansible over the other providers. I also gave demo at the end showing how to provision out both Windows and Linux services in the same environment.\n\n> I'm also pretty sure I jinxed the whole [Red Hat acquired by IBM story](https://www.redhat.com/en/about/press-releases/ibm-acquire-red-hat-completely-changing-cloud-landscape-and-becoming-worlds-1-hybrid-cloud-provider), by saying \"Ansible is Open Source and backed by Red Hat, which is the perfect company to have as a guardian\". Sorry about that, guys ;)\n\nMy slides and the talk itself (in Norwegian):\n\n<script async class=\"speakerdeck-embed\" data-id=\"e41471b7fd274d01b053c94af1b2b537\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n<iframe src=\"https://player.vimeo.com/video/296639173\" width=\"640\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>","mobiledoc":null,"html":"<p>Three years since my last trip to <a href=\"https://2018.trondheimdc.no/\">TDC</a>, I again got a talk approved. <br />\nThis time I submitted the talk <em>Ansible: Configuration Management for Windows og Linux i skjønn forening</em> (translates to something like <em>Ansible: Configuration Management for Windows and Linux in great harmony</em>)</p>\n\n<p>As readers of my blog know, I use Ansible a lot when working on both Linux and Windows, so that topic should not come as a suprise.</p>\n\n<p><img src=\"https://i.imgur.com/eZOT4GY.jpg\" alt=\"\" /></p>\n\n<p>In abstract, I talked about the journey DIPS has had in to the microservice pattern, and how that quickly revealed the need for configuration management and infrastructure as code, leading to the choice of Ansible over the other providers. I also gave demo at the end showing how to provision out both Windows and Linux services in the same environment.</p>\n\n<blockquote>\n  <p>I'm also pretty sure I jinxed the whole <a href=\"https://www.redhat.com/en/about/press-releases/ibm-acquire-red-hat-completely-changing-cloud-landscape-and-becoming-worlds-1-hybrid-cloud-provider\">Red Hat acquired by IBM story</a>, by saying \"Ansible is Open Source and backed by Red Hat, which is the perfect company to have as a guardian\". Sorry about that, guys ;)</p>\n</blockquote>\n\n<p>My slides and the talk itself (in Norwegian):</p>\n\n<script async class=\"speakerdeck-embed\" data-id=\"e41471b7fd274d01b053c94af1b2b537\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n<iframe src=\"https://player.vimeo.com/video/296639173\" width=\"640\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2018-11-08 11:55:53","created_by":1,"updated_at":"2018-11-08 12:02:13","updated_by":1,"published_at":"2018-11-08 12:02:13","published_by":1},{"id":30,"uuid":"4e562b9e-adf2-4d08-8b81-3e12617d76ef","title":"Ensure consistent Markdown style with Markdownlint","slug":"ensure-consistent-markdown-style-with-markdownlint","markdown":"Markdown is great. It's easy and flexible, and provides a good markup language even non-technical people can understand and enjoy. But, that flexibility and customizability can come at a cost. Document buildup can be done in many ways, and it can be hard to ensure consitency when working with multiple documents and contributors.\n\nI like to think of markup languages as code, and most code deserves a good style guide. [Markdownlint](https://github.com/DavidAnson/markdownlint) is a good alternative.\n\n`markdownlint` provides [a nice set of standard rules](https://github.com/DavidAnson/markdownlint/blob/master/doc/Rules.md) when writing markdown, like:\n\n* Heading levels should only increment by one level at a time\n* Lists should be surrounded by blank lines\n* First line in file should be a top level heading\n* No empty links\n* No trailing spaces\n* No multiple consecutive blank lines\n\nto name a few. It also ensures concistensy in headers, like:\n\n```markdown\nMy Heading\n===\n```\n\nvs.\n\n```markdown\n# My Heading\n```\n\nAnother smart rule is ensuring language description when writing code blocks.\n\nIf some rules don't fit your style or project, they can be overrided with a `.markdownlint.json` file:\n\n```markdown\n{\n    \"MD013\": false, // Disable line length rule.  \n    \"MD024\": false // Allow Multiple headings with the same content.\n}\n```\n\nThe easiest way to start using `markdownlint` is to install the extention for [VSCode](https://marketplace.visualstudio.com/items?itemName=DavidAnson.vscode-markdownlint) or [Atom](https://atom.io/packages/linter-node-markdownlint), or integrated with builds using [Grunt](https://github.com/sagiegurari/grunt-markdownlint) or [markdownlint-cli](https://github.com/igorshubovych/markdownlint-cli).\n\nFor my [Coffee recipes](https://github.com/andmos/Coffee) I use a simple container with Travis:\n\n```yaml\nsudo: required\nlanguage: generic\n\nservices:\n  - docker\nscript:\n  - docker run --rm -v $(pwd):/usr/src/app/files -it andmos/markdownlint *.md\n```\n\nIf any rules are broken, it breaks the build.","mobiledoc":null,"html":"<p>Markdown is great. It's easy and flexible, and provides a good markup language even non-technical people can understand and enjoy. But, that flexibility and customizability can come at a cost. Document buildup can be done in many ways, and it can be hard to ensure consitency when working with multiple documents and contributors.</p>\n\n<p>I like to think of markup languages as code, and most code deserves a good style guide. <a href=\"https://github.com/DavidAnson/markdownlint\">Markdownlint</a> is a good alternative.</p>\n\n<p><code>markdownlint</code> provides <a href=\"https://github.com/DavidAnson/markdownlint/blob/master/doc/Rules.md\">a nice set of standard rules</a> when writing markdown, like:</p>\n\n<ul>\n<li>Heading levels should only increment by one level at a time</li>\n<li>Lists should be surrounded by blank lines</li>\n<li>First line in file should be a top level heading</li>\n<li>No empty links</li>\n<li>No trailing spaces</li>\n<li>No multiple consecutive blank lines</li>\n</ul>\n\n<p>to name a few. It also ensures concistensy in headers, like:</p>\n\n<pre><code class=\"language-markdown\">My Heading  \n===\n</code></pre>\n\n<p>vs.</p>\n\n<pre><code class=\"language-markdown\"># My Heading\n</code></pre>\n\n<p>Another smart rule is ensuring language description when writing code blocks.</p>\n\n<p>If some rules don't fit your style or project, they can be overrided with a <code>.markdownlint.json</code> file:</p>\n\n<pre><code class=\"language-markdown\">{\n    \"MD013\": false, // Disable line length rule.  \n    \"MD024\": false // Allow Multiple headings with the same content.\n}\n</code></pre>\n\n<p>The easiest way to start using <code>markdownlint</code> is to install the extention for <a href=\"https://marketplace.visualstudio.com/items?itemName=DavidAnson.vscode-markdownlint\">VSCode</a> or <a href=\"https://atom.io/packages/linter-node-markdownlint\">Atom</a>, or integrated with builds using <a href=\"https://github.com/sagiegurari/grunt-markdownlint\">Grunt</a> or <a href=\"https://github.com/igorshubovych/markdownlint-cli\">markdownlint-cli</a>.</p>\n\n<p>For my <a href=\"https://github.com/andmos/Coffee\">Coffee recipes</a> I use a simple container with Travis:</p>\n\n<pre><code class=\"language-yaml\">sudo: required  \nlanguage: generic\n\nservices:  \n  - docker\nscript:  \n  - docker run --rm -v $(pwd):/usr/src/app/files -it andmos/markdownlint *.md\n</code></pre>\n\n<p>If any rules are broken, it breaks the build.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2019-01-05 16:03:16","created_by":1,"updated_at":"2019-01-05 16:13:35","updated_by":1,"published_at":"2019-01-05 16:04:00","published_by":1},{"id":31,"uuid":"04fb64ee-3fee-40a8-84c9-c8e2d9279c91","title":"Code Coverage for dotnet core with Coverlet, multi-stage Dockerfile and codecov.io","slug":"code-coverage-for-dotnet-core-with-coverlet-multistage-dockerfile-and-codecov-io","markdown":"## Enter Coverlet\n\nThe one thing I missed when moving away from full-framework and Visual Studio to VSCode and dotnet core, was simple code coverage.\n\nGiven the easy tooling `dotnet` provides, with `dotnet build`, `dotnet test` and `dotnet publish`, I looked for something that integrated nicely with these commands without adding to much complexity to the code project itself. After som googling, I stumbled over Scott Hanselman's [blogpost](https://www.hanselman.com/blog/NETCoreCodeCoverageAsAGlobalToolWithCoverlet.aspx) about a cool little project called [Coverlet](https://github.com/tonerdo/coverlet). Coverlet was just what I was looking for:\n\n>Coverlet is a cross platform code coverage library for .NET Core, with support for line, branch and method coverage.\n\n`coverlet` can be installed as a `dotnet tool` with\n\n```shell\ndotnet tool install --global coverlet.console\n```\n\nto make it globally available, providing it's own [CLI tool running directly at the test assemblies](https://github.com/tonerdo/coverlet#code-coverage).\n\nThe strategy I have settled on is using the `coverlet.msbuild` package that can be added to your test projects with\n```shell\ndotnet add package coverlet.msbuild\n```\n\nWhen using the `coverlet.msbuild` package, no extra setup is needed, and `coverlet` integrates directly with `dotnet test` with some extra parameters,\n\n```\ndotnet test /p:CollectCoverage=true /p:Threshold=80 /p:ThresholdType=line /p:CoverletOutputFormat=opencover\n```\n\nThe clue here is `/p:CollectCoverage=true`, the parameter that enables collection of code coverage. if no other option is specified, the coverage will be reported to the console when the tests are finished running:\n\n```shell\n+-----------------+--------+--------+--------+\n| Module          | Line   | Branch | Method |\n+-----------------+--------+--------+--------+\n| BikeshareClient | 93.2%  | 94.6%  | 85.7%  |\n+-----------------+--------+--------+--------+\n```\n\nNow the other parameters specified in the example is `/p:Threshold=80` and `/p:ThresholdType=line`. So if the code coverage drops below 80%, the build breaks here, while `/p:CoverletOutputFormat=opencover` writes a report in the [opencover](https://github.com/opencover/opencover/wiki/Reports) format.\n\n## Multi-stage Dockerfile\n\nFor most new projects, I have found myself using a simple `Dockerfile` along with some CI/CD tool like [Travis](https://travis-ci.org/), [AppVeyor](https://www.appveyor.com/) or [Azure Pipelines](https://azure.microsoft.com/nb-no/services/devops/pipelines/). This approach helps keeping the builds simple, as large `Dockerfiles` are harder to work with. The sole purpose of `Docker` is to keep things reproducible no mather the environment it builds and runs images in, so migrating from one CI provider to another is hardly any work. Building locally will always match the result on the CI system.\n\nBut, let's say build using [multi-stage Dockerfiles](https://docs.docker.com/develop/develop-images/multistage-build/). In a multi-stage build, we separate the SDK, build and test tools in one image, while copying the resulting artifacts to another image, more suitable for production runtimes. The rule is, have a small production image containing just what is needed for running your artifacts. Just one problem: How do we take care of that `coverage.opencover.xml` file? We don't what to transfer that file to the production image to grab hold of it, code coverage results don't belong in a production image.\n\nThankfully, `Docker` stores _layers_ that can be brought up after building the image.\nHere is our example multi-stage `Dockerfile`:\n\n<script src=\"https://gist.github.com/andmos/1ccfb13473a896f598cd51cccbe3fa4c.js\"></script>\n\nIn short, we build, test and publish the app with the `microsoft/dotnet:2.2-sdk` base image, before copying over the binaries to the `microsoft/dotnet:2.2-aspnetcore-runtime` image.\n\nTo use `coverlet` and extract code coverage, this line does the trick:\n\n```shell\nRUN dotnet test /p:CollectCoverage=true /p:Include=\"[BikeDashboard*]*\" /p:CoverletOutputFormat=opencover\n```\n\nNotice the `label` on line 3:\n\n```shell\nLABEL test=true\n```\n\nWith the label, it is possible to look up the id of the `docker build` _layer_ containing the code coverage file, create a container from that _image layer_ and use `docker copy` to grab hold of the coverage XML. Take a look:\n\n```shell\nexport id=$(docker images --filter \"label=test=true\" -q | head -1)\ndocker create --name testcontainer $id\ndocker cp testcontainer:/app/TestBikedashboard/coverage.opencover.xml .\n```\n\n## Wrapping it up with Travis and codecov.io\n\nSo now we have a simple build chain with a multi-stage `Dockerfile` and code coverage generation. As a last feature, the coverage report can be used by code coverage analyzers like [codecov.io](https://codecov.io/). codecov.io [integrates with Github](https://github.com/apps/codecov), and can automatically analyze incoming pull-request and break a build if coverage drops by merging the PR. Quite nifty.\n\nIntegrating codecov.io with CI systems like Travis is done with a one-liner, thanks to the provided [upload-script](https://docs.codecov.io/docs/about-the-codecov-bash-uploader). When using Travis, not even a token is required.\n\n`.travis` example file:\n\n<script src=\"https://gist.github.com/andmos/65143919934e8f5deeb02c6705f9e780.js\"></script>\n","mobiledoc":null,"html":"<h2 id=\"entercoverlet\">Enter Coverlet</h2>\n\n<p>The one thing I missed when moving away from full-framework and Visual Studio to VSCode and dotnet core, was simple code coverage.</p>\n\n<p>Given the easy tooling <code>dotnet</code> provides, with <code>dotnet build</code>, <code>dotnet test</code> and <code>dotnet publish</code>, I looked for something that integrated nicely with these commands without adding to much complexity to the code project itself. After som googling, I stumbled over Scott Hanselman's <a href=\"https://www.hanselman.com/blog/NETCoreCodeCoverageAsAGlobalToolWithCoverlet.aspx\">blogpost</a> about a cool little project called <a href=\"https://github.com/tonerdo/coverlet\">Coverlet</a>. Coverlet was just what I was looking for:</p>\n\n<blockquote>\n  <p>Coverlet is a cross platform code coverage library for .NET Core, with support for line, branch and method coverage.</p>\n</blockquote>\n\n<p><code>coverlet</code> can be installed as a <code>dotnet tool</code> with</p>\n\n<pre><code class=\"language-shell\">dotnet tool install --global coverlet.console  \n</code></pre>\n\n<p>to make it globally available, providing it's own <a href=\"https://github.com/tonerdo/coverlet#code-coverage\">CLI tool running directly at the test assemblies</a>.</p>\n\n<p>The strategy I have settled on is using the <code>coverlet.msbuild</code> package that can be added to your test projects with  </p>\n\n<pre><code class=\"language-shell\">dotnet add package coverlet.msbuild  \n</code></pre>\n\n<p>When using the <code>coverlet.msbuild</code> package, no extra setup is needed, and <code>coverlet</code> integrates directly with <code>dotnet test</code> with some extra parameters,</p>\n\n<pre><code>dotnet test /p:CollectCoverage=true /p:Threshold=80 /p:ThresholdType=line /p:CoverletOutputFormat=opencover  \n</code></pre>\n\n<p>The clue here is <code>/p:CollectCoverage=true</code>, the parameter that enables collection of code coverage. if no other option is specified, the coverage will be reported to the console when the tests are finished running:</p>\n\n<pre><code class=\"language-shell\">+-----------------+--------+--------+--------+\n| Module          | Line   | Branch | Method |\n+-----------------+--------+--------+--------+\n| BikeshareClient | 93.2%  | 94.6%  | 85.7%  |\n+-----------------+--------+--------+--------+\n</code></pre>\n\n<p>Now the other parameters specified in the example is <code>/p:Threshold=80</code> and <code>/p:ThresholdType=line</code>. So if the code coverage drops below 80%, the build breaks here, while <code>/p:CoverletOutputFormat=opencover</code> writes a report in the <a href=\"https://github.com/opencover/opencover/wiki/Reports\">opencover</a> format.</p>\n\n<h2 id=\"multistagedockerfile\">Multi-stage Dockerfile</h2>\n\n<p>For most new projects, I have found myself using a simple <code>Dockerfile</code> along with some CI/CD tool like <a href=\"https://travis-ci.org/\">Travis</a>, <a href=\"https://www.appveyor.com/\">AppVeyor</a> or <a href=\"https://azure.microsoft.com/nb-no/services/devops/pipelines/\">Azure Pipelines</a>. This approach helps keeping the builds simple, as large <code>Dockerfiles</code> are harder to work with. The sole purpose of <code>Docker</code> is to keep things reproducible no mather the environment it builds and runs images in, so migrating from one CI provider to another is hardly any work. Building locally will always match the result on the CI system.</p>\n\n<p>But, let's say build using <a href=\"https://docs.docker.com/develop/develop-images/multistage-build/\">multi-stage Dockerfiles</a>. In a multi-stage build, we separate the SDK, build and test tools in one image, while copying the resulting artifacts to another image, more suitable for production runtimes. The rule is, have a small production image containing just what is needed for running your artifacts. Just one problem: How do we take care of that <code>coverage.opencover.xml</code> file? We don't what to transfer that file to the production image to grab hold of it, code coverage results don't belong in a production image.</p>\n\n<p>Thankfully, <code>Docker</code> stores <em>layers</em> that can be brought up after building the image. <br />\nHere is our example multi-stage <code>Dockerfile</code>:</p>\n\n<script src=\"https://gist.github.com/andmos/1ccfb13473a896f598cd51cccbe3fa4c.js\"></script>\n\n<p>In short, we build, test and publish the app with the <code>microsoft/dotnet:2.2-sdk</code> base image, before copying over the binaries to the <code>microsoft/dotnet:2.2-aspnetcore-runtime</code> image.</p>\n\n<p>To use <code>coverlet</code> and extract code coverage, this line does the trick:</p>\n\n<pre><code class=\"language-shell\">RUN dotnet test /p:CollectCoverage=true /p:Include=\"[BikeDashboard*]*\" /p:CoverletOutputFormat=opencover  \n</code></pre>\n\n<p>Notice the <code>label</code> on line 3:</p>\n\n<pre><code class=\"language-shell\">LABEL test=true  \n</code></pre>\n\n<p>With the label, it is possible to look up the id of the <code>docker build</code> <em>layer</em> containing the code coverage file, create a container from that <em>image layer</em> and use <code>docker copy</code> to grab hold of the coverage XML. Take a look:</p>\n\n<pre><code class=\"language-shell\">export id=$(docker images --filter \"label=test=true\" -q | head -1)  \ndocker create --name testcontainer $id  \ndocker cp testcontainer:/app/TestBikedashboard/coverage.opencover.xml .  \n</code></pre>\n\n<h2 id=\"wrappingitupwithtravisandcodecovio\">Wrapping it up with Travis and codecov.io</h2>\n\n<p>So now we have a simple build chain with a multi-stage <code>Dockerfile</code> and code coverage generation. As a last feature, the coverage report can be used by code coverage analyzers like <a href=\"https://codecov.io/\">codecov.io</a>. codecov.io <a href=\"https://github.com/apps/codecov\">integrates with Github</a>, and can automatically analyze incoming pull-request and break a build if coverage drops by merging the PR. Quite nifty.</p>\n\n<p>Integrating codecov.io with CI systems like Travis is done with a one-liner, thanks to the provided <a href=\"https://docs.codecov.io/docs/about-the-codecov-bash-uploader\">upload-script</a>. When using Travis, not even a token is required.</p>\n\n<p><code>.travis</code> example file:</p>\n\n<script src=\"https://gist.github.com/andmos/65143919934e8f5deeb02c6705f9e780.js\"></script>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2019-05-26 13:32:42","created_by":1,"updated_at":"2019-05-26 13:38:51","updated_by":1,"published_at":"2019-05-26 13:38:51","published_by":1},{"id":32,"uuid":"3d9c2fe7-0b73-41cd-a03f-39929041ee9e","title":"Running GBFS bikeshare functions with OpenFaaS for fun and profit","slug":"running-gbfs-bikeshare-functions-with-openfaas-for-fun-and-profit","markdown":"## Intro\n\nMicro-mobility has gotten a lot of hype over the last couple of years. In many cities all over the world rentable city bikes, cargo bikes and electrical scooters has popped out and seriously changed the way people move, and it doesn't look like you can [spell \"smart city\" without micro-mobility](https://companies.bnpparibasfortis.be/en/article?n=will-micro-mobility-redesign-the-smart-city). The consulting company McKinsey is estimating that the value of the micro-mobility market will reach a [value of $200 billion to $300 billion in the United States in 2030](https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/micromobilitys-15000-mile-checkup). What gives micro-mobility systems advantage is the accessability and digital-first approach providers have taken. Most bikeshare systems are activated via smartphones, and the bikes themselves are IoT devices producing data [for the providers](https://urbansharing.com/) or [the public](https://trondheimbysykkel.no/en/open-data) to explore. To make micro-mobility more useful interoperability is important, and many standards have surfaced. One of these is the [General Bikeshare Feed Specification](https://github.com/NABSA/gbfs) (GBFS).\n\n## General Bikeshare Feed Specification (GBFS) and use cases\n\nGBFS is an open data standard for bikeshare systems developed by [North American Bikeshare Association](http://www.nabsa.net). GBFS provides info about the system itself, including real-time data like available stations, capacity, available bikes and locks etc. This API can be freely used to build 3rd party systems or apps. [How about a dashboard that shows your closest station and local weather forecast](https://github.com/andmos/BikeDashboard), or [Amazon Echo skill to check for available bikes](https://github.com/gffny/blue-bike-skill)?\nThe [GBFS systems overiew](https://github.com/NABSA/gbfs/blob/master/systems.csv) currently contains 228 providers using GBFS.\n\nA search for GBFS [on github topics](https://github.com/topics/gbfs) shows a lot of project integrating with the standard, including my [GBFS Bikeshare client for dotnet](https://github.com/andmos/BikeshareClient). This client is a great starting point for exploring several exiting concepts: serverless and function as a service.\n\n## Serverless, function as a service and OpenFaaS\n\n[Serverless architecture](https://martinfowler.com/articles/serverless.html) comes as a result of the rising popularity of cloud computing, where providers like Google, Microsoft and Amazon have raised the abstraction level when deploying software. At the infrastructure as a service (IaaS) level you have to mange VMs, the platform as a service (PaaS) level want your binaries or containers, while the function as a service (FaaS) provider needs one thing: your code. The runtime, scalability etc. Is taken care of by the cloud vendor.\n\nFaaS can be looked at as breaking up the [microservice pattern](https://martinfowler.com/articles/microservices.html) into smaller pieces. Examples of functions can be transforming input data and store it in a database, resize images, handle messages on a queue or, to stay in the micro-mobility domain, check the availability status on a bikeshare station.\n\nThe biggest criticism directed at serverless and FaaS is vendor lock-in. Amazon has Lambda, Microsoft has Azure Functions, and Google has Cloud Functions. Since these platforms require plain code to run, some platform specific boilerplate or project types are needed for each platform, thus not contributing to portability between vendors.\n\nSo vendor lock-in is one thing, but I would also like the possibility to run a serverless FaaS solution on that old VMWare cluster in the basement, the Mac mini rack or on the Raspberry Pi spotted in the wild. Luckily, [OpenFaaS](https://www.openfaas.com/) is here to help. OpenFaaS is a framework that leverages container technology to run functions in containers on top of orchestrators like Docker Swarm and Kubernetes. This breaks the FaaS architecture free from the cloud vendors, providing the freedom to deploy serverless application in any environment offering a container orchestrator.\n\n![OpenFaaS architecture](https://pbs.twimg.com/media/DFrkF4NXoAAJwN2.jpg)\nThe OpenFaaS architecture.\n\n## Baby's first GBFS bikeshare functions\n\nSo let's put OpenFaaS to work and build some GBFS powered bikeshare functions.\n\nFor development purposes, running OpenFaaS via Docker Swarm is a good approach.\n[The installation of OpenFaaS and initialization of a Swarm cluster is straight forward](https://docs.openfaas.com/deployment/docker-swarm/).\n\nMy preferred language is `C#` and `dotnet core`, so to write the `dotnet` function a [template](https://docs.openfaas.com/cli/templates/#templates) is needed.\nGithub user [burtonr](https://github.com/burtonr) has written a nice [dotnet template](https://github.com/burtonr/csharp-kestrel-template) with Kestrel and async support, perfect for high performance HTTP functions. To fetch the template:\n\n```shell\n$ faas-cli template pull https://github.com/burtonr/csharp-kestrel-template\n```\n\nNext, let's have a look at what a typical function might look like. The [GBFS systems list](https://github.com/NABSA/gbfs/blob/master/systems.csv) is currently in `CSV` format, but I prefer to abstract it away and offer it as `JSON` via HTTP endpoint.\n\nTo create a new OpenFaaS function:\n\n```shell\n$ faas-cli new --lang csharp-kestrel gbfs-systems-function\nFunction created in folder: gbfs-systems-function\nStack file written: gbfs-systems-function.yml\n```\n\nAs the output shows, a folder for the function and `stack` file are now created, next to the template.\n\n```shell\n$ tree\n.\n├── gbfs-systems-function\n│   ├── FunctionHandler.cs\n│   └── FunctionHandler.csproj\n├── gbfs-systems-function.yml\n└── template\n    └── csharp-kestrel\n        ├── Dockerfile\n        ├── Program.cs\n        ├── Startup.cs\n        ├── function\n        │   ├── FunctionHandler.cs\n        │   └── FunctionHandler.csproj\n        ├── root.csproj\n        └── template.yml\n```\n\nTo write the function, `gbfs-systems-function/FunctionHandler.cs` is edited. Here is the full function:\n\n<script src=\"https://gist.github.com/andmos/13822f83e76cfab2b764d40f6ca884e8.js\"></script>\n\nThe `Handle` method is entrypoint for the function. In this case the input is not validated, but upon triggering the function parses the `systems.csv` file and serializes it as JSON.\n\nTo build and deploy the function, take a look at the `stack` file:\n\n<script src=\"https://gist.github.com/andmos/5f8244497cf2bf36687d14d8ffdb3e7d.js\"></script>\n\nNotice the `image:` tag. Since OpenFaaS runs functions in Docker containers, this is the name of the container image that is created, and is the artifact of the build.\n\nTo build, deploy and trigger the function:\n\n```shell\n$ faas-cli build -f gbfs-systems-function.yml\n$ faas-cli deploy -f gbfs-systems-function.yml\n\n$ echo \"\" |faas-cli invoke gbfs-systems-function # or via Curl\n$ curl -d \"\" localhost:8080/function/gbfs-systems-function\n```\n\nThe two last commands will return `JSON` straight from the new function.\n\nTo push the image to [Docker hub](https://hub.docker.com/):\n\n```shell\n$ faas-cli push -f gbfs-systems-function.yml\n```\n\n## Building a GBFS powered Slack bot\n\nSo that example function was rather simple. Let's make something more useful: A Slack bot for showing available bikes and locks at bikeshare stations. Right off the bat this seems like a nice use case for multiple functions, since a function should optimally do only one thing. So for the bot we need a `bikeshare-function` that takes the name of a bikeshare systems station as input, and return the number of available bikes and locks for this station as output.\n\nNot much code needed here neither:\n\n<script src=\"https://gist.github.com/andmos/a8116f0121bd8e75d4d3371a01642775.js\"></script>\n\nTo link the function ta a GBFS system, the GBFS discovery URL must be exposed to the function via environment variable in the `stack` file.\n\nWhen deployed, the `bikeshare-function` can be invoked:\n\n```shell\n$ curl -d \"skansen\" localhost:8080/function/bikeshare-function\n{\"Name\":\"Skansen\",\"BikesAvailable\":19,\"LocksAvailable\":2}\n```\n\nThe next function is the Slack bot itself. The bot will trigger on mentions and call the `bikeshare-function` with a bikeshare station name. For creating Slack apps, [see the documentation](https://api.slack.com/slack-apps).\n\nAs expected, under 100 lines of code here too:\n\n<script src=\"https://gist.github.com/andmos/6e992d653377f7bb934ddb817cf47388.js\"></script>\n\nNow there are a couple of things to notice here.\nFor the bot to be able to call the `bikeshare-function`, it needs to know where the [OpenFaaS API gateway](https://docs.openfaas.com/architecture/gateway/) is. When running via Docker Swarm this address defaults to `http://gateway:8080/`, but it is [recommended to make this address customizable](https://github.com/openfaas/workshop/blob/master/lab4.md#call-one-function-from-another), as it may vary from environment to environment and other orchestrators.\n\nThe next thing to notice is secrets. The bot needs a OAUTH token when connecting to Slack, and this token should be kept secret. OpenFaaS [integrates with Swarm and Kubernetes secrets](https://docs.openfaas.com/reference/secrets/), both reachable from `openfaas-cli`:\n\n```shell\n$ faas-cli secret create secret-api-key \\\n  --from-file=slackToken.txt\n```\n\nThe token is then written to a file inside the container that must be read up:\n```csharp\nvar botToken = File.ReadAllText(@\"/var/openfaas/secrets/bikeBotSlackToken\");\n```\n\nThe final `stack` file:\n\n<script src=\"https://gist.github.com/andmos/b7037ab2266393737db10097366bc20f.js\"></script>\n\n\nTo initialize the Slack bot:\n```shell\n$ curl -d \"\" localhost:8080/function/bikeshare-slack-function\nBot initializing\n```\n\nThe bot is now online and can be asked for station status:\n\n<img width=\"651\" alt=\"Skjermbilde 2019-06-04 kl  22 00 29\" src=\"https://user-images.githubusercontent.com/1283556/58909797-4d732c00-8714-11e9-8bf6-026fe7e1dff5.png\">\n\n## Conclusion\n\nIt is exiting times for micro-mobility and the city of the future. For solutions like bikeshare systems to reach it's potential and help cities become more accessible, integration with other smart city systems is almost a requirement. Thanks to open standards like GBFS and the serverless paradigm, creating new application leveraging and combining data is only a couple of code lines away. Frameworks like OpenFaaS helps democratize serverless and FaaS, giving developers tools to run functions where and how they want.\n\nFinally, all code for this post [can be found on Github.](https://github.com/andmos/BikeshareFunction).\n","mobiledoc":null,"html":"<h2 id=\"intro\">Intro</h2>\n\n<p>Micro-mobility has gotten a lot of hype over the last couple of years. In many cities all over the world rentable city bikes, cargo bikes and electrical scooters has popped out and seriously changed the way people move, and it doesn't look like you can <a href=\"https://companies.bnpparibasfortis.be/en/article?n=will-micro-mobility-redesign-the-smart-city\">spell \"smart city\" without micro-mobility</a>. The consulting company McKinsey is estimating that the value of the micro-mobility market will reach a <a href=\"https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/micromobilitys-15000-mile-checkup\">value of $200 billion to $300 billion in the United States in 2030</a>. What gives micro-mobility systems advantage is the accessability and digital-first approach providers have taken. Most bikeshare systems are activated via smartphones, and the bikes themselves are IoT devices producing data <a href=\"https://urbansharing.com/\">for the providers</a> or <a href=\"https://trondheimbysykkel.no/en/open-data\">the public</a> to explore. To make micro-mobility more useful interoperability is important, and many standards have surfaced. One of these is the <a href=\"https://github.com/NABSA/gbfs\">General Bikeshare Feed Specification</a> (GBFS).</p>\n\n<h2 id=\"generalbikesharefeedspecificationgbfsandusecases\">General Bikeshare Feed Specification (GBFS) and use cases</h2>\n\n<p>GBFS is an open data standard for bikeshare systems developed by <a href=\"http://www.nabsa.net\">North American Bikeshare Association</a>. GBFS provides info about the system itself, including real-time data like available stations, capacity, available bikes and locks etc. This API can be freely used to build 3rd party systems or apps. <a href=\"https://github.com/andmos/BikeDashboard\">How about a dashboard that shows your closest station and local weather forecast</a>, or <a href=\"https://github.com/gffny/blue-bike-skill\">Amazon Echo skill to check for available bikes</a>? <br />\nThe <a href=\"https://github.com/NABSA/gbfs/blob/master/systems.csv\">GBFS systems overiew</a> currently contains 228 providers using GBFS.</p>\n\n<p>A search for GBFS <a href=\"https://github.com/topics/gbfs\">on github topics</a> shows a lot of project integrating with the standard, including my <a href=\"https://github.com/andmos/BikeshareClient\">GBFS Bikeshare client for dotnet</a>. This client is a great starting point for exploring several exiting concepts: serverless and function as a service.</p>\n\n<h2 id=\"serverlessfunctionasaserviceandopenfaas\">Serverless, function as a service and OpenFaaS</h2>\n\n<p><a href=\"https://martinfowler.com/articles/serverless.html\">Serverless architecture</a> comes as a result of the rising popularity of cloud computing, where providers like Google, Microsoft and Amazon have raised the abstraction level when deploying software. At the infrastructure as a service (IaaS) level you have to mange VMs, the platform as a service (PaaS) level want your binaries or containers, while the function as a service (FaaS) provider needs one thing: your code. The runtime, scalability etc. Is taken care of by the cloud vendor.</p>\n\n<p>FaaS can be looked at as breaking up the <a href=\"https://martinfowler.com/articles/microservices.html\">microservice pattern</a> into smaller pieces. Examples of functions can be transforming input data and store it in a database, resize images, handle messages on a queue or, to stay in the micro-mobility domain, check the availability status on a bikeshare station.</p>\n\n<p>The biggest criticism directed at serverless and FaaS is vendor lock-in. Amazon has Lambda, Microsoft has Azure Functions, and Google has Cloud Functions. Since these platforms require plain code to run, some platform specific boilerplate or project types are needed for each platform, thus not contributing to portability between vendors.</p>\n\n<p>So vendor lock-in is one thing, but I would also like the possibility to run a serverless FaaS solution on that old VMWare cluster in the basement, the Mac mini rack or on the Raspberry Pi spotted in the wild. Luckily, <a href=\"https://www.openfaas.com/\">OpenFaaS</a> is here to help. OpenFaaS is a framework that leverages container technology to run functions in containers on top of orchestrators like Docker Swarm and Kubernetes. This breaks the FaaS architecture free from the cloud vendors, providing the freedom to deploy serverless application in any environment offering a container orchestrator.</p>\n\n<p><img src=\"https://pbs.twimg.com/media/DFrkF4NXoAAJwN2.jpg\" alt=\"OpenFaaS architecture\" />\nThe OpenFaaS architecture.</p>\n\n<h2 id=\"babysfirstgbfsbikesharefunctions\">Baby's first GBFS bikeshare functions</h2>\n\n<p>So let's put OpenFaaS to work and build some GBFS powered bikeshare functions.</p>\n\n<p>For development purposes, running OpenFaaS via Docker Swarm is a good approach. <br />\n<a href=\"https://docs.openfaas.com/deployment/docker-swarm/\">The installation of OpenFaaS and initialization of a Swarm cluster is straight forward</a>.</p>\n\n<p>My preferred language is <code>C#</code> and <code>dotnet core</code>, so to write the <code>dotnet</code> function a <a href=\"https://docs.openfaas.com/cli/templates/#templates\">template</a> is needed. <br />\nGithub user <a href=\"https://github.com/burtonr\">burtonr</a> has written a nice <a href=\"https://github.com/burtonr/csharp-kestrel-template\">dotnet template</a> with Kestrel and async support, perfect for high performance HTTP functions. To fetch the template:</p>\n\n<pre><code class=\"language-shell\">$ faas-cli template pull https://github.com/burtonr/csharp-kestrel-template\n</code></pre>\n\n<p>Next, let's have a look at what a typical function might look like. The <a href=\"https://github.com/NABSA/gbfs/blob/master/systems.csv\">GBFS systems list</a> is currently in <code>CSV</code> format, but I prefer to abstract it away and offer it as <code>JSON</code> via HTTP endpoint.</p>\n\n<p>To create a new OpenFaaS function:</p>\n\n<pre><code class=\"language-shell\">$ faas-cli new --lang csharp-kestrel gbfs-systems-function\nFunction created in folder: gbfs-systems-function  \nStack file written: gbfs-systems-function.yml  \n</code></pre>\n\n<p>As the output shows, a folder for the function and <code>stack</code> file are now created, next to the template.</p>\n\n<pre><code class=\"language-shell\">$ tree\n.\n├── gbfs-systems-function\n│   ├── FunctionHandler.cs\n│   └── FunctionHandler.csproj\n├── gbfs-systems-function.yml\n└── template\n    └── csharp-kestrel\n        ├── Dockerfile\n        ├── Program.cs\n        ├── Startup.cs\n        ├── function\n        │   ├── FunctionHandler.cs\n        │   └── FunctionHandler.csproj\n        ├── root.csproj\n        └── template.yml\n</code></pre>\n\n<p>To write the function, <code>gbfs-systems-function/FunctionHandler.cs</code> is edited. Here is the full function:</p>\n\n<script src=\"https://gist.github.com/andmos/13822f83e76cfab2b764d40f6ca884e8.js\"></script>\n\n<p>The <code>Handle</code> method is entrypoint for the function. In this case the input is not validated, but upon triggering the function parses the <code>systems.csv</code> file and serializes it as JSON.</p>\n\n<p>To build and deploy the function, take a look at the <code>stack</code> file:</p>\n\n<script src=\"https://gist.github.com/andmos/5f8244497cf2bf36687d14d8ffdb3e7d.js\"></script>\n\n<p>Notice the <code>image:</code> tag. Since OpenFaaS runs functions in Docker containers, this is the name of the container image that is created, and is the artifact of the build.</p>\n\n<p>To build, deploy and trigger the function:</p>\n\n<pre><code class=\"language-shell\">$ faas-cli build -f gbfs-systems-function.yml\n$ faas-cli deploy -f gbfs-systems-function.yml\n\n$ echo \"\" |faas-cli invoke gbfs-systems-function # or via Curl\n$ curl -d \"\" localhost:8080/function/gbfs-systems-function\n</code></pre>\n\n<p>The two last commands will return <code>JSON</code> straight from the new function.</p>\n\n<p>To push the image to <a href=\"https://hub.docker.com/\">Docker hub</a>:</p>\n\n<pre><code class=\"language-shell\">$ faas-cli push -f gbfs-systems-function.yml\n</code></pre>\n\n<h2 id=\"buildingagbfspoweredslackbot\">Building a GBFS powered Slack bot</h2>\n\n<p>So that example function was rather simple. Let's make something more useful: A Slack bot for showing available bikes and locks at bikeshare stations. Right off the bat this seems like a nice use case for multiple functions, since a function should optimally do only one thing. So for the bot we need a <code>bikeshare-function</code> that takes the name of a bikeshare systems station as input, and return the number of available bikes and locks for this station as output.</p>\n\n<p>Not much code needed here neither:</p>\n\n<script src=\"https://gist.github.com/andmos/a8116f0121bd8e75d4d3371a01642775.js\"></script>\n\n<p>To link the function ta a GBFS system, the GBFS discovery URL must be exposed to the function via environment variable in the <code>stack</code> file.</p>\n\n<p>When deployed, the <code>bikeshare-function</code> can be invoked:</p>\n\n<pre><code class=\"language-shell\">$ curl -d \"skansen\" localhost:8080/function/bikeshare-function\n{\"Name\":\"Skansen\",\"BikesAvailable\":19,\"LocksAvailable\":2}\n</code></pre>\n\n<p>The next function is the Slack bot itself. The bot will trigger on mentions and call the <code>bikeshare-function</code> with a bikeshare station name. For creating Slack apps, <a href=\"https://api.slack.com/slack-apps\">see the documentation</a>.</p>\n\n<p>As expected, under 100 lines of code here too:</p>\n\n<script src=\"https://gist.github.com/andmos/6e992d653377f7bb934ddb817cf47388.js\"></script>\n\n<p>Now there are a couple of things to notice here. <br />\nFor the bot to be able to call the <code>bikeshare-function</code>, it needs to know where the <a href=\"https://docs.openfaas.com/architecture/gateway/\">OpenFaaS API gateway</a> is. When running via Docker Swarm this address defaults to <code>http://gateway:8080/</code>, but it is <a href=\"https://github.com/openfaas/workshop/blob/master/lab4.md#call-one-function-from-another\">recommended to make this address customizable</a>, as it may vary from environment to environment and other orchestrators.</p>\n\n<p>The next thing to notice is secrets. The bot needs a OAUTH token when connecting to Slack, and this token should be kept secret. OpenFaaS <a href=\"https://docs.openfaas.com/reference/secrets/\">integrates with Swarm and Kubernetes secrets</a>, both reachable from <code>openfaas-cli</code>:</p>\n\n<pre><code class=\"language-shell\">$ faas-cli secret create secret-api-key \\\n  --from-file=slackToken.txt\n</code></pre>\n\n<p>The token is then written to a file inside the container that must be read up:  </p>\n\n<pre><code class=\"language-csharp\">var botToken = File.ReadAllText(@\"/var/openfaas/secrets/bikeBotSlackToken\");  \n</code></pre>\n\n<p>The final <code>stack</code> file:</p>\n\n<script src=\"https://gist.github.com/andmos/b7037ab2266393737db10097366bc20f.js\"></script>\n\n<p>To initialize the Slack bot:  </p>\n\n<pre><code class=\"language-shell\">$ curl -d \"\" localhost:8080/function/bikeshare-slack-function\nBot initializing  \n</code></pre>\n\n<p>The bot is now online and can be asked for station status:</p>\n\n<p><img width=\"651\" alt=\"Skjermbilde 2019-06-04 kl  22 00 29\" src=\"https://user-images.githubusercontent.com/1283556/58909797-4d732c00-8714-11e9-8bf6-026fe7e1dff5.png\"></p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>It is exiting times for micro-mobility and the city of the future. For solutions like bikeshare systems to reach it's potential and help cities become more accessible, integration with other smart city systems is almost a requirement. Thanks to open standards like GBFS and the serverless paradigm, creating new application leveraging and combining data is only a couple of code lines away. Frameworks like OpenFaaS helps democratize serverless and FaaS, giving developers tools to run functions where and how they want.</p>\n\n<p>Finally, all code for this post <a href=\"https://github.com/andmos/BikeshareFunction\">can be found on Github.</a>.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2019-06-04 20:15:50","created_by":1,"updated_at":"2020-06-09 12:24:54","updated_by":1,"published_at":"2019-06-04 20:22:59","published_by":1},{"id":33,"uuid":"9c4507a5-3c83-4582-8749-e92f2e73c8aa","title":"Github Actions and publishing artifacts to Azure Blob Storage","slug":"github-actions-and-publishing-artifacts-to-azure-blob-storage","markdown":"### Intro\n\n[Github Actions](https://github.com/features/actions) is a welcomed edition to the (still) growing world of CI/CD tools.\nSince Actions is Github's own tool, it integrates more closely to your repo and the Github Workflow, with actions to automate tasks around issues, pull-requests, releases etc. Writing a task that regularly, say, check issues and mark them as stalled if it hasn't been any activity for some time has, would mean leveraging the Github API when running in other tools, while abstractions for these kinds of integrations are present directly in Github Actions. That makes their \"workflow\" semantics more comprehensive than just plain CI/CD capabilities.\n\nExtendability is at the core of Github Actions. All workflows consists of one to many actions, and these actions can be run natively or via containers. Referencing a third party actions is as easy as knowing the action's github namespace.\n\n<script src=\"https://gist.github.com/andmos/22a0276f9288c9eb281fc49e6833a114.js\"></script>\n\nIn this example the _workflow_ `Tests` will trigger on `git push`, run on macOS, Ubuntu and Windows, checkout code with the action [actions/checkout](https://github.com/actions/checkout), and install Python via [actions/setup-python](https:(//github.com/actions/setup-python). These two actions are \"official\", hence the \"actions\" namespace. The next task installs the Python package manager [Poetry](https://poetry.eustace.io/) and is a third party action: [dschep/install-poetry-action](https://github.com/dschep/install-poetry-action). Since these actions directly reference living repositories, specifing a release or branch (`dschep/install-poetry-action@v1.2`) will save you from some unpleasant discoveries.\n\nTo speed up the build, these actions can be run directly from DockerHub:\n\n<script src=\"https://gist.github.com/andmos/1ddb8949fba768fc6373c91beab4f7a1.js\"></script>\n\n\n\n### Uscase: Upload artifacts to Azure Blob Storage\n\nGithub Actions is still in it's early days, so there are not actions for everything just yet. The other day I needed a workflow to build and publish an Electron App to MacOS and Linux, with the artifacts stored in Azure Blob Storage. In Azure Blob Storage we have two containers, one for `dev` and one for `release`, so the app can be tested before released out to the world. Here is the workflow:\n\n<script src=\"https://gist.github.com/andmos/416601771109493b49aba3591e3f7c2c.js\"></script>\n\nSo this workflow will only trigger on push to the `dev` branch. \n[actions/setup-node@master](https://github.com/actions/setup-node) installs node on version `12.10`, and `electron-builder` is used to package and sign (the macOS) app. Here we also see secrets in play, `${{ secrets.BASE_64_CERT }}` holds an encryptet\nsigning certificate.\n\nFinally, I could not find a suitable action for uploading to Azure Blob Storage directly, but going via [azure/actions/login](https://github.com/azure/actions/) worked great to auth against Azure and give access to the `az` CLI. All that is needed is generating [Azure Service Principal](https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?view=azure-cli-latest) to give the workflow access to just the Blob Storage Containers, store them as a secret and we are good to go. For a great intro to using dotnet core with Github Actions, check out [Runar's post](https://hjerpbakk.com/blog/2019/10/03/asp-net-core-and-github-actions).\n","mobiledoc":null,"html":"<h3 id=\"intro\">Intro</h3>\n\n<p><a href=\"https://github.com/features/actions\">Github Actions</a> is a welcomed edition to the (still) growing world of CI/CD tools.\nSince Actions is Github's own tool, it integrates more closely to your repo and the Github Workflow, with actions to automate tasks around issues, pull-requests, releases etc. Writing a task that regularly, say, check issues and mark them as stalled if it hasn't been any activity for some time has, would mean leveraging the Github API when running in other tools, while abstractions for these kinds of integrations are present directly in Github Actions. That makes their \"workflow\" semantics more comprehensive than just plain CI/CD capabilities.</p>\n\n<p>Extendability is at the core of Github Actions. All workflows consists of one to many actions, and these actions can be run natively or via containers. Referencing a third party actions is as easy as knowing the action's github namespace.</p>\n\n<script src=\"https://gist.github.com/andmos/22a0276f9288c9eb281fc49e6833a114.js\"></script>\n\n<p>In this example the <em>workflow</em> <code>Tests</code> will trigger on <code>git push</code>, run on macOS, Ubuntu and Windows, checkout code with the action <a href=\"https://github.com/actions/checkout\">actions/checkout</a>, and install Python via <a href=\"https:(//github.com/actions/setup-python). These two actions are \"official\", hence the \"actions\" namespace. The next task installs the Python package manager [Poetry](https://poetry.eustace.io/\">actions/setup-python</a> and is a third party action: <a href=\"https://github.com/dschep/install-poetry-action\">dschep/install-poetry-action</a>. Since these actions directly reference living repositories, specifing a release or branch (<code>dschep/install-poetry-action@v1.2</code>) will save you from some unpleasant discoveries.</p>\n\n<p>To speed up the build, these actions can be run directly from DockerHub:</p>\n\n<script src=\"https://gist.github.com/andmos/1ddb8949fba768fc6373c91beab4f7a1.js\"></script>\n\n<h3 id=\"uscaseuploadartifactstoazureblobstorage\">Uscase: Upload artifacts to Azure Blob Storage</h3>\n\n<p>Github Actions is still in it's early days, so there are not actions for everything just yet. The other day I needed a workflow to build and publish an Electron App to MacOS and Linux, with the artifacts stored in Azure Blob Storage. In Azure Blob Storage we have two containers, one for <code>dev</code> and one for <code>release</code>, so the app can be tested before released out to the world. Here is the workflow:</p>\n\n<script src=\"https://gist.github.com/andmos/416601771109493b49aba3591e3f7c2c.js\"></script>\n\n<p>So this workflow will only trigger on push to the <code>dev</code> branch. <br />\n<a href=\"https://github.com/actions/setup-node\">actions/setup-node@master</a> installs node on version <code>12.10</code>, and <code>electron-builder</code> is used to package and sign (the macOS) app. Here we also see secrets in play, <code>${{ secrets.BASE_64_CERT }}</code> holds an encryptet\nsigning certificate.</p>\n\n<p>Finally, I could not find a suitable action for uploading to Azure Blob Storage directly, but going via <a href=\"https://github.com/azure/actions/\">azure/actions/login</a> worked great to auth against Azure and give access to the <code>az</code> CLI. All that is needed is generating <a href=\"https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?view=azure-cli-latest\">Azure Service Principal</a> to give the workflow access to just the Blob Storage Containers, store them as a secret and we are good to go. For a great intro to using dotnet core with Github Actions, check out <a href=\"https://hjerpbakk.com/blog/2019/10/03/asp-net-core-and-github-actions\">Runar's post</a>.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2019-10-03 16:55:25","created_by":1,"updated_at":"2019-10-05 11:15:22","updated_by":1,"published_at":"2019-10-03 16:59:13","published_by":1},{"id":34,"uuid":"48e6b568-63f1-434d-8518-6a78915cc32c","title":"Automate Docker base image updates with Watchtower","slug":"automate-docker-base-image-updates-with-watchtower","markdown":"If you host some simple hobby services with plain old Docker, chances are high that you have been thinking about how to automate the deployment process. If the services are small enough and you host them on your own servers or VM's, going to the PaaS cloud or introducing Kubernetes with a sophisticated CI/CD pipeline is, in most cases, total overkill.\n\nWhy invest more time in setting up the complicated hosting and scheduling platform than it took to write that 500 lines single-container web service?\n\nDon't fear, [watchtower](https://containrrr.github.io/watchtower/) is here.\n\nWatchtower is a single process container that runs on your system and _polls_ a container registry (private or public, like Dockerhub) at given intervals to check for new versions of the base image on the running container(s) you want to update. If it detects a new image, Watchtower stores the parameters used to start the running container, like startup argument and environment variables, pulls down the new image, stops the running container, and starts it up again, with the same parameters, but with the new image. Easy as that. This simplifies the deployment process, and the only automation needed is a CI pipeline that builds the service's container image and pushes it to the registry when code is committed. Here is an example from my simple [ReadingList](https://github.com/andmos/ReadingList) API:\n\nIn `.travis.yml`:\n\n<script src=\"https://gist.github.com/andmos/4783be0dda67cd8e74d598ef92c6006b.js\"></script>\n\nOn push to master, if the build and test steps are successful, the image is tagged as `latest` and pushed to DockerHub. Nothing more to it.\n\nNow on my private [Linode](https://welcome.linode.com/) server the ReadingList API has been started _once_ with the correct env-variables, so it's running as intended. Then Watchtower comes in:\n\n```shell\ndocker run -d --name watchtower  -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower readinglist\n```\n\nWatchtower is now running and has access to `docker.sock` to be able to start and stop container.\n\nWhen a change to ReadingList is merged and new image pushed, watchtower kicks in and replaces the running container with a new, updated version:\n\n```shell\ndocker logs watchtower\ntime=\"2019-10-30T13:06:39Z\" level=info msg=\"Found new andmos/readinglist:latest image (sha256:803aa566d2dd53f3ec774406f6bd8e20cb4e926006cdc1012dc663e206fbc9dc)\"\ntime=\"2019-10-30T13:06:40Z\" level=info msg=\"Stopping /readinglist (45ceaaa2de930b22424438d1f2d078796feead127b0ab578c0ff8ac14dc8630e) with SIGTERM\"\ntime=\"2019-10-30T13:06:41Z\" level=info msg=\"Creating /readinglist\"\ntime=\"2019-10-30T13:13:38Z\" level=info msg=\"Waiting for running update to be finished...\"\n```\n\nAnd thats it. Be mindful, this approach has limitations. When managing larger multi-container systems in production something like Kubernetes and a more thorough setup is recommended, but for the casual, single container hobby project Watchtower works just fine.\n","mobiledoc":null,"html":"<p>If you host some simple hobby services with plain old Docker, chances are high that you have been thinking about how to automate the deployment process. If the services are small enough and you host them on your own servers or VM's, going to the PaaS cloud or introducing Kubernetes with a sophisticated CI/CD pipeline is, in most cases, total overkill.</p>\n\n<p>Why invest more time in setting up the complicated hosting and scheduling platform than it took to write that 500 lines single-container web service?</p>\n\n<p>Don't fear, <a href=\"https://containrrr.github.io/watchtower/\">watchtower</a> is here.</p>\n\n<p>Watchtower is a single process container that runs on your system and <em>polls</em> a container registry (private or public, like Dockerhub) at given intervals to check for new versions of the base image on the running container(s) you want to update. If it detects a new image, Watchtower stores the parameters used to start the running container, like startup argument and environment variables, pulls down the new image, stops the running container, and starts it up again, with the same parameters, but with the new image. Easy as that. This simplifies the deployment process, and the only automation needed is a CI pipeline that builds the service's container image and pushes it to the registry when code is committed. Here is an example from my simple <a href=\"https://github.com/andmos/ReadingList\">ReadingList</a> API:</p>\n\n<p>In <code>.travis.yml</code>:</p>\n\n<script src=\"https://gist.github.com/andmos/4783be0dda67cd8e74d598ef92c6006b.js\"></script>\n\n<p>On push to master, if the build and test steps are successful, the image is tagged as <code>latest</code> and pushed to DockerHub. Nothing more to it.</p>\n\n<p>Now on my private <a href=\"https://welcome.linode.com/\">Linode</a> server the ReadingList API has been started <em>once</em> with the correct env-variables, so it's running as intended. Then Watchtower comes in:</p>\n\n<pre><code class=\"language-shell\">docker run -d --name watchtower  -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower readinglist  \n</code></pre>\n\n<p>Watchtower is now running and has access to <code>docker.sock</code> to be able to start and stop container.</p>\n\n<p>When a change to ReadingList is merged and new image pushed, watchtower kicks in and replaces the running container with a new, updated version:</p>\n\n<pre><code class=\"language-shell\">docker logs watchtower  \ntime=\"2019-10-30T13:06:39Z\" level=info msg=\"Found new andmos/readinglist:latest image (sha256:803aa566d2dd53f3ec774406f6bd8e20cb4e926006cdc1012dc663e206fbc9dc)\"  \ntime=\"2019-10-30T13:06:40Z\" level=info msg=\"Stopping /readinglist (45ceaaa2de930b22424438d1f2d078796feead127b0ab578c0ff8ac14dc8630e) with SIGTERM\"  \ntime=\"2019-10-30T13:06:41Z\" level=info msg=\"Creating /readinglist\"  \ntime=\"2019-10-30T13:13:38Z\" level=info msg=\"Waiting for running update to be finished...\"  \n</code></pre>\n\n<p>And thats it. Be mindful, this approach has limitations. When managing larger multi-container systems in production something like Kubernetes and a more thorough setup is recommended, but for the casual, single container hobby project Watchtower works just fine.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2019-11-03 09:42:37","created_by":1,"updated_at":"2019-11-03 09:43:52","updated_by":1,"published_at":"2019-11-03 09:43:52","published_by":1},{"id":35,"uuid":"f369dc9d-7e66-4773-9aa1-55bed4d422a9","title":"Local reverse-proxy with Nginx, mkcert and Docker-Compose","slug":"local-reverse-proxy-with-nginx-mkcert-and-docker-compose","markdown":"## Good practices from the Twelve-Factor app\nWhen developing modern web application or services, the [Twelve-factor app](https://12factor.net/port-binding) taught us that our services\n\n>is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port.\n\nWhat this means is that our apps written with modern frameworks (like [ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-3.1)) should provide their own web-servers, exposing a HTTP port, and not require anything in front for hosting, like `IIS` or Apache `HTTPD`. For local development, you should be able to run the app without any third-party component requirements for hosting, and the app should be reachable on `http://localhost:5000/` as an example.\n\n Now hosting the app _directly_ in this way in a production setting is something you _don't want_ to do for obvious reasons, since the infrastructure-layer of the app would grow thick, and the developer must code in all sort of hardening, routing etc, not to mention the security concerns - that self-hosted HTTP server would be exposed all on it's own as an attack-vector. When running in production, a component suitable for the [reverse-proxy](https://en.wikipedia.org/wiki/Reverse_proxy) role should be responsible for binding a public-facing hostname to the app(s), as well as do HTTPS termination - the app itself should focus on what it does best, the business logic (this is why it exists in the first place), while a component like [nginx](https://www.nginx.com/) or [HAProxy](http://www.haproxy.org/) should handle hostname binding, HTTPS and load-balance incoming requests.\n\n Modern platforms like [Kubernetes](https://kubernetes.io/) or [OpenShift](https://www.openshift.com/) offers _routes_ that gives the app an external-reachable hostname, and load-balances the application when running on different nodes, as well as provide HTTPS termination up-front. For small solutions not needing a container orchestrator, plain old nginx in front works great.\n\n All modern application _should_ be hosted with SSL and HTTPS. Thanks to projects like [Let's Encrypt](https://letsencrypt.org/), trusted SSL certificates can be obtained for free, and the world is now, slowly but surely, moving to HTTPS as default. This does not mean that our application's first meeting with HTTPS should be in a staging or production environment, it should also ble possible to develop and test locally with HTTPS as default. Thanks to modern tools, running a local reverse-proxy with a valid HTTPS certificate is quite straight forward.\n\n## Local reverse-proxy with SSL termination\n\n Let's say we have a single application, `MyService`, that is written with ASP NET, running with [Kestrel](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?view=aspnetcore-3.1). The app has no code for HTTPS-redirects, and knows nothing about any SSL certificate or setup, it only talks HTTP on port 80. The application is destined for a life in a container orchestrator of some kind, so it has a `Dockerfile`. To be able to run the `MyService` application via HTTPS in an environment _similar but not equal to_, let's say Kubernets, we need to run it behind a reverse-proxy when testing locally. We also need some sort of SSL certificate. Earlier in the post I mentioned Let's Encrypt that offers free certificates, but to be able to leverage it, a public-facing hostname is needed. For local development, a self-signed certificate is plenty. Now the road down self-signed certificates can be quite dirty and lead to many half-working solutions and \"not-trusted\" warnings in the browser. One easy solution is using a great tool called [mkcert](https://github.com/FiloSottile/mkcert). `mkcert` is a simple CLI that registers a trusted CA on your machine, both in the local certificate store and in all installed browsers, and can generate certificates from this CA.\n\n To install `mkcert` with `brew`:\n\n`$ brew install mkcert`\n\nBefore installing the `mkcert` CA and generating a certificate:\n\n<script src=\"https://gist.github.com/andmos/7fae6b63942f0c27f65cd1fd5dc9e47d.js\"></script>\n\nPlease note, this CA and certificates generated from it is for _local_ purposes only.\n\nNext up, let's configure `nginx` to work as a reverse-proxy with SSL termination:\n\n<script src=\"https://gist.github.com/andmos/76fed99e90c2370eab3abcfd316d604e.js\"></script>\n\nThis configuration will tell `nginx` to listen on `localhost`, port `5000` with the generated certificate from `mkcert`. Requests to `/` is then forwarded to the app, listening on plain old HTTP on port `80`.\n\nThe whole thing is then tied together with `docker-compose`:\n\n<script src=\"https://gist.github.com/andmos/b09aeb7bdef0e0d991140e199f41ea6f.js\"></script>\n\nNow run the whole thing with\n\n```shell\n$ docker-compose up\n```\n\nNavigating to `https://localhost:5000/` reveals a nice HTTPS symbol:\n\n<img src=\"https://i.imgur.com/Yo2Jqgt.png\" style=\"zoom:50%;\" />\n\n## Note\n\nUnless `nginx` is used as reverse-proxy in the live environment, this solution will not be _excactly_ on parity with staging or production, but the mechanisms and practices should be similar. As a rule of thumb, the Twelve-Factor app [talks about the importance of  dev/prod parity](https://12factor.net/dev-prod-parity).","mobiledoc":null,"html":"<h2 id=\"goodpracticesfromthetwelvefactorapp\">Good practices from the Twelve-Factor app</h2>\n\n<p>When developing modern web application or services, the <a href=\"https://12factor.net/port-binding\">Twelve-factor app</a> taught us that our services</p>\n\n<blockquote>\n  <p>is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port.</p>\n</blockquote>\n\n<p>What this means is that our apps written with modern frameworks (like <a href=\"https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-3.1\">ASP.NET Core</a>) should provide their own web-servers, exposing a HTTP port, and not require anything in front for hosting, like <code>IIS</code> or Apache <code>HTTPD</code>. For local development, you should be able to run the app without any third-party component requirements for hosting, and the app should be reachable on <code>http://localhost:5000/</code> as an example.</p>\n\n<p>Now hosting the app <em>directly</em> in this way in a production setting is something you <em>don't want</em> to do for obvious reasons, since the infrastructure-layer of the app would grow thick, and the developer must code in all sort of hardening, routing etc, not to mention the security concerns - that self-hosted HTTP server would be exposed all on it's own as an attack-vector. When running in production, a component suitable for the <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\">reverse-proxy</a> role should be responsible for binding a public-facing hostname to the app(s), as well as do HTTPS termination - the app itself should focus on what it does best, the business logic (this is why it exists in the first place), while a component like <a href=\"https://www.nginx.com/\">nginx</a> or <a href=\"http://www.haproxy.org/\">HAProxy</a> should handle hostname binding, HTTPS and load-balance incoming requests.</p>\n\n<p>Modern platforms like <a href=\"https://kubernetes.io/\">Kubernetes</a> or <a href=\"https://www.openshift.com/\">OpenShift</a> offers <em>routes</em> that gives the app an external-reachable hostname, and load-balances the application when running on different nodes, as well as provide HTTPS termination up-front. For small solutions not needing a container orchestrator, plain old nginx in front works great.</p>\n\n<p>All modern application <em>should</em> be hosted with SSL and HTTPS. Thanks to projects like <a href=\"https://letsencrypt.org/\">Let's Encrypt</a>, trusted SSL certificates can be obtained for free, and the world is now, slowly but surely, moving to HTTPS as default. This does not mean that our application's first meeting with HTTPS should be in a staging or production environment, it should also ble possible to develop and test locally with HTTPS as default. Thanks to modern tools, running a local reverse-proxy with a valid HTTPS certificate is quite straight forward.</p>\n\n<h2 id=\"localreverseproxywithssltermination\">Local reverse-proxy with SSL termination</h2>\n\n<p>Let's say we have a single application, <code>MyService</code>, that is written with ASP NET, running with <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?view=aspnetcore-3.1\">Kestrel</a>. The app has no code for HTTPS-redirects, and knows nothing about any SSL certificate or setup, it only talks HTTP on port 80. The application is destined for a life in a container orchestrator of some kind, so it has a <code>Dockerfile</code>. To be able to run the <code>MyService</code> application via HTTPS in an environment <em>similar but not equal to</em>, let's say Kubernets, we need to run it behind a reverse-proxy when testing locally. We also need some sort of SSL certificate. Earlier in the post I mentioned Let's Encrypt that offers free certificates, but to be able to leverage it, a public-facing hostname is needed. For local development, a self-signed certificate is plenty. Now the road down self-signed certificates can be quite dirty and lead to many half-working solutions and \"not-trusted\" warnings in the browser. One easy solution is using a great tool called <a href=\"https://github.com/FiloSottile/mkcert\">mkcert</a>. <code>mkcert</code> is a simple CLI that registers a trusted CA on your machine, both in the local certificate store and in all installed browsers, and can generate certificates from this CA.</p>\n\n<p>To install <code>mkcert</code> with <code>brew</code>:</p>\n\n<p><code>$ brew install mkcert</code></p>\n\n<p>Before installing the <code>mkcert</code> CA and generating a certificate:</p>\n\n<script src=\"https://gist.github.com/andmos/7fae6b63942f0c27f65cd1fd5dc9e47d.js\"></script>\n\n<p>Please note, this CA and certificates generated from it is for <em>local</em> purposes only.</p>\n\n<p>Next up, let's configure <code>nginx</code> to work as a reverse-proxy with SSL termination:</p>\n\n<script src=\"https://gist.github.com/andmos/76fed99e90c2370eab3abcfd316d604e.js\"></script>\n\n<p>This configuration will tell <code>nginx</code> to listen on <code>localhost</code>, port <code>5000</code> with the generated certificate from <code>mkcert</code>. Requests to <code>/</code> is then forwarded to the app, listening on plain old HTTP on port <code>80</code>.</p>\n\n<p>The whole thing is then tied together with <code>docker-compose</code>:</p>\n\n<script src=\"https://gist.github.com/andmos/b09aeb7bdef0e0d991140e199f41ea6f.js\"></script>\n\n<p>Now run the whole thing with</p>\n\n<pre><code class=\"language-shell\">$ docker-compose up\n</code></pre>\n\n<p>Navigating to <code>https://localhost:5000/</code> reveals a nice HTTPS symbol:</p>\n\n<p><img src=\"https://i.imgur.com/Yo2Jqgt.png\" style=\"zoom:50%;\" /></p>\n\n<h2 id=\"note\">Note</h2>\n\n<p>Unless <code>nginx</code> is used as reverse-proxy in the live environment, this solution will not be <em>excactly</em> on parity with staging or production, but the mechanisms and practices should be similar. As a rule of thumb, the Twelve-Factor app <a href=\"https://12factor.net/dev-prod-parity\">talks about the importance of  dev/prod parity</a>.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2020-04-10 11:42:22","created_by":1,"updated_at":"2020-04-10 15:25:23","updated_by":1,"published_at":"2020-04-10 11:43:29","published_by":1},{"id":36,"uuid":"b8420786-72f6-4963-96e1-375a15e3f8a6","title":"Cross post: Take Argo CD for a spin with K3s and k3d","slug":"cross-post-take-argo-cd-for-a-spin-with-k3s-and-k3d","markdown":"For the second year in a row, my current employer [Bekk](https://www.bekk.no/) has set out on a ambitious December journey:\n\n[Bekk Christmas](https://bekk.christmas/), 264 tech articles in 24 days, each day of the advent calendar.\n\nOne of the categories was _thecloud.christmas_, so it felt natural to me to contribute here.\n\nSo here it it, from the 13th of December, my post about Kubernetes, ArgoCD, K3s and k3d. Enjoy [Take Argo CD for a spin with K3s\nand k3d](https://www.bekk.christmas/post/2020/13/take-argo-cd-for-a-spin-with-k3s-and-k3d)!","mobiledoc":null,"html":"<p>For the second year in a row, my current employer <a href=\"https://www.bekk.no/\">Bekk</a> has set out on a ambitious December journey:</p>\n\n<p><a href=\"https://bekk.christmas/\">Bekk Christmas</a>, 264 tech articles in 24 days, each day of the advent calendar.</p>\n\n<p>One of the categories was <em>thecloud.christmas</em>, so it felt natural to me to contribute here.</p>\n\n<p>So here it it, from the 13th of December, my post about Kubernetes, ArgoCD, K3s and k3d. Enjoy <a href=\"https://www.bekk.christmas/post/2020/13/take-argo-cd-for-a-spin-with-k3s-and-k3d\">Take Argo CD for a spin with K3s <br />\nand k3d</a>!</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2020-12-20 11:32:25","created_by":1,"updated_at":"2021-12-22 12:59:17","updated_by":1,"published_at":"2020-12-20 11:33:43","published_by":1},{"id":37,"uuid":"84db819b-9d5d-4a01-aba2-8e49a3132de7","title":"Containerize FluentMigrator for effortless db migrations","slug":"containerize-fluentmigrator-for-effortless-db-migrations","markdown":"## Continuing the containerization\n\nLast year I wrote about how to set up [a local reverse proxy with nginx and mkcert via Docker-Compose](https://blog.amosti.net/local-reverse-proxy-with-nginx-mkcert-and-docker-compose/).\nBeing able to spin up a local, production-like reverse proxy to use while developing is great, but why stop there?\n\nSooner or later the need for a database to store the applications data will emerge, and with any data structure, the need for change - adding, updating or deleting elements of the structure is needed. In other words, the need for _migrations_.\n\nBack in the day, a usual practice for a team depending on a database (depending on the complexity of the application and maturity of the team), was to share a database instance for development. Setting up a local database can be tricky, and up until 2017 Microsoft's [SQL Server was only available on the Windows platform](https://blogs.microsoft.com/blog/2016/03/07/announcing-sql-server-on-linux/), requiring a VM for local development for *nix users. The single, shared instance strategy is also quite limiting for teams working in parallel on tasks requiring database and / or application code changes. A developer testing a database change on a single branch can easily break the main branch when a single instance is used.\n\nIn 2017 Microsoft release SQL Server 2017 (and now 2019) with Linux support, and with it, thankfully, [Docker support](https://hub.docker.com/_/microsoft-mssql-server).\n\nA clean instance of MS SQL 2019 can be added to a `docker-compose` setup as easily as this:\n\n<script src=\"https://gist.github.com/andmos/ec3838d72ea0e6137e9798f267ee59b4.js\"></script>\n\nWith `docker-compose`, every developer on a team can have their own version of the database.\n\n## Running migrations with FluentMigrator\n\nThe next step is bootstrapping the structure of the database. For .NET, [Entity Framework](https://docs.microsoft.com/en-us/ef/) or [FluentMigrator](https://fluentmigrator.github.io/) are popular choices. Let's focus on FluentMigrator.\n\nA common pattern for handling migrations is creating a dedicated .NET `csproj` file where the migrations live. Let's call it `MyApp.Migrations`.\nAfter [writing some migrations](https://fluentmigrator.github.io/articles/quickstart.html?tabs=runner-in-process), the easiest way of running them is via the FluentMigrator [dotnet tool dotnet-fm](https://fluentmigrator.github.io/articles/runners/dotnet-fm.html). With `dotnet-fm`, running migrations is as easy as\n\n```sh\ndotnet-fm migrate --processor SqlServer2016 --assembly MyApp.Migrations.dll --connection \"Data Source=myConnectionString\"`\n```\n\n## Bootstrapping the database with Docker-Compose\n\nNow we have a `docker-compose` containing an MS SQL instance, and we have a `csproj` file containing some database migrations. To save us from having to run the migrations manually, the process of bootstrapping the development environment can be automated by containerizing the process running the migrations. For this, we create a `Dockerfile` that compiles the migration project, grabs the `dotnet-fm` tool for running FluentMigrator and wraps it up with an `entrypoint` for running.\n\nThe `Dockerfile`:\n\n<script src=\"https://gist.github.com/andmos/b33e2f07b6b1ceb8b9e6e6bfe074f5d6.js\"></script>\n\nSome things to notice here.\n\nThe FluentMigrator library (installed with NuGet) and the `dotnet-fm` tool needs to be the same version, so the `sed` command on line 7 grabs the version-string from the `csproj` file and uses it to install the correct version of `dotnet-fm` tool on line 9.\n\nOn line 12 a little shell-script called `wait-for` is cloned. [https://github.com/eficode/wait-for.git](wait-for) is used to wrap the execution of `dotnet-fm` and _wait_ for the database the become available. This is a neat trick to handle the timing issues that can occur when running via `docker-compose`, where the migrations can be executed before the database is ready.\n\nThe `Dockerfile` is multi-stage, so the compiled library, `dotnet-fm` binary and `wait-for` script is copied to a `dotnet` runtime image. The `netcat` package installed on line 17 is a runtime dependency for `wait-for`.\n\nWith the `Dockerfile` in place, the final `docker-compose.yaml` file:\n\n<script src=\"https://gist.github.com/andmos/cc5d63023d68cdfad5de953fcdc22c78.js\"></script>\n\nThe whole thing spins up with `docker-compose up`. When `db` is ready, the migrations will be run and bootstraps the database.\n","mobiledoc":null,"html":"<h2 id=\"continuingthecontainerization\">Continuing the containerization</h2>\n\n<p>Last year I wrote about how to set up <a href=\"https://blog.amosti.net/local-reverse-proxy-with-nginx-mkcert-and-docker-compose/\">a local reverse proxy with nginx and mkcert via Docker-Compose</a>. <br />\nBeing able to spin up a local, production-like reverse proxy to use while developing is great, but why stop there?</p>\n\n<p>Sooner or later the need for a database to store the applications data will emerge, and with any data structure, the need for change - adding, updating or deleting elements of the structure is needed. In other words, the need for <em>migrations</em>.</p>\n\n<p>Back in the day, a usual practice for a team depending on a database (depending on the complexity of the application and maturity of the team), was to share a database instance for development. Setting up a local database can be tricky, and up until 2017 Microsoft's <a href=\"https://blogs.microsoft.com/blog/2016/03/07/announcing-sql-server-on-linux/\">SQL Server was only available on the Windows platform</a>, requiring a VM for local development for *nix users. The single, shared instance strategy is also quite limiting for teams working in parallel on tasks requiring database and / or application code changes. A developer testing a database change on a single branch can easily break the main branch when a single instance is used.</p>\n\n<p>In 2017 Microsoft release SQL Server 2017 (and now 2019) with Linux support, and with it, thankfully, <a href=\"https://hub.docker.com/_/microsoft-mssql-server\">Docker support</a>.</p>\n\n<p>A clean instance of MS SQL 2019 can be added to a <code>docker-compose</code> setup as easily as this:</p>\n\n<script src=\"https://gist.github.com/andmos/ec3838d72ea0e6137e9798f267ee59b4.js\"></script>\n\n<p>With <code>docker-compose</code>, every developer on a team can have their own version of the database.</p>\n\n<h2 id=\"runningmigrationswithfluentmigrator\">Running migrations with FluentMigrator</h2>\n\n<p>The next step is bootstrapping the structure of the database. For .NET, <a href=\"https://docs.microsoft.com/en-us/ef/\">Entity Framework</a> or <a href=\"https://fluentmigrator.github.io/\">FluentMigrator</a> are popular choices. Let's focus on FluentMigrator.</p>\n\n<p>A common pattern for handling migrations is creating a dedicated .NET <code>csproj</code> file where the migrations live. Let's call it <code>MyApp.Migrations</code>. <br />\nAfter <a href=\"https://fluentmigrator.github.io/articles/quickstart.html?tabs=runner-in-process\">writing some migrations</a>, the easiest way of running them is via the FluentMigrator <a href=\"https://fluentmigrator.github.io/articles/runners/dotnet-fm.html\">dotnet tool dotnet-fm</a>. With <code>dotnet-fm</code>, running migrations is as easy as</p>\n\n<pre><code class=\"language-sh\">dotnet-fm migrate --processor SqlServer2016 --assembly MyApp.Migrations.dll --connection \"Data Source=myConnectionString\"`  \n</code></pre>\n\n<h2 id=\"bootstrappingthedatabasewithdockercompose\">Bootstrapping the database with Docker-Compose</h2>\n\n<p>Now we have a <code>docker-compose</code> containing an MS SQL instance, and we have a <code>csproj</code> file containing some database migrations. To save us from having to run the migrations manually, the process of bootstrapping the development environment can be automated by containerizing the process running the migrations. For this, we create a <code>Dockerfile</code> that compiles the migration project, grabs the <code>dotnet-fm</code> tool for running FluentMigrator and wraps it up with an <code>entrypoint</code> for running.</p>\n\n<p>The <code>Dockerfile</code>:</p>\n\n<script src=\"https://gist.github.com/andmos/b33e2f07b6b1ceb8b9e6e6bfe074f5d6.js\"></script>\n\n<p>Some things to notice here.</p>\n\n<p>The FluentMigrator library (installed with NuGet) and the <code>dotnet-fm</code> tool needs to be the same version, so the <code>sed</code> command on line 7 grabs the version-string from the <code>csproj</code> file and uses it to install the correct version of <code>dotnet-fm</code> tool on line 9.</p>\n\n<p>On line 12 a little shell-script called <code>wait-for</code> is cloned. <a href=\"wait-for\">https://github.com/eficode/wait-for.git</a> is used to wrap the execution of <code>dotnet-fm</code> and <em>wait</em> for the database the become available. This is a neat trick to handle the timing issues that can occur when running via <code>docker-compose</code>, where the migrations can be executed before the database is ready.</p>\n\n<p>The <code>Dockerfile</code> is multi-stage, so the compiled library, <code>dotnet-fm</code> binary and <code>wait-for</code> script is copied to a <code>dotnet</code> runtime image. The <code>netcat</code> package installed on line 17 is a runtime dependency for <code>wait-for</code>.</p>\n\n<p>With the <code>Dockerfile</code> in place, the final <code>docker-compose.yaml</code> file:</p>\n\n<script src=\"https://gist.github.com/andmos/cc5d63023d68cdfad5de953fcdc22c78.js\"></script>\n\n<p>The whole thing spins up with <code>docker-compose up</code>. When <code>db</code> is ready, the migrations will be run and bootstraps the database.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2021-02-21 11:47:50","created_by":1,"updated_at":"2021-02-21 11:50:17","updated_by":1,"published_at":"2021-02-21 11:50:17","published_by":1},{"id":38,"uuid":"4c869f73-1644-4fb4-b3d7-3c618f088230","title":"Deterministic systems with Nix","slug":"deterministic-systems-with-nix","markdown":"> This is a cross-post of [my contribution](https://www.bekk.christmas/post/2021/13/deterministic-systems-with-nix) to this year's [advent calendar](https://www.bekk.christmas/) we do over at Bekk. Hope you like it!\n\n## Introduction\n\nSetting up reliable environments for our software is tricky.\nThe task has kept developers and sysadmins up at night for decades. Making environments and packages truly reproducible and reliable for more than a few weeks before regression sets in, is surely no easy task. In this post, we'll see how we can set up truly deterministic, reproducible and even ephemeral environments with the help of a clever set of tools called Nix, so we can sleep better, knowing our systems can be installed from literary scratch and be guaranteed the same binary packages down to the lowest dependencies.\n\n## What's in a name?\n\nLet's face it, \"Nix\" has a quite ambiguous name that can reference a lot of things, so first, let's get the naming out of the way.\n\nWhen people hear \"Nix\", they might think about \"*nix\", or the commonly spoken variant (without the asterix) \"nix\", the short name the industry has adopted for systems based on good old UNIX. Linux is \"nix\", macOS is \"nix\", BSD is \"nix\" - and in a way, \"Nix\" is also... well, \"nix.\" Confused? Yeah.\n\nNix, in our context, refers to three things: the [Nix Expression Language](https://nixos.wiki/wiki/Nix_Expression_Language), a pure, lazy, functional language. This language makes up the foundational building blocks of the Nix package manager, which can be installed on any \"*nix\" system ([like Linux or macOS](https://nixos.org/manual/nix/stable/quick-start.html)) or as it's own unique Linux distro, [NixOS](https://nixos.org/). So a language, a package manager and even a distro. What's this all about?\n\n\n## What makes Nix so special?\n \nWith the naming out of the way, what makes Nix so special? What does it have to offer that `apt`, `yum`, or `brew` don't have?\n\nFirst off, it's cross platform. The Nix Package Manager [can run on the most common Linux systems, as well as macOS](https://nixos.org/manual/nix/unstable/installation/supported-platforms.html), but that is true for many package managers these days, and is not it's main advantage.\n\nWhat makes Nix special is how it manages packages and dependencies. Nix guarantees reproducible packages, which means that all steps involved in building a package can be run again and again with the same outcome, and if any of the variables in the dependency chain change (all the way down to low-level packages like `libc`), it will result in a new version of this package, that can be installed side-by-side with the old version. This is possible thanks to the nature of the functional Nix language. From the docs:\n\n>Nix is a purely functional package manager. This means that it treats packages like values in purely functional programming languages such as Haskell — they are built by functions that don’t have side-effects, and they never change after they have been built. \n\nNix stores packages in the Nix store, usually the directory `/nix/store`, where each package has its own unique subdirectory such as\n\n```sh\n/nix/store/b6gvzjyb2pg0kjfwrjmg1vfhh54ad73z-firefox-33.1\n```\n where `b6gvzjyb2pg0…` is a unique identifier for the package that captures all its dependencies (it’s a cryptographic hash of the package’s build dependency graph). This enables many powerful features.\n\nIn more practical terms, this is accomplished by generating hash values of all dependencies going _in_ to the package build, as well as the _outcome_ of the build itself.\n\n## Kicking the tires\n\nLet's look at an example package called `hello`. \nThe Nix-script responsible for building the package can be found in the [nixpkgs-github repo](https://github.com/NixOS/nixpkgs)\n(all packages are essentially built and installed from these scripts) and it looks like this:\n\n<script src=\"https://gist.github.com/andmos/9c56554310a6a1dd653d997bcfeae943.js\"></script>\n\nWe begin by installing it to the user's environment:\n\n<script src=\"https://gist.github.com/andmos/c1d48189a5ad662c59bbf25c54f9bb53.js\"></script>\n\nAs we can see, a lot of things was required for a simple program that prints out `Hello, World!`.\n\nOne might look at this list and think \"Hey, i see curl on there - curl is already installed on my machine, why do I need it again, and won't multiple versions of the same package wreck-havoc on my machine?\"\n\nTo address the first comment, this neat little trick is what makes Nix-packages self-contained and immune to what else might be installed on the system.\nOther package managers, like `apt` or `brew` are often heavily dependent on there existing _one_ version of a package or it's transitive dependency.\nThis is why installing packages on different systems can lead to quite different results, and why a package update can break a system.\nWith Nix, all package dependencies comes bundled and are stored in their own hashed directories.\nThe model Nix follows is that every transitive dependency must be defined in a Nix-expression that can be built itself, thus supplying a dependency chain of \"build instructions\" all the way to the lowest parts.\nIf one lower-level dependency change, the main package can not be seen as the same exact version as we had before, and will be installed side-by-side with the old version, completely isolated.\nThis nifty feature is why Nix and NixOS has become a favorite among developers and system administrators alike, it makes for highly robust and deterministic systems, easy to update or rollback.\n\nTo address the concern of multiple versions of `curl`, let's take a look at what we have on our `PATH` after the install of `hello`:\n\n```sh\n$ which curl\n/usr/bin/curl\n```\n\n`curl` being a transitive dependency for `hello` does not place it on our `PATH`, the version of `curl` provided by macOS is still in place.\n\nIf we install `curl` as a top level package, the story would be different:\n\n<script src=\"https://gist.github.com/andmos/19dd36c37fa4b3afa2c942bb5a9e8f5b.js\"></script>\n\nTo keep score of what version of a Nix-package is currently being used, Nix takes leverage of symlinks:\n\n<script src=\"https://gist.github.com/andmos/0e5c437602621d098c0dcb7c62a06602.js\"></script>\n\nIf we regret installing `curl` via Nix or something broke, Nix keeps track of the users environment in\n[Generations](https://nixos.wiki/wiki/NixOS#Generations), making it easy to rollback the system:\n\n<script src=\"https://gist.github.com/andmos/0361e12c6b59dd874450370052556350.js\"></script>\n\n## Creating reproducible development environments with nix-shell\n\nAnother powerful tool provided with Nix, is `nix-shell`. Software teams have always been struggling with the famous \"works on my machine\" syndrome, where a build or piece of code works as expected on one machine, but not on another.\nCreating reproducible development environments has been the holy grail for many, and tools like [Packer](https://www.packer.io/) and [Vagrant](https://www.vagrantup.com/) takes the virtual machine way to solve this, by building VM images that can have tool pre-installed or installed via provisioning systems like [Ansible](https://www.vagrantup.com/docs/provisioning/ansible).\n\nAnother way to solve this is with containers, typically with [Docker](https://www.docker.com/) and [Docker-Compose](https://docs.docker.com/compose/).\nBoth virtual machines and container technology has pros and cons, but the mayor drawback is that it is quite hard to make truly reproducible environments.\nBoth are great for freezing a setup in time (like a VM image or a container image), but are hardly deterministic. \nA badly written `Dockerfile` can produce different results when built on two different systems.\n>As a side note, it is possible to build reproducible and small Docker images [with Nix](https://nix.dev/tutorials/building-and-running-docker-images).\n\n`nix-shell` on the other hand leverages the power of Nix to build local, reproducible, isolated and ephemeral shell-environments.\n\nLet's say we want  `python3` but don't want to install it user/system-wide. It is possible to use `nix-shell` to provide an on-demand shell with just `python3`:\n\n<script src=\"https://gist.github.com/andmos/c89cfa43a073fd6c263307ac0279e7f9.js\"></script>\n\nAs we can see, no `python3` package was installed on the system, but with `nix-shell` we are able to download the package with all dependencies and make it available in a local nix-shell.\nWhen exiting the shell, no version of `python3` is available on path.\n\nWith the Nix language, it is possible to write declarations for these shells that can be shared among the development team.\nLet's say the team is maintaining a Java application, deployed on Kubernetes, and want a setup that just works™ on all systems:\n\n<script src=\"https://gist.github.com/andmos/d6c853be08f78def1e6241bc9470aff5.js\"></script>\n\nThis script can be stored in the root of the Java-project and added to version control. When a developer wants the environment, a simple command will provide it:\n\n<script src=\"https://gist.github.com/andmos/0d76eda18d21d0f502958f464fe861e4.js\"></script>\n\nAs we can se, the selected packages and dependencies are all installed.\n\nTo clean up old `nix-shell` sessions, we can simply run\n```sh\n$ nix-collect-garbage\n```\n\n## Conclusion\n\nThis post has been a brief intro to Nix and what at can provide in terms of reproducible, isolated systems. It is possible to do so much more than just install pre-built packages. I would recommend [How Shopify Uses Nix](https://shopify.engineering/shipit-presents-how-shopify-uses-nix) for further inspiration on how to build and deliver software using Nix, as well as checking out the [home-manager](https://github.com/nix-community/home-manager) project for managing user environments.\n\nInterested in building your first Nix package? See the excellent [nix-tutorials](https://nix-tutorial.gitlabpages.inria.fr/nix-tutorial/first-package.html) website and start hacking!","mobiledoc":null,"html":"<blockquote>\n  <p>This is a cross-post of <a href=\"https://www.bekk.christmas/post/2021/13/deterministic-systems-with-nix\">my contribution</a> to this year's <a href=\"https://www.bekk.christmas/\">advent calendar</a> we do over at Bekk. Hope you like it!</p>\n</blockquote>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Setting up reliable environments for our software is tricky. <br />\nThe task has kept developers and sysadmins up at night for decades. Making environments and packages truly reproducible and reliable for more than a few weeks before regression sets in, is surely no easy task. In this post, we'll see how we can set up truly deterministic, reproducible and even ephemeral environments with the help of a clever set of tools called Nix, so we can sleep better, knowing our systems can be installed from literary scratch and be guaranteed the same binary packages down to the lowest dependencies.</p>\n\n<h2 id=\"whatsinaname\">What's in a name?</h2>\n\n<p>Let's face it, \"Nix\" has a quite ambiguous name that can reference a lot of things, so first, let's get the naming out of the way.</p>\n\n<p>When people hear \"Nix\", they might think about \"*nix\", or the commonly spoken variant (without the asterix) \"nix\", the short name the industry has adopted for systems based on good old UNIX. Linux is \"nix\", macOS is \"nix\", BSD is \"nix\" - and in a way, \"Nix\" is also... well, \"nix.\" Confused? Yeah.</p>\n\n<p>Nix, in our context, refers to three things: the <a href=\"https://nixos.wiki/wiki/Nix_Expression_Language\">Nix Expression Language</a>, a pure, lazy, functional language. This language makes up the foundational building blocks of the Nix package manager, which can be installed on any \"*nix\" system (<a href=\"https://nixos.org/manual/nix/stable/quick-start.html\">like Linux or macOS</a>) or as it's own unique Linux distro, <a href=\"https://nixos.org/\">NixOS</a>. So a language, a package manager and even a distro. What's this all about?</p>\n\n<h2 id=\"whatmakesnixsospecial\">What makes Nix so special?</h2>\n\n<p>With the naming out of the way, what makes Nix so special? What does it have to offer that <code>apt</code>, <code>yum</code>, or <code>brew</code> don't have?</p>\n\n<p>First off, it's cross platform. The Nix Package Manager <a href=\"https://nixos.org/manual/nix/unstable/installation/supported-platforms.html\">can run on the most common Linux systems, as well as macOS</a>, but that is true for many package managers these days, and is not it's main advantage.</p>\n\n<p>What makes Nix special is how it manages packages and dependencies. Nix guarantees reproducible packages, which means that all steps involved in building a package can be run again and again with the same outcome, and if any of the variables in the dependency chain change (all the way down to low-level packages like <code>libc</code>), it will result in a new version of this package, that can be installed side-by-side with the old version. This is possible thanks to the nature of the functional Nix language. From the docs:</p>\n\n<blockquote>\n  <p>Nix is a purely functional package manager. This means that it treats packages like values in purely functional programming languages such as Haskell — they are built by functions that don’t have side-effects, and they never change after they have been built. </p>\n</blockquote>\n\n<p>Nix stores packages in the Nix store, usually the directory <code>/nix/store</code>, where each package has its own unique subdirectory such as</p>\n\n<pre><code class=\"language-sh\">/nix/store/b6gvzjyb2pg0kjfwrjmg1vfhh54ad73z-firefox-33.1\n</code></pre>\n\n<p>where <code>b6gvzjyb2pg0…</code> is a unique identifier for the package that captures all its dependencies (it’s a cryptographic hash of the package’s build dependency graph). This enables many powerful features.</p>\n\n<p>In more practical terms, this is accomplished by generating hash values of all dependencies going <em>in</em> to the package build, as well as the <em>outcome</em> of the build itself.</p>\n\n<h2 id=\"kickingthetires\">Kicking the tires</h2>\n\n<p>Let's look at an example package called <code>hello</code>. <br />\nThe Nix-script responsible for building the package can be found in the <a href=\"https://github.com/NixOS/nixpkgs\">nixpkgs-github repo</a> <br />\n(all packages are essentially built and installed from these scripts) and it looks like this:</p>\n\n<script src=\"https://gist.github.com/andmos/9c56554310a6a1dd653d997bcfeae943.js\"></script>\n\n<p>We begin by installing it to the user's environment:</p>\n\n<script src=\"https://gist.github.com/andmos/c1d48189a5ad662c59bbf25c54f9bb53.js\"></script>\n\n<p>As we can see, a lot of things was required for a simple program that prints out <code>Hello, World!</code>.</p>\n\n<p>One might look at this list and think \"Hey, i see curl on there - curl is already installed on my machine, why do I need it again, and won't multiple versions of the same package wreck-havoc on my machine?\"</p>\n\n<p>To address the first comment, this neat little trick is what makes Nix-packages self-contained and immune to what else might be installed on the system. <br />\nOther package managers, like <code>apt</code> or <code>brew</code> are often heavily dependent on there existing <em>one</em> version of a package or it's transitive dependency. <br />\nThis is why installing packages on different systems can lead to quite different results, and why a package update can break a system. <br />\nWith Nix, all package dependencies comes bundled and are stored in their own hashed directories. <br />\nThe model Nix follows is that every transitive dependency must be defined in a Nix-expression that can be built itself, thus supplying a dependency chain of \"build instructions\" all the way to the lowest parts. <br />\nIf one lower-level dependency change, the main package can not be seen as the same exact version as we had before, and will be installed side-by-side with the old version, completely isolated. <br />\nThis nifty feature is why Nix and NixOS has become a favorite among developers and system administrators alike, it makes for highly robust and deterministic systems, easy to update or rollback.</p>\n\n<p>To address the concern of multiple versions of <code>curl</code>, let's take a look at what we have on our <code>PATH</code> after the install of <code>hello</code>:</p>\n\n<pre><code class=\"language-sh\">$ which curl\n/usr/bin/curl\n</code></pre>\n\n<p><code>curl</code> being a transitive dependency for <code>hello</code> does not place it on our <code>PATH</code>, the version of <code>curl</code> provided by macOS is still in place.</p>\n\n<p>If we install <code>curl</code> as a top level package, the story would be different:</p>\n\n<script src=\"https://gist.github.com/andmos/19dd36c37fa4b3afa2c942bb5a9e8f5b.js\"></script>\n\n<p>To keep score of what version of a Nix-package is currently being used, Nix takes leverage of symlinks:</p>\n\n<script src=\"https://gist.github.com/andmos/0e5c437602621d098c0dcb7c62a06602.js\"></script>\n\n<p>If we regret installing <code>curl</code> via Nix or something broke, Nix keeps track of the users environment in <br />\n<a href=\"https://nixos.wiki/wiki/NixOS#Generations\">Generations</a>, making it easy to rollback the system:</p>\n\n<script src=\"https://gist.github.com/andmos/0361e12c6b59dd874450370052556350.js\"></script>\n\n<h2 id=\"creatingreproducibledevelopmentenvironmentswithnixshell\">Creating reproducible development environments with nix-shell</h2>\n\n<p>Another powerful tool provided with Nix, is <code>nix-shell</code>. Software teams have always been struggling with the famous \"works on my machine\" syndrome, where a build or piece of code works as expected on one machine, but not on another. <br />\nCreating reproducible development environments has been the holy grail for many, and tools like <a href=\"https://www.packer.io/\">Packer</a> and <a href=\"https://www.vagrantup.com/\">Vagrant</a> takes the virtual machine way to solve this, by building VM images that can have tool pre-installed or installed via provisioning systems like <a href=\"https://www.vagrantup.com/docs/provisioning/ansible\">Ansible</a>.</p>\n\n<p>Another way to solve this is with containers, typically with <a href=\"https://www.docker.com/\">Docker</a> and <a href=\"https://docs.docker.com/compose/\">Docker-Compose</a>. <br />\nBoth virtual machines and container technology has pros and cons, but the mayor drawback is that it is quite hard to make truly reproducible environments. <br />\nBoth are great for freezing a setup in time (like a VM image or a container image), but are hardly deterministic. <br />\nA badly written <code>Dockerfile</code> can produce different results when built on two different systems.  </p>\n\n<blockquote>\n  <p>As a side note, it is possible to build reproducible and small Docker images <a href=\"https://nix.dev/tutorials/building-and-running-docker-images\">with Nix</a>.</p>\n</blockquote>\n\n<p><code>nix-shell</code> on the other hand leverages the power of Nix to build local, reproducible, isolated and ephemeral shell-environments.</p>\n\n<p>Let's say we want  <code>python3</code> but don't want to install it user/system-wide. It is possible to use <code>nix-shell</code> to provide an on-demand shell with just <code>python3</code>:</p>\n\n<script src=\"https://gist.github.com/andmos/c89cfa43a073fd6c263307ac0279e7f9.js\"></script>\n\n<p>As we can see, no <code>python3</code> package was installed on the system, but with <code>nix-shell</code> we are able to download the package with all dependencies and make it available in a local nix-shell. <br />\nWhen exiting the shell, no version of <code>python3</code> is available on path.</p>\n\n<p>With the Nix language, it is possible to write declarations for these shells that can be shared among the development team. <br />\nLet's say the team is maintaining a Java application, deployed on Kubernetes, and want a setup that just works™ on all systems:</p>\n\n<script src=\"https://gist.github.com/andmos/d6c853be08f78def1e6241bc9470aff5.js\"></script>\n\n<p>This script can be stored in the root of the Java-project and added to version control. When a developer wants the environment, a simple command will provide it:</p>\n\n<script src=\"https://gist.github.com/andmos/0d76eda18d21d0f502958f464fe861e4.js\"></script>\n\n<p>As we can se, the selected packages and dependencies are all installed.</p>\n\n<p>To clean up old <code>nix-shell</code> sessions, we can simply run  </p>\n\n<pre><code class=\"language-sh\">$ nix-collect-garbage\n</code></pre>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>This post has been a brief intro to Nix and what at can provide in terms of reproducible, isolated systems. It is possible to do so much more than just install pre-built packages. I would recommend <a href=\"https://shopify.engineering/shipit-presents-how-shopify-uses-nix\">How Shopify Uses Nix</a> for further inspiration on how to build and deliver software using Nix, as well as checking out the <a href=\"https://github.com/nix-community/home-manager\">home-manager</a> project for managing user environments.</p>\n\n<p>Interested in building your first Nix package? See the excellent <a href=\"https://nix-tutorial.gitlabpages.inria.fr/nix-tutorial/first-package.html\">nix-tutorials</a> website and start hacking!</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2021-12-16 18:47:50","created_by":1,"updated_at":"2021-12-16 19:06:09","updated_by":1,"published_at":"2021-12-16 19:01:24","published_by":1},{"id":39,"uuid":"c58ea838-0ddb-4628-8334-ac1c62716b62","title":"Backing up API data to GitHub with Flat-Data","slug":"backing-up-api-data-to-github-with-flat-data","markdown":"For the last 10 years or so I have used Trello as my preferred service to keep track of my reading.\n\nI [wrote a blog post](https://blog.amosti.net/how-i-read/) about this setup and my technique with regards to reading, which, upon re-reading, I see that both my technique for reading and writing-skills has improve, so that's that.\n\nIn 2017 [Atlassian acquired Trello](https://techcrunch.com/2017/01/09/atlassian-acquires-trello/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAB4EB-u4iDYnVbIaNgQ5Zc3vh2a4oO12KL7tXvnrIAjHpVo76z6XqXP_F2I7-TDusMGfPvzMr21IRMIyUbenmxznHq_Ap4RcAzuG0yRLxve7Kc8naDdpLEgavPQkU7wuxfOIet-bTDAHjct5eB8DZujMtQiVKaZ2JSq1ji7v7Ypy), and safe to say, thing are starting to become more \"enterprisey\" over there.\n\nI figured I won't be staying with Trello forever, and the amount of sunk-cost I have compounded in my simple reading list is astonishing. After some grooming, the backlog has currently 242 items and my done list has 450 entries. \n\nWhat can I say, I like to read.\n\nSome years ago I began my humble planning for moving away from Trello by writing a [wrapper API service](https://github.com/andmos/ReadingList) for the reading list,\nusing the Trello API itself as repository layer, so the switch to some other service could be easier. Maybe this API will act on top of something like SQLite or Firebase, who knows. To make sure I always had a copy of the data from Trello, I wrote a small bash-script with some `curl` calls and set up a cronjob. Now cronjobs are messy, I need to remember that I have it running, correct it if it runs into some error, and - after all - back up the data that comes from it.\n\nSearching for a better solution than bash and cronjobs, the next step on this pet project of mine came February 2021, when GitHub releases a cool project called [Flat-Data](https://next.github.com/projects/flat-data).\n\nIn the projects own words:\n\n> Flat Data is a GitHub action which makes it easy to fetch data and commit it to your repository as flatfiles. The action is intended to be run on a schedule, retrieving data from any supported target and creating a commit if there is any change to the fetched data.\n\nSo in a nutshell, Flat-Data allows us to set up a simple GitHub Action, point it at some data source (like an API or SQL database), query the data, run some processing on it, and have it checked in to Git. A great tool for the data scientists I would\nimagine, or for a guy who just want to back up his reading list from an over-engineered API layer on top of Trello.\n\nIt's easy to get started. All we need is a repository and a GitHub Action. \nMy repo is [ReadingList-Data](https://github.com/andmos/ReadingList-Data) and contains a few files: `backlog.json`, `done.json`, `reading.json` and `postprocess.js`.\n\nThe GitHub Action itself using Flat-Data looks like this:\n\n<script src=\"https://gist.github.com/andmos/00fe8179aac87a805e2d4d12a749058e.js\"></script>\n\nEvery Monday afternoon this action runs, fetches the current status from the reading list API, runs it through the post-processor\nscript (a simple Deno script that only formats the JSON) and creates commits with the changes. It could not be simpler.\n\nAs a bonus the commit history works as a great timeline to see when a book was added to the backlog or I was done reading it. \n\n![Commit-history](https://user-images.githubusercontent.com/1283556/149825223-9894be37-ff14-4788-92dd-e3eb654e06cd.png)\n\nThis is a quite simple and banal use case for Flat-Data, but hopefully it will inspire someone out there.","mobiledoc":null,"html":"<p>For the last 10 years or so I have used Trello as my preferred service to keep track of my reading.</p>\n\n<p>I <a href=\"https://blog.amosti.net/how-i-read/\">wrote a blog post</a> about this setup and my technique with regards to reading, which, upon re-reading, I see that both my technique for reading and writing-skills has improve, so that's that.</p>\n\n<p>In 2017 <a href=\"https://techcrunch.com/2017/01/09/atlassian-acquires-trello/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAB4EB-u4iDYnVbIaNgQ5Zc3vh2a4oO12KL7tXvnrIAjHpVo76z6XqXP_F2I7-TDusMGfPvzMr21IRMIyUbenmxznHq_Ap4RcAzuG0yRLxve7Kc8naDdpLEgavPQkU7wuxfOIet-bTDAHjct5eB8DZujMtQiVKaZ2JSq1ji7v7Ypy\">Atlassian acquired Trello</a>, and safe to say, thing are starting to become more \"enterprisey\" over there.</p>\n\n<p>I figured I won't be staying with Trello forever, and the amount of sunk-cost I have compounded in my simple reading list is astonishing. After some grooming, the backlog has currently 242 items and my done list has 450 entries. </p>\n\n<p>What can I say, I like to read.</p>\n\n<p>Some years ago I began my humble planning for moving away from Trello by writing a <a href=\"https://github.com/andmos/ReadingList\">wrapper API service</a> for the reading list, <br />\nusing the Trello API itself as repository layer, so the switch to some other service could be easier. Maybe this API will act on top of something like SQLite or Firebase, who knows. To make sure I always had a copy of the data from Trello, I wrote a small bash-script with some <code>curl</code> calls and set up a cronjob. Now cronjobs are messy, I need to remember that I have it running, correct it if it runs into some error, and - after all - back up the data that comes from it.</p>\n\n<p>Searching for a better solution than bash and cronjobs, the next step on this pet project of mine came February 2021, when GitHub releases a cool project called <a href=\"https://next.github.com/projects/flat-data\">Flat-Data</a>.</p>\n\n<p>In the projects own words:</p>\n\n<blockquote>\n  <p>Flat Data is a GitHub action which makes it easy to fetch data and commit it to your repository as flatfiles. The action is intended to be run on a schedule, retrieving data from any supported target and creating a commit if there is any change to the fetched data.</p>\n</blockquote>\n\n<p>So in a nutshell, Flat-Data allows us to set up a simple GitHub Action, point it at some data source (like an API or SQL database), query the data, run some processing on it, and have it checked in to Git. A great tool for the data scientists I would <br />\nimagine, or for a guy who just want to back up his reading list from an over-engineered API layer on top of Trello.</p>\n\n<p>It's easy to get started. All we need is a repository and a GitHub Action. <br />\nMy repo is <a href=\"https://github.com/andmos/ReadingList-Data\">ReadingList-Data</a> and contains a few files: <code>backlog.json</code>, <code>done.json</code>, <code>reading.json</code> and <code>postprocess.js</code>.</p>\n\n<p>The GitHub Action itself using Flat-Data looks like this:</p>\n\n<script src=\"https://gist.github.com/andmos/00fe8179aac87a805e2d4d12a749058e.js\"></script>\n\n<p>Every Monday afternoon this action runs, fetches the current status from the reading list API, runs it through the post-processor <br />\nscript (a simple Deno script that only formats the JSON) and creates commits with the changes. It could not be simpler.</p>\n\n<p>As a bonus the commit history works as a great timeline to see when a book was added to the backlog or I was done reading it. </p>\n\n<p><img src=\"https://user-images.githubusercontent.com/1283556/149825223-9894be37-ff14-4788-92dd-e3eb654e06cd.png\" alt=\"Commit-history\" /></p>\n\n<p>This is a quite simple and banal use case for Flat-Data, but hopefully it will inspire someone out there.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2022-01-17 19:11:33","created_by":1,"updated_at":"2022-01-17 19:22:00","updated_by":1,"published_at":"2022-01-17 19:16:18","published_by":1}],"users":[{"id":1,"uuid":"bda5739e-5fec-4fef-a602-f0587a031875","name":"Andreas Mosti","slug":"andreas","password":"$2a$10$SUWV5avN.sg3sRsUziM92OohCr67OiAQgRGfLydkCS3iFEm9aKYLy","email":"andreas.mosti@gmail.com","image":"//www.gravatar.com/avatar/99f481c5ef312c5f3d299b5957a22a07?s=250&d=mm&r=x","cover":null,"bio":"Software developer for Bekk Consulting, previously Blueye Robotics and DIPS AS. I like shiny things that automate infrastructure. Passionate about release engineering.","website":null,"location":"Trondheim, Norway","facebook":null,"twitter":"@amostii","accessibility":null,"status":"active","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_login":"2022-01-31 16:36:06","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2022-01-31 16:36:06","updated_by":1}],"roles":[{"id":1,"uuid":"8209e092-4bb9-4dc4-a0f6-bd7fb23cc0f3","name":"Administrator","description":"Administrators","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":2,"uuid":"ae01657b-5e2e-4292-883d-7ab8a7c7986a","name":"Editor","description":"Editors","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":3,"uuid":"2ac0b974-98c8-437d-b12f-28fbd0946595","name":"Author","description":"Authors","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":4,"uuid":"9a227e34-91d9-4e71-a769-6ae3ff26efc5","name":"Owner","description":"Blog Owner","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1}],"roles_users":[{"id":1,"role_id":4,"user_id":1}],"permissions":[{"id":1,"uuid":"3609fb00-89d9-4bfb-9bca-f298eaebab03","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":2,"uuid":"f6f904b5-05e8-43db-a6dc-056e9c6c952b","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":3,"uuid":"daa7afc6-81b2-4632-bded-6e3e569a7acb","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":4,"uuid":"891c6845-eeba-49a0-abcf-ebe3ac412501","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":5,"uuid":"642a10b4-3e8f-413e-b6ff-e538a185b003","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":6,"uuid":"a77933d4-9ceb-4a05-92cd-84c355d8edec","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":7,"uuid":"1a200086-6b5e-44ad-b348-1e1eda6046dc","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":8,"uuid":"1a95efcd-ee50-4f06-8bf9-698dfab551e6","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":9,"uuid":"938c84ee-f087-4cd4-aefb-26ee5ab6126d","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":10,"uuid":"e413bfc4-50c5-4384-976e-157cf7678ab7","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":11,"uuid":"1a550544-2ffc-4db7-9437-9e7879485f8d","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":12,"uuid":"03de42b6-8223-4f10-a859-94b1553bad09","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":13,"uuid":"054db0e1-9fa8-4e05-8ea4-5f4f44ccb45c","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":14,"uuid":"3289c179-d8b2-46b5-92db-c94be9bf05bf","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":15,"uuid":"cd11086a-fb3e-4d13-b07c-36f2e7fa0fb5","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":16,"uuid":"2f1f7f32-e074-45ff-8ac4-1bfb0f07f068","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":17,"uuid":"203fe97b-dd5a-4d9a-9e48-9a30535d5536","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":18,"uuid":"5f7f2029-4ad0-4322-a3d7-f9237f06f39f","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":19,"uuid":"ff658ee3-a6e9-47ef-90f7-dc2297a768ff","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":20,"uuid":"438472a7-3996-4dbe-98f8-d92b3513d40f","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":21,"uuid":"adaa0897-b53b-4999-b48c-80850ddbb989","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":22,"uuid":"d6c012ec-f36a-4f74-8375-7d2d0922c6f1","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":23,"uuid":"d2397cb9-320a-4b5f-899c-951d1e00884c","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":24,"uuid":"a761dd30-9052-4b19-819f-8115f71d179d","name":"Upload themes","object_type":"theme","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":25,"uuid":"ab71aa24-f21c-4637-9f5a-9f2d6b24f4f1","name":"Download themes","object_type":"theme","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":26,"uuid":"1f3c35cd-2adf-489b-82eb-6deb8c92419b","name":"Delete themes","object_type":"theme","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":27,"uuid":"14b840dd-3f3d-4b14-97be-be9a7642ca3a","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":28,"uuid":"19eca015-7a0b-4d96-a81d-c539de45d7e5","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":29,"uuid":"1111bb3a-17d1-475d-96ad-1b00899680d9","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":30,"uuid":"625d4b94-6371-444e-b324-2bed04a97439","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":31,"uuid":"ecd059da-919a-424b-b1d5-eb0f7e761e1a","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":32,"uuid":"dfc4a66d-de52-4a22-a293-dd672a53f2d6","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":33,"uuid":"559dd253-6f96-4eab-8aa7-7942ce597a90","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":34,"uuid":"0dbcba25-9ba6-4dfc-837c-c9108c3ed6c6","name":"Browse clients","object_type":"client","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":35,"uuid":"d44a9143-d6b0-461e-ba5f-86edb55f1bd0","name":"Read clients","object_type":"client","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":36,"uuid":"706f4340-a4c3-4662-b643-67d39f2849aa","name":"Edit clients","object_type":"client","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":37,"uuid":"48935317-38af-4319-a948-2b8f4d258703","name":"Add clients","object_type":"client","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":38,"uuid":"9dc66c29-bf78-43a0-8d31-2c06010cab24","name":"Delete clients","object_type":"client","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":39,"uuid":"0427fa13-a921-49ef-8c5f-e53ee23b7f1e","name":"Browse subscribers","object_type":"subscriber","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":40,"uuid":"5abe8380-1f52-4117-9d6d-0b810507f2c1","name":"Read subscribers","object_type":"subscriber","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":41,"uuid":"1acb21a0-78ef-4941-821e-55d4ea705599","name":"Edit subscribers","object_type":"subscriber","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":42,"uuid":"e2ca950f-c630-41d3-a063-535011874fcc","name":"Add subscribers","object_type":"subscriber","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":43,"uuid":"0be529bb-9c4d-45d4-8c71-98e9e9a173b0","name":"Delete subscribers","object_type":"subscriber","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1}],"permissions_users":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3},{"id":4,"role_id":1,"permission_id":4},{"id":5,"role_id":1,"permission_id":5},{"id":6,"role_id":1,"permission_id":6},{"id":7,"role_id":1,"permission_id":7},{"id":8,"role_id":1,"permission_id":8},{"id":9,"role_id":1,"permission_id":9},{"id":10,"role_id":1,"permission_id":10},{"id":11,"role_id":1,"permission_id":11},{"id":12,"role_id":1,"permission_id":12},{"id":13,"role_id":1,"permission_id":13},{"id":14,"role_id":1,"permission_id":14},{"id":15,"role_id":1,"permission_id":15},{"id":16,"role_id":1,"permission_id":16},{"id":17,"role_id":1,"permission_id":17},{"id":18,"role_id":1,"permission_id":18},{"id":19,"role_id":1,"permission_id":19},{"id":20,"role_id":1,"permission_id":20},{"id":21,"role_id":1,"permission_id":21},{"id":22,"role_id":1,"permission_id":22},{"id":23,"role_id":1,"permission_id":23},{"id":24,"role_id":1,"permission_id":24},{"id":25,"role_id":1,"permission_id":25},{"id":26,"role_id":1,"permission_id":26},{"id":27,"role_id":1,"permission_id":27},{"id":28,"role_id":1,"permission_id":28},{"id":29,"role_id":1,"permission_id":29},{"id":30,"role_id":1,"permission_id":30},{"id":31,"role_id":1,"permission_id":31},{"id":32,"role_id":1,"permission_id":32},{"id":33,"role_id":1,"permission_id":33},{"id":34,"role_id":1,"permission_id":34},{"id":35,"role_id":1,"permission_id":35},{"id":36,"role_id":1,"permission_id":36},{"id":37,"role_id":1,"permission_id":37},{"id":38,"role_id":1,"permission_id":38},{"id":39,"role_id":1,"permission_id":39},{"id":40,"role_id":1,"permission_id":40},{"id":41,"role_id":1,"permission_id":41},{"id":42,"role_id":1,"permission_id":42},{"id":43,"role_id":1,"permission_id":43},{"id":44,"role_id":2,"permission_id":8},{"id":45,"role_id":2,"permission_id":9},{"id":46,"role_id":2,"permission_id":10},{"id":47,"role_id":2,"permission_id":11},{"id":48,"role_id":2,"permission_id":12},{"id":49,"role_id":2,"permission_id":13},{"id":50,"role_id":2,"permission_id":14},{"id":51,"role_id":2,"permission_id":16},{"id":52,"role_id":2,"permission_id":17},{"id":53,"role_id":2,"permission_id":18},{"id":54,"role_id":2,"permission_id":19},{"id":55,"role_id":2,"permission_id":20},{"id":56,"role_id":2,"permission_id":21},{"id":57,"role_id":2,"permission_id":27},{"id":58,"role_id":2,"permission_id":28},{"id":59,"role_id":2,"permission_id":29},{"id":60,"role_id":2,"permission_id":30},{"id":61,"role_id":2,"permission_id":31},{"id":62,"role_id":2,"permission_id":32},{"id":63,"role_id":2,"permission_id":33},{"id":64,"role_id":2,"permission_id":34},{"id":65,"role_id":2,"permission_id":35},{"id":66,"role_id":2,"permission_id":36},{"id":67,"role_id":2,"permission_id":37},{"id":68,"role_id":2,"permission_id":38},{"id":69,"role_id":2,"permission_id":42},{"id":70,"role_id":3,"permission_id":8},{"id":71,"role_id":3,"permission_id":9},{"id":72,"role_id":3,"permission_id":11},{"id":73,"role_id":3,"permission_id":13},{"id":74,"role_id":3,"permission_id":14},{"id":75,"role_id":3,"permission_id":16},{"id":76,"role_id":3,"permission_id":17},{"id":77,"role_id":3,"permission_id":18},{"id":78,"role_id":3,"permission_id":20},{"id":79,"role_id":3,"permission_id":27},{"id":80,"role_id":3,"permission_id":28},{"id":81,"role_id":3,"permission_id":33},{"id":82,"role_id":3,"permission_id":34},{"id":83,"role_id":3,"permission_id":35},{"id":84,"role_id":3,"permission_id":36},{"id":85,"role_id":3,"permission_id":37},{"id":86,"role_id":3,"permission_id":38},{"id":87,"role_id":3,"permission_id":42}],"permissions_apps":[],"settings":[{"id":1,"uuid":"fbcea552-3f0e-4ce5-bf8e-89c765cdc263","key":"databaseVersion","value":"009","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":2,"uuid":"e134ed6e-309a-49d6-8900-92c1fe0cc0b7","key":"dbHash","value":"0915185b-175d-4182-8481-01f164edbb9f","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:44","updated_by":1},{"id":3,"uuid":"a25a9ea3-f26a-4e04-8b27-1276a9eac143","key":"nextUpdateCheck","value":"1643733363","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2022-01-31 16:36:02","updated_by":1},{"id":4,"uuid":"3ac97335-8843-45f6-b82c-60446904af39","key":"displayUpdateNotification","value":"0.11.14","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2022-01-31 16:36:02","updated_by":1},{"id":5,"uuid":"4fb0885d-1fcd-4791-a0ba-9cb9abaa75e8","key":"seenNotifications","value":"[]","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":6,"uuid":"74a7806e-82aa-48e5-b40c-9008f3f87568","key":"migrations","value":"{}","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":7,"uuid":"12d7fbc2-37c9-4994-ac95-24b49e1c40db","key":"title","value":"Dev&Ops","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":8,"uuid":"486db554-2a42-4cfb-a4f1-8a6aaabd399b","key":"description","value":"Personal blog of Andreas Mosti. Thoughts, ideas and crazy hacks from my life as a developer and operations guy.","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":9,"uuid":"e1710282-950c-4aad-a844-6735a802210f","key":"logo","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":10,"uuid":"9e9204d2-dce9-4a71-b882-1dc3760b36d7","key":"cover","value":"/content/images/2017/06/Fil-17-10-2015--10-53-59.jpeg","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":11,"uuid":"f4e1c721-81e2-43d1-b746-2c9ff58fded1","key":"defaultLang","value":"en_US","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":12,"uuid":"fc613b33-0161-4ba9-8649-e8aa44afb08d","key":"postsPerPage","value":"10","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":13,"uuid":"f4bd3202-921e-4709-b34d-ad13390b8828","key":"activeTimezone","value":"Etc/UTC","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":14,"uuid":"a63cdd2c-c136-4d3b-88d8-e7c9bc63d6c3","key":"forceI18n","value":"true","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":15,"uuid":"a7bfc2d3-810e-4523-8696-dc6066682117","key":"permalinks","value":"/:slug/","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":16,"uuid":"a04de4ae-2a4b-4c99-9ee0-f01757e8c4df","key":"amp","value":"true","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":17,"uuid":"f23c183f-8738-43b4-a045-88b18f3e3b0c","key":"ghost_head","value":"<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-79783580-1', 'auto');\n  ga('send', 'pageview');\n\n</script>","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":18,"uuid":"8ebee5bf-a952-48d8-8daf-ea75228955d6","key":"ghost_foot","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":19,"uuid":"c4cb0321-9fc0-4ab7-9bf5-f257c54f4cc6","key":"facebook","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":20,"uuid":"5a54bc28-cf66-4876-88ae-259d12f16b3b","key":"twitter","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":21,"uuid":"e6a34f47-8a33-433a-90f6-457552ee7710","key":"labs","value":"{}","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":22,"uuid":"7442e48b-2226-4bc4-9ad5-3b420d380932","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"}]","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":23,"uuid":"ae9a1f17-f1be-47ff-85d7-0d4cdfe91efe","key":"slack","value":"[{\"url\":\"\"}]","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":24,"uuid":"548cb79b-c158-47e0-8868-adba2b4288f3","key":"activeApps","value":"[]","type":"app","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":25,"uuid":"82bc810f-9ac9-4d43-8e89-c7c65aba0f72","key":"installedApps","value":"[]","type":"app","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2022-01-18 07:37:51","updated_by":1},{"id":26,"uuid":"b007dbba-5f7e-424e-87e6-5fd4daa6d4ae","key":"isPrivate","value":"false","type":"private","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":27,"uuid":"2bc9432a-b8a7-4bd1-bb4a-7842a6c46ef8","key":"password","value":"","type":"private","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":28,"uuid":"b325fff2-0925-45a3-aea0-d7f6c60098ca","key":"activeTheme","value":"casper","type":"theme","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1}],"tags":[{"id":1,"uuid":"a25b49cf-e284-4103-a69f-ad6f6ca9aef7","name":"Getting Started","slug":"getting-started","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1}],"posts_tags":[],"apps":[],"app_settings":[],"app_fields":[],"subscribers":[]}}]}