{"db":[{"meta":{"exported_on":1497553493944,"version":"009"},"data":{"posts":[{"id":2,"uuid":"d65b4afa-9611-4e7b-8852-e62a02d05969","title":"Self-hosted HTTP service in C# with Nancy and TopShelf","slug":"self-hosted-http-service-in-c-with-nancy-and-topshelf","markdown":"\nI found myself in need of a standalone, self-hosted HTTP Service for a REST-backend at work the other day. I like my services to be flexible and easy to deploy with a low footprint. Here's the catch: At work we write in .Net and I truly hate IIS. I kinda like C#, but I don't want my webservices to be tightly locked onto the platform-specific overhead hell that is IIS. Thanks to [OWIN](http://owin.org/), [Nancy](http://nancyfx.org/) and [TopShelf](http://topshelf-project.com/) it easy to write a self-hosted HTTP service (Ruby or Node.js style!) in C# and have it run as a standalone application or as a Windows Service. Here is a super duper easy example using Nancys Self-Host and TopShelf.\n\n\n###The Code\nFirst of all we need som packages from NuGet:\n\n\t\tInstall-Package Nancy.Hosting.Self \n\t\tInstall-Package Topshelf \n\t\tInstall-Package Topshelf.Linux\n\t\t\n\nFirst the NancySelfHost class:\n\n\t\tusing System;\n\t\tusing System.Diagnostics;\n\t\tusing Nancy.Hosting.Self;\n\n\t\tnamespace AwesomeNancySelfHost\n\t\t{\n\t\t\tpublic class NancySelfHost\n\t\t\t{\n\t\t\t\tprivate NancyHost m_nancyHost;\n\n\t\t\t\tpublic void Start()\n\t\t\t\t{\n\t\t\t\t\tm_nancyHost = new NancyHost(new Uri(\"http://localhost:5000\"));\n\t\t\t\t\tm_nancyHost.Start();\n\t\t\t\n\t\t\t\t}\n\n\t\t\t\tpublic void Stop()\n\t\t\t\t{\n\t\t\t\t\tm_nancyHost.Stop();\n\t\t\t\t\tConsole.WriteLine(\"Stopped. Good bye!\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\t\t\n\t\t\nNow we need a Nancy module to describe the route for the webservice. Lets make a simple API-example and return some JSON:\n\n\t\tusing System;\n\t\tusing Nancy;\n\n\t\tnamespace AwesomeNancySelfHost\n\t\t{\n\t\t\tpublic class ExampleNancyModule : NancyModule\n\t\t\t{\n\n\t\t\t\tpublic NancyModule() \n\t\t\t\t{\n\n\t\t\t\t\tGet[\"/v1/feeds\"] = parameters =>\n\t\t\t\t\t{\n\t\t\t\t\t\tvar feeds = new string[] {\"foo\", \"bar\"};\n\t\t\t\t\t\treturn Response.AsJson(feeds);\n\t\t\t\t\t};\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\nFinaly we strap the whole thing together and make a service with TopShelf: \n\n\t\tusing System;\n\t\tusing Topshelf;\n\n\t\tnamespace AwesomeNancySelfHost\n\t\t{\n\t\t\tpublic class Program\n\t\t\t{\n\t\t\t\tpublic static void Main()\n\t\t\t\t{\n\t\t\t\t\tHostFactory.Run(x => \n\t\t\t\t\t{\n\t\t\t\t\t\tx.UseLinuxIfAvailable();\n\t\t\t\t\t\tx.Service<NancySelfHost>(s => \n\t\t\t\t\t\t{\n\t\t\t\t\t\t\ts.ConstructUsing(name => new NancySelfHost()); \n\t\t\t\t\t\t\ts.WhenStarted(tc => tc.Start()); \n\t\t\t\t\t\t\ts.WhenStopped(tc => tc.Stop()); \n\t\t\t\t\t\t});\n\n\t\t\t\t\t\tx.RunAsLocalSystem(); \n\t\t\t\t\t\tx.SetDescription(\"Nancy-SelfHost example\"); \n\t\t\t\t\t\tx.SetDisplayName(\"Nancy-SelfHost Service\"); \n\t\t\t\t\t\tx.SetServiceName(\"Nancy-SelfHost\"); \n\t\t\t\t\t}); \n\t\t\t\t}\n\t\t\t}\n\t\t}\n\nThats all the code we need! If we know run the application, a console window will show the following:\n\n\t\tConfiguration Result:\n\t\t[Success] Name Nancy-SelfHost\n\t\t[Success] DisplayName Nancy-SelfHost Service\n\t\t[Success] Description Nancy-SelfHost example\n\t\t[Success] ServiceName Nancy-SelfHost\n\t\tTopshelf v3.1.135.0, .NET Framework v4.0.30319.17020\n\t    The Nancy-SelfHost service is now running, press Control+C to exit.\n\t\t\nNavigate to ``http://localhost:5000/v1/feeds`` with you're favorite browser and get yourself some JSON! \n\n### Install the Windows Service \n\nRunning the console application is no use for us if we want this example API to run on a Windows Server as a Windows Service. To make it so, hit up a CMD (or Powershell) window as administrator. Navigate to the projects bin/debug folder and type \n\t\t\n\t\tAwesomeNancySelfHost.exe install\n\t\tAwesomeNancySelfHost.exe start\nIf you know check the ``services``snap-in (``run => mmc``) you will see the AwesomeNancySelfHost in the list of Windows Services. Again, check ``http://localhost:5000/v1/feeds`` and check out the foobar JSON!\n\n### Bonus Round: Linux hosting\nDid you notice the ``Install-Package Topshelf.Linux`` NuGet-package and the ``x.UseLinuxIfAvailable();``statement in TopShelfs Main method? It is true, both Nancy and TopShelf runs natively in Mono, so our tiny webservice will run under Linux too, making it not only standalone but cross-platform too. If we pack the whole thing together with my [Docker-image for Mono](https://github.com/andmos/Docker-Mono) deployment get realy easy and the service can scale horizontally with ease.\n\n### Wrapping it up\n\nThanks to [OWIN](http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api) and [ASP.NET V-Next](http://www.asp.net/vnext) Microsoft has got its game up on the web front. We now see a decoupling between service and server, which I think is a good thing. Specific platforms and servers should not matter when writing web-services, and it seems like Microsoft finally has realized that fact. With projects like Nancy and TopShelf I no longer fear writing web APIs in .Net, making C# a language that can be used on the entire application stack, as well as cross-platform.\n","mobiledoc":null,"html":"<p>I found myself in need of a standalone, self-hosted HTTP Service for a REST-backend at work the other day. I like my services to be flexible and easy to deploy with a low footprint. Here's the catch: At work we write in .Net and I truly hate IIS. I kinda like C#, but I don't want my webservices to be tightly locked onto the platform-specific overhead hell that is IIS. Thanks to <a href=\"http://owin.org/\">OWIN</a>, <a href=\"http://nancyfx.org/\">Nancy</a> and <a href=\"http://topshelf-project.com/\">TopShelf</a> it easy to write a self-hosted HTTP service (Ruby or Node.js style!) in C# and have it run as a standalone application or as a Windows Service. Here is a super duper easy example using Nancys Self-Host and TopShelf.</p>\n\n<h3 id=\"thecode\">The Code</h3>\n\n<p>First of all we need som packages from NuGet:</p>\n\n<pre><code>    Install-Package Nancy.Hosting.Self \n    Install-Package Topshelf \n    Install-Package Topshelf.Linux\n</code></pre>\n\n<p>First the NancySelfHost class:</p>\n\n<pre><code>    using System;\n    using System.Diagnostics;\n    using Nancy.Hosting.Self;\n\n    namespace AwesomeNancySelfHost\n    {\n        public class NancySelfHost\n        {\n            private NancyHost m_nancyHost;\n\n            public void Start()\n            {\n                m_nancyHost = new NancyHost(new Uri(\"http://localhost:5000\"));\n                m_nancyHost.Start();\n\n            }\n\n            public void Stop()\n            {\n                m_nancyHost.Stop();\n                Console.WriteLine(\"Stopped. Good bye!\");\n            }\n        }\n    }       \n</code></pre>\n\n<p>Now we need a Nancy module to describe the route for the webservice. Lets make a simple API-example and return some JSON:</p>\n\n<pre><code>    using System;\n    using Nancy;\n\n    namespace AwesomeNancySelfHost\n    {\n        public class ExampleNancyModule : NancyModule\n        {\n\n            public NancyModule() \n            {\n\n                Get[\"/v1/feeds\"] = parameters =&gt;\n                {\n                    var feeds = new string[] {\"foo\", \"bar\"};\n                    return Response.AsJson(feeds);\n                };\n            }\n        }\n    }\n</code></pre>\n\n<p>Finaly we strap the whole thing together and make a service with TopShelf: </p>\n\n<pre><code>    using System;\n    using Topshelf;\n\n    namespace AwesomeNancySelfHost\n    {\n        public class Program\n        {\n            public static void Main()\n            {\n                HostFactory.Run(x =&gt; \n                {\n                    x.UseLinuxIfAvailable();\n                    x.Service&lt;NancySelfHost&gt;(s =&gt; \n                    {\n                        s.ConstructUsing(name =&gt; new NancySelfHost()); \n                        s.WhenStarted(tc =&gt; tc.Start()); \n                        s.WhenStopped(tc =&gt; tc.Stop()); \n                    });\n\n                    x.RunAsLocalSystem(); \n                    x.SetDescription(\"Nancy-SelfHost example\"); \n                    x.SetDisplayName(\"Nancy-SelfHost Service\"); \n                    x.SetServiceName(\"Nancy-SelfHost\"); \n                }); \n            }\n        }\n    }\n</code></pre>\n\n<p>Thats all the code we need! If we know run the application, a console window will show the following:</p>\n\n<pre><code>    Configuration Result:\n    [Success] Name Nancy-SelfHost\n    [Success] DisplayName Nancy-SelfHost Service\n    [Success] Description Nancy-SelfHost example\n    [Success] ServiceName Nancy-SelfHost\n    Topshelf v3.1.135.0, .NET Framework v4.0.30319.17020\n    The Nancy-SelfHost service is now running, press Control+C to exit.\n</code></pre>\n\n<p>Navigate to <code>http://localhost:5000/v1/feeds</code> with you're favorite browser and get yourself some JSON! </p>\n\n<h3 id=\"installthewindowsservice\">Install the Windows Service</h3>\n\n<p>Running the console application is no use for us if we want this example API to run on a Windows Server as a Windows Service. To make it so, hit up a CMD (or Powershell) window as administrator. Navigate to the projects bin/debug folder and type </p>\n\n<pre><code>    AwesomeNancySelfHost.exe install\n    AwesomeNancySelfHost.exe start\n</code></pre>\n\n<p>If you know check the <code>services</code>snap-in (<code>run =&gt; mmc</code>) you will see the AwesomeNancySelfHost in the list of Windows Services. Again, check <code>http://localhost:5000/v1/feeds</code> and check out the foobar JSON!</p>\n\n<h3 id=\"bonusroundlinuxhosting\">Bonus Round: Linux hosting</h3>\n\n<p>Did you notice the <code>Install-Package Topshelf.Linux</code> NuGet-package and the <code>x.UseLinuxIfAvailable();</code>statement in TopShelfs Main method? It is true, both Nancy and TopShelf runs natively in Mono, so our tiny webservice will run under Linux too, making it not only standalone but cross-platform too. If we pack the whole thing together with my <a href=\"https://github.com/andmos/Docker-Mono\">Docker-image for Mono</a> deployment get realy easy and the service can scale horizontally with ease.</p>\n\n<h3 id=\"wrappingitup\">Wrapping it up</h3>\n\n<p>Thanks to <a href=\"http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api\">OWIN</a> and <a href=\"http://www.asp.net/vnext\">ASP.NET V-Next</a> Microsoft has got its game up on the web front. We now see a decoupling between service and server, which I think is a good thing. Specific platforms and servers should not matter when writing web-services, and it seems like Microsoft finally has realized that fact. With projects like Nancy and TopShelf I no longer fear writing web APIs in .Net, making C# a language that can be used on the entire application stack, as well as cross-platform.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-03-03 20:20:30","created_by":1,"updated_at":"2015-03-03 20:21:09","updated_by":1,"published_at":"2015-03-03 20:21:09","published_by":1},{"id":3,"uuid":"9a107ef3-b781-4e11-a175-a15e1b3417ca","title":"Build, test and deploy .NET apps with Vagrant and Docker","slug":"build-test-and-deploy-net-apps-with-vagrant-and-docker","markdown":"On a resent project at work we build a cross-platform chat-application with [Xamarin](www.xamarin.com) and [SignalR](http://signalr.net/).\nThe SignalR-hub was to find it's home on a Linux-server, given ASP.NETs new found love for other platforms than Windows and IIS.\nWe had limited time and the back-end guys were developing the hub in Visual Studio. To help them make sure the code they wrote would be Mono-compatible (and easy to deploy for testing), I turned to my two favorite pieces of open source technology: [Vagrant](https://www.vagrantup.com/) and [Docker](https://www.docker.com/).\n\n###The code\nFirst, I wrote a simple Dockerfile based on the latest Mono-baseimage that adds the code and runs xbuild in the build-process. When the container is run without parameters, it deploys the server.\n\n    FROM mono:latest\n    ADD SignalRServer SignalRServer\n    RUN xbuild /SignalRServer/SignalRServer.sln\n\n    EXPOSE 8080\n\n    CMD mono /SignalRServer/SignalRServer.LocalWebServer/bin/Debug/SignalRServer.LocalWebServer.exe\n\nTo automate this process, a simple script...\n\n    # /bin/bash\n    sudo docker build -t dirc/signalrhub /vagrant/\n    sudo docker run -p 8080:8080 -td dirc/signalrhub\n\n ...before we finally spin up a VM via this Vagrantfile and `vagrant up`.\n\n    # -*- mode: ruby -*-\n    # vi: set ft=ruby :\n    Vagrant::Config.run do |config|\n\n      config.vm.box = \"virtualUbuntu64\"\n      config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n\n      config.vm.provision :shell, :inline => \"sudo apt-get update\"\n      config.vm.provision :shell, :inline => \"sudo apt-get install curl -y\"\n      config.vm.provision :shell, :inline => \"curl -s https://get.docker.io/ubuntu/ | sudo sh > /dev/null 2>&1\"\n      config.vm.provision :shell, :inline => \"/vagrant/buildAndDeploySignalRHub\"\n      config.vm.forward_port 8080, 8080\n\n      end\n\n      Vagrant.configure(\"2\") do |config|\n      config.vm.provider :virtualbox do |virtualbox|\n      virtualbox.customize [\"modifyvm\", :id, \"--memory\", \"1024\"]\n      end\n      end\n\nThats it! With a simple `vagrant up` we get an Ubuntu VM, Docker installed, the code compiled and the hub deployed, and is available at `http://localhost:8080`.\nWhen the Hub is finished the Docker-container can easily be moved to the production server. If the Windows-guys uses code that breaks the Mono-compability, it is easily discovered.\n","mobiledoc":null,"html":"<p>On a resent project at work we build a cross-platform chat-application with <a href=\"www.xamarin.com\">Xamarin</a> and <a href=\"http://signalr.net/\">SignalR</a>. <br />\nThe SignalR-hub was to find it's home on a Linux-server, given ASP.NETs new found love for other platforms than Windows and IIS. <br />\nWe had limited time and the back-end guys were developing the hub in Visual Studio. To help them make sure the code they wrote would be Mono-compatible (and easy to deploy for testing), I turned to my two favorite pieces of open source technology: <a href=\"https://www.vagrantup.com/\">Vagrant</a> and <a href=\"https://www.docker.com/\">Docker</a>.</p>\n\n<h3 id=\"thecode\">The code</h3>\n\n<p>First, I wrote a simple Dockerfile based on the latest Mono-baseimage that adds the code and runs xbuild in the build-process. When the container is run without parameters, it deploys the server.</p>\n\n<pre><code>FROM mono:latest\nADD SignalRServer SignalRServer\nRUN xbuild /SignalRServer/SignalRServer.sln\n\nEXPOSE 8080\n\nCMD mono /SignalRServer/SignalRServer.LocalWebServer/bin/Debug/SignalRServer.LocalWebServer.exe\n</code></pre>\n\n<p>To automate this process, a simple script...</p>\n\n<pre><code># /bin/bash\nsudo docker build -t dirc/signalrhub /vagrant/\nsudo docker run -p 8080:8080 -td dirc/signalrhub\n</code></pre>\n\n<p>...before we finally spin up a VM via this Vagrantfile and <code>vagrant up</code>.</p>\n\n<pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\nVagrant::Config.run do |config|\n\n  config.vm.box = \"virtualUbuntu64\"\n  config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n\n  config.vm.provision :shell, :inline =&gt; \"sudo apt-get update\"\n  config.vm.provision :shell, :inline =&gt; \"sudo apt-get install curl -y\"\n  config.vm.provision :shell, :inline =&gt; \"curl -s https://get.docker.io/ubuntu/ | sudo sh &gt; /dev/null 2&gt;&amp;1\"\n  config.vm.provision :shell, :inline =&gt; \"/vagrant/buildAndDeploySignalRHub\"\n  config.vm.forward_port 8080, 8080\n\n  end\n\n  Vagrant.configure(\"2\") do |config|\n  config.vm.provider :virtualbox do |virtualbox|\n  virtualbox.customize [\"modifyvm\", :id, \"--memory\", \"1024\"]\n  end\n  end\n</code></pre>\n\n<p>Thats it! With a simple <code>vagrant up</code> we get an Ubuntu VM, Docker installed, the code compiled and the hub deployed, and is available at <code>http://localhost:8080</code>. <br />\nWhen the Hub is finished the Docker-container can easily be moved to the production server. If the Windows-guys uses code that breaks the Mono-compability, it is easily discovered.</p>","amp":null,"image":null,"featured":1,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"Build, test and deploy .NET apps with Vagrant and Docker","meta_description":null,"author_id":1,"created_at":"2015-03-03 20:26:09","created_by":1,"updated_at":"2015-03-04 17:55:38","updated_by":1,"published_at":"2015-03-03 20:26:57","published_by":1},{"id":4,"uuid":"636eb571-70c2-4ada-9b9a-c5258d8a257d","title":"How I Read","slug":"how-i-read","markdown":"\nDuring my last year of University College, I rediscovered my childhood joy of reading. At the time I was working on my bachelor’s degree (an app for the [company I now work for](www.dips.no)) in a structured way from morning to afternoon, so my evenings were open for the first time in years. At one point, I had watched all seasons of The Wire, all seasons of House MD and Breaking Bad had come to its conclusion. Out of pure boredom, I decided, \"hey, might as well do some research on that bachelor thesis.\" So I went on Amazon and ordered Donald Norman's [The Design of Everyday Things](http://www.amazon.com/The-Design-Everyday-Things-Expanded/dp/0465050654/ref=tmm_pap_title_0?ie=UTF8&qid=1426446749&sr=1-1). \nWhen I got it the book I read it from cover to cover in 4 days. It was THAT good. I ordered Norman's [Living with Complexity](http://www.amazon.com/Living-Complexity-Donald-A-Norman/dp/0262014866/ref=tmm_hrd_title_0?ie=UTF8&qid=1426447113&sr=1-1) next, before Robert C. Martin's [Clean Code](http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_1?s=books&ie=UTF8&qid=1426448269&sr=1-1&keywords=clean+code).\n\nThe list goes on, and now after 15 months, I have read about 60 books, both fictional and non-fictional. How do I get through so many books? A lot has to do with organization.\n###Tips and tricks for reading lots of books\nA while ago I read an excellent [article](http://lifehacker.com/my-secret-to-reading-a-lot-of-books-514189426) on [Lifehacker](http://lifehacker.com) where a guy used [Trello](https://trello.com) (a free service for organizing Kanban-boards) to keep track of his reading lists. I tried it out and found it really useful. In my board I have 4 categories: 'Backlog', 'To Read', 'Reading' and 'Done'. If I stumble upon a book I want to read, I put it in the Backlog. If the 'To Read' list has less than 7 books in it or I want to read it in the near future, I put it in that category. Each book is also color coded, with blue for fiction and green for non-fiction and novels. As a rule of thumb, I try to read at least one non-fictional book each month to keep my brain up to speed and actually learn something.\n\n![](http://i.imgur.com/ycM2jrX.png)\n\nThe Kanban board also helps with motivation: Nothing is as good as dragging a book from 'Reading' to 'Done'.\n\nTo remember as much as possible I always keep a notebook handy when reading non-fiction. Before each reading session, I read the notes I wrote last time I sat down with the book to refresh. This technique has done wonders for my memory. Thanks to this habit, a notebook is now always with me, and it have saves more than one idea from oblivion. For reading academia and non-fiction, I try to set aside 30 minutes each day. A great place to get some reading done is in airports and on planes. Let’s face it, you just sit there and wait to get to where you are going anyway.\n\nFor reading two books simultaneously, audiobooks has revolutionized my life. If I want to read fiction (I find it too hard to concentrate while listening to non-fiction), I always check [Audible](http://www.audible.com/) first.\n\nBy just listening to a audiobook to and from work, I get 40 minutes of book each day. Out for a jog? 30 minutes. Doing laundry? 15 minutes. All this time adds up, so going through a couple of books a month is easy. It is also easier for my brain to distinguishing between two books when one is read on paper and the other is listened to.\n\nFor e-books I use my Kindle and the Kindle-app is installed everywear. I use the excelente application [Calibre](http://calibre-ebook.com/) to mange it all. This library is stored on my Dropbox.\n","mobiledoc":null,"html":"<p>During my last year of University College, I rediscovered my childhood joy of reading. At the time I was working on my bachelor’s degree (an app for the <a href=\"www.dips.no\">company I now work for</a>) in a structured way from morning to afternoon, so my evenings were open for the first time in years. At one point, I had watched all seasons of The Wire, all seasons of House MD and Breaking Bad had come to its conclusion. Out of pure boredom, I decided, \"hey, might as well do some research on that bachelor thesis.\" So I went on Amazon and ordered Donald Norman's <a href=\"http://www.amazon.com/The-Design-Everyday-Things-Expanded/dp/0465050654/ref=tmm_pap_title_0?ie=UTF8&amp;qid=1426446749&amp;sr=1-1\">The Design of Everyday Things</a>. <br />\nWhen I got it the book I read it from cover to cover in 4 days. It was THAT good. I ordered Norman's <a href=\"http://www.amazon.com/Living-Complexity-Donald-A-Norman/dp/0262014866/ref=tmm_hrd_title_0?ie=UTF8&amp;qid=1426447113&amp;sr=1-1\">Living with Complexity</a> next, before Robert C. Martin's <a href=\"http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1426448269&amp;sr=1-1&amp;keywords=clean+code\">Clean Code</a>.</p>\n\n<p>The list goes on, and now after 15 months, I have read about 60 books, both fictional and non-fictional. How do I get through so many books? A lot has to do with organization.  </p>\n\n<h3 id=\"tipsandtricksforreadinglotsofbooks\">Tips and tricks for reading lots of books</h3>\n\n<p>A while ago I read an excellent <a href=\"http://lifehacker.com/my-secret-to-reading-a-lot-of-books-514189426\">article</a> on <a href=\"http://lifehacker.com\">Lifehacker</a> where a guy used <a href=\"https://trello.com\">Trello</a> (a free service for organizing Kanban-boards) to keep track of his reading lists. I tried it out and found it really useful. In my board I have 4 categories: 'Backlog', 'To Read', 'Reading' and 'Done'. If I stumble upon a book I want to read, I put it in the Backlog. If the 'To Read' list has less than 7 books in it or I want to read it in the near future, I put it in that category. Each book is also color coded, with blue for fiction and green for non-fiction and novels. As a rule of thumb, I try to read at least one non-fictional book each month to keep my brain up to speed and actually learn something.</p>\n\n<p><img src=\"http://i.imgur.com/ycM2jrX.png\" alt=\"\" /></p>\n\n<p>The Kanban board also helps with motivation: Nothing is as good as dragging a book from 'Reading' to 'Done'.</p>\n\n<p>To remember as much as possible I always keep a notebook handy when reading non-fiction. Before each reading session, I read the notes I wrote last time I sat down with the book to refresh. This technique has done wonders for my memory. Thanks to this habit, a notebook is now always with me, and it have saves more than one idea from oblivion. For reading academia and non-fiction, I try to set aside 30 minutes each day. A great place to get some reading done is in airports and on planes. Let’s face it, you just sit there and wait to get to where you are going anyway.</p>\n\n<p>For reading two books simultaneously, audiobooks has revolutionized my life. If I want to read fiction (I find it too hard to concentrate while listening to non-fiction), I always check <a href=\"http://www.audible.com/\">Audible</a> first.</p>\n\n<p>By just listening to a audiobook to and from work, I get 40 minutes of book each day. Out for a jog? 30 minutes. Doing laundry? 15 minutes. All this time adds up, so going through a couple of books a month is easy. It is also easier for my brain to distinguishing between two books when one is read on paper and the other is listened to.</p>\n\n<p>For e-books I use my Kindle and the Kindle-app is installed everywear. I use the excelente application <a href=\"http://calibre-ebook.com/\">Calibre</a> to mange it all. This library is stored on my Dropbox.</p>","amp":null,"image":"/content/images/2015/03/2015-03-16-21-41-47-1.jpg","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-03-16 20:32:33","created_by":1,"updated_at":"2015-03-22 21:18:06","updated_by":1,"published_at":"2015-03-16 20:52:47","published_by":1},{"id":5,"uuid":"b53e93f1-1278-444c-81c2-4cbafe5807e7","title":"Zookeeper-aware application server","slug":"zookeeper-aware-application-server","markdown":"\n###The situation\nAt work we use [Apache Solr](http://lucene.apache.org/solr/) from our application server to index and offer search in documents. A LOT of documents. The largest customer of our system has as much as 130 million documents, and the amount keeps growing exponentially. With this in mind, we knew early that we had to shard up and cluster the Solr-collection, this meant using [SolrCloud](https://wiki.apache.org/solr/SolrCloud). SolrCloud uses [Apache ZooKeeper](https://zookeeper.apache.org/) to keep track of config files, live nodes, coordination etc. Zookeeper is a distributed file system that supports putting watchers on its files, thus calling back to the application when a file changes.\n\nAs a abstraction layer we use [SolrNet](https://github.com/mausch/SolrNet) to talk to Solr. The only problem here is that SolrNet has no support for SolrCloud ([yet!](https://github.com/mausch/SolrNet/pull/187)), which means that there is no good way to talk to the cluster, oure system's application server must connect to a single, specified node. If this node goes down, we lose the connection to the cluster. So we have redundancy in SolrCloud, but the consumer can't use it? We can't live with that.\n\n###The solution\nWe decided to write a ZooKeeper-API ourself, letting the application server receive a suitable Solr-node on startup. If this node goes down in production, a watcher on the clusterstate-file kicks inn and changes the active Solr-node, giving us failover without restart.\n\nThere are some ways to talk to Zookeeper from .NET, mainly the [Apache ZooKeeper .NET Client](https://www.nuget.org/packages/ZooKeeper.Net/) package on NuGet. In this case, I decided to exploit the opportunity to test out [IKVM](http://www.ikvm.net/), a (magical) project that let's you convert Java JAR-files to DLLs. Yeah.\n\nHere is some code from the API:\n\n\n    using System;\n    using System.Diagnostics.CodeAnalysis;\n    using Common.Logging;\n    using org.apache.zookeeper;\n    using org.apache.zookeeper.data;\n\n    public class ZookeeperNodeDataChangeWatcher : Watcher, org.apache.zookeeper.AsyncCallback.DataCallback,\n        org.apache.zookeeper.AsyncCallback.StatCallback, IDisposable\n    {\n        private static readonly ILog s_log = LogManager.GetLogger(typeof(ZookeeperNodeDataChangeWatcher));\n\n        private const int SessionTimeout = 2000;\n\n        private static string s_nodePath;\n\n        private readonly ZooKeeper m_zookeeper;\n\n        private readonly IObserver<Maybe<byte[]>> m_observer;\n\n        private object m_nodeExists;\n        private bool IsDisposed { get; set; }\n\n        public ZookeeperNodeDataChangeWatcher(string zookeeperName, IObserver<Maybe<byte[]>> observer, string nodePath)\n        {\n            this.m_observer = observer;\n            this.IsDisposed = false;\n            s_nodePath = nodePath;\n            this.m_zookeeper = new ZooKeeper(zookeeperName, SessionTimeout, this);\n\n            this.GetData();\n\n            s_log.Info(string.Format(\"Zookeeperwatcher: started watcher: {0},{1}\", this.m_zookeeper, s_nodePath));\n        }\n\n        private void GetData()\n        {\n            try\n            {\n                this.m_zookeeper.getData(s_nodePath, this, this, null);\n\n            }\n            catch (Exception e)\n            {\n                this.m_observer.OnError(e);\n                s_log.Info(string.Format(\"Error in getData: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n            }\n        }\n\n        private void WatchAgain()\n        {\n            try\n            {\n                this.m_zookeeper.exists(s_nodePath, this, this, null);\n                s_log.Debug(string.Format(\"Rewatching: {0},{1}\", this.m_zookeeper, s_nodePath));\n            }\n            catch (Exception e)\n            {\n                this.m_observer.OnError(e);\n                s_log.Error(string.Format(\"Error in rewatching: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n            }\n        }\n\n        public void processResult(int i, string str, object obj, byte[] barr, Stat s)\n        {\n            var returnCode = i;\n            var data = barr;\n\n            if (this.IsDisposed)\n            {\n                return;\n            }\n\n            if (returnCode == KeeperException.Code.OK.intValue())\n            {\n                this.m_nodeExists = true;\n                this.m_observer.OnNext(Maybe.Return(data));\n                this.WatchAgain();\n            }\n            else if (returnCode == KeeperException.Code.NONODE.intValue())\n            {\n                this.m_nodeExists = false;\n                this.WatchAgain();\n                this.m_observer.OnNext(Maybe.Empty<byte[]>());\n            }\n            else\n            {\n                this.m_observer.OnError(\n                    KeeperException.create(KeeperException.Code.get(i)));\n            }\n        }\n\n        public void processResult(int i, string str, object obj, Stat s)\n        {\n            if (this.IsDisposed)\n            {\n                return;\n            }\n\n            var returnCode = i;\n            if (returnCode == KeeperException.Code.OK.intValue() ||\n                returnCode == KeeperException.Code.NONODE.intValue() ||\n                returnCode == KeeperException.Code.NODEEXISTS.intValue())\n            {\n                \n                var nodeExistsNow = s != null;\n                var oldExists = (bool?)this.m_nodeExists;\n                this.m_nodeExists = nodeExistsNow;\n                if (nodeExistsNow && nodeExistsNow != oldExists)\n                {\n                    this.GetData();\n                }\n            }\n            else\n            {\n               this.m_observer.OnError(KeeperException.create(\n                    KeeperException.Code.get(returnCode)));\n            }\n        }\n\n        public void process(WatchedEvent e)\n        {\n            if (this.IsDisposed)\n            {\n                return;\n            }\n\n            if (s_nodePath != null && (e == null || s_nodePath != e.getPath()))\n            {\n                this.WatchAgain();\n                return;\n            }\n\n            var type = e.getType();\n            if (type == Watcher.Event.EventType.NodeCreated ||\n                type == Watcher.Event.EventType.NodeDataChanged)\n            {\n                this.GetData();\n            }\n            else if (type == Watcher.Event.EventType.NodeDeleted)\n            {\n                this.m_nodeExists = false;\n                this.m_observer.OnNext(Maybe.Empty<byte[]>());\n                this.WatchAgain();\n            }\n            else\n            {\n                this.WatchAgain();\n            }\n        }\n\n        public void Dispose()\n        {\n            s_log.Debug(string.Format(\"Watcher is disposed: {0},{1}\", this.m_zookeeper, s_nodePath));\n            GC.SuppressFinalize(this);\n        }\n    }\n\nThe implementation of the Clusterstate-watcher:\n\n\n    using System;\n    using System.Collections.Generic;\n    using System.Linq;\n    using System.Reactive.Linq;\n    using System.Text;\n    using Common.Logging;\n    using DIPS.Zookeeper.Model;\n\n    using Newtonsoft.Json;\n    using Newtonsoft.Json.Linq;\n\n    public class ZooKeeperClusterstateWatcher\n    {\n        public event ActiveSolrCollectionChangedDelegate ActiveSolrCollectionChanged;\n\n        private static readonly ILog s_log = LogManager.GetLogger(typeof(ZooKeeperClusterstateWatcher));\n\n        private readonly string m_solrCollectionName;\n\n        private readonly string m_zookeeperConnectionString;\n\n        private const string ClusterState = \"/clusterstate.json\";\n\n        public ZooKeeperClusterstateWatcher(string zookeeperConnectionString, string collectionName)\n        {\n            this.m_solrCollectionName = collectionName;\n            m_zookeeperConnectionString = zookeeperConnectionString;\n            Subscriber();\n        }\n\n        public IObservable<Maybe<byte[]>> WatchData(string zookeeper, string nodePath)\n        {\n            var ob = Observable.Create<Maybe<byte[]>>(\n                    observer =>\n                        new ZookeeperNodeDataChangeWatcher(zookeeper, observer, nodePath));\n            return ob;\n        }\n\n        public void UpdateActiveSolrCollection(SolrCollection collection)\n        {\n            ActiveSolrCollection.SolrCollection = collection;\n            if (ActiveSolrCollectionChanged != null)\n            {\n                ActiveSolrCollectionChanged(this, new ActiveSolrCollectionChangedDelegateArgs() { ActiveCollection = collection });\n            }\n        }\n\n        public void HandleException(Exception ex)\n        {\n            s_log.Error(string.Format(\"Error in clusterstate.json watcher : {0}\", ex));\n\n            if (ex is org.apache.zookeeper.KeeperException.ConnectionLossException)\n            {\n                Subscriber();\n            }\n        }\n\n        public SolrCollection ParseClusterstateJson(string content)\n        {\n            try\n            {\n                var clusterstate = JsonConvert.DeserializeObject<dynamic>(content);\n                var collection = new SolrCollection { Name = this.m_solrCollectionName };\n                var varShards = new List<Shard>();\n\n                foreach (var solrCollection in clusterstate[collection.Name])\n                {\n                    collection.MaxShardsPerNode = int.Parse(clusterstate[collection.Name][\"maxShardsPerNode\"].ToString());\n                    collection.ReplicationFactor = int.Parse(clusterstate[collection.Name][\"replicationFactor\"].ToString());\n                    collection.AutoAddReplicas = bool.Parse(clusterstate[collection.Name][\"autoAddReplicas\"].ToString());\n                    collection.Router = new Router { Name = clusterstate[collection.Name][\"router\"][\"name\"].ToString() };\n                    foreach (JObject shards in solrCollection.Children<JObject>())\n                    {\n                        foreach (var shard in shards.Properties().Where(prop => !prop.Name.Equals(\"name\")))\n                        {\n                            var varShard = new Shard();\n\n                            if (clusterstate[collection.Name][\"shards\"][shard.Name][\"state\"].ToString().Equals(\"active\"))\n                            {\n                                varShard.IsActive = true;\n                            }\n\n                            varShard.Name = shard.Name;\n                            varShard.Range = clusterstate[collection.Name][\"shards\"][shard.Name][\"range\"].ToString();\n                            var coreNodes = new List<CoreNode>();\n\n                            foreach (var replicas in clusterstate[collection.Name][\"shards\"][shard.Name][\"replicas\"])\n                            {\n                                var coreNode = new CoreNode();\n                                foreach (JObject replicass in replicas.Children<JObject>())\n                                {\n                                    foreach (var coreNodeInReplica in replicass.Properties())\n                                    {\n                                        switch (coreNodeInReplica.Name)\n                                        {\n                                            case \"state\":\n                                                coreNode.State = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"node_name\":\n                                                coreNode.NodeName = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"core\":\n                                                coreNode.CoreName = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"base_url\":\n                                                coreNode.BaseUrl = coreNodeInReplica.Value.ToString();\n                                                break;\n                                            case \"leader\":\n                                                coreNode.IsLeader = Convert.ToBoolean(coreNodeInReplica.Value.ToString());\n                                                break;\n                                        }\n                                    }\n\n                                    coreNodes.Add(coreNode);\n                                }\n\n                                varShard.ReplicaCores = coreNodes;\n                            }\n\n                            varShards.Add(varShard);\n                        }\n                    }\n                }\n\n                collection.Shards = varShards;\n                s_log.Debug(string.Format(\"Registered new Solr-collection: {0}. Number of shards is {1}\", collection.Name, collection.Shards.Count()));\n                return collection;\n            }\n            catch (Exception e)\n            {\n                s_log.Error(string.Format(\"Zookeeperwatcher: error in clusterstate.json parser : {0}\", e.Message));\n            }\n\n            return new SolrCollection();\n        }\n\n        private void Subscriber()\n        {\n            var observableClusterState = this.WatchData(m_zookeeperConnectionString, ClusterState);\n            var subscription =\n                observableClusterState.Subscribe(\n                    x => this.UpdateActiveSolrCollection(this.ParseClusterstateJson(Encoding.UTF8.GetString(x.Value))),\n                    this.HandleException);\n        }\n    }\n\n    public delegate void ActiveSolrCollectionChangedDelegate(object sender, ActiveSolrCollectionChangedDelegateArgs args);\n\n\nFrom code we simply read out ``ActiveSolrCollection.SolrCollection.LeaderNode``.\n\n>NOTE: In retrospect, we ended up rewriting and using the [Apache ZooKeeper .NET Client](https://www.nuget.org/packages/ZooKeeper.Net/) package instead, simply because IKVM makes much mess, giving us dependencies to a lot of DLLs and so on.\n\n### Some slides\nI ended up giving a short talk about the work as well as presenting extension that can be made. In the future I will look at ZooKeeper as a place to place distributed config for services, as well as service discovery.\n\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/46646103\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/mostii/zookeeper-aware\" title=\"Zookeeper-aware application server\" target=\"_blank\">Zookeeper-aware application server</a> </strong> from <strong><a href=\"//www.slideshare.net/mostii\" target=\"_blank\">Andreas Mosti</a></strong> </div>\n\nThis work is inspired by the work of the guys over at [Palladium Consulting](http://www.palladiumconsulting.com/2014/06/using-zookeeper-from-net-ikvm-task/).\n","mobiledoc":null,"html":"<h3 id=\"thesituation\">The situation</h3>\n\n<p>At work we use <a href=\"http://lucene.apache.org/solr/\">Apache Solr</a> from our application server to index and offer search in documents. A LOT of documents. The largest customer of our system has as much as 130 million documents, and the amount keeps growing exponentially. With this in mind, we knew early that we had to shard up and cluster the Solr-collection, this meant using <a href=\"https://wiki.apache.org/solr/SolrCloud\">SolrCloud</a>. SolrCloud uses <a href=\"https://zookeeper.apache.org/\">Apache ZooKeeper</a> to keep track of config files, live nodes, coordination etc. Zookeeper is a distributed file system that supports putting watchers on its files, thus calling back to the application when a file changes.</p>\n\n<p>As a abstraction layer we use <a href=\"https://github.com/mausch/SolrNet\">SolrNet</a> to talk to Solr. The only problem here is that SolrNet has no support for SolrCloud (<a href=\"https://github.com/mausch/SolrNet/pull/187\">yet!</a>), which means that there is no good way to talk to the cluster, oure system's application server must connect to a single, specified node. If this node goes down, we lose the connection to the cluster. So we have redundancy in SolrCloud, but the consumer can't use it? We can't live with that.</p>\n\n<h3 id=\"thesolution\">The solution</h3>\n\n<p>We decided to write a ZooKeeper-API ourself, letting the application server receive a suitable Solr-node on startup. If this node goes down in production, a watcher on the clusterstate-file kicks inn and changes the active Solr-node, giving us failover without restart.</p>\n\n<p>There are some ways to talk to Zookeeper from .NET, mainly the <a href=\"https://www.nuget.org/packages/ZooKeeper.Net/\">Apache ZooKeeper .NET Client</a> package on NuGet. In this case, I decided to exploit the opportunity to test out <a href=\"http://www.ikvm.net/\">IKVM</a>, a (magical) project that let's you convert Java JAR-files to DLLs. Yeah.</p>\n\n<p>Here is some code from the API:</p>\n\n<pre><code>using System;\nusing System.Diagnostics.CodeAnalysis;\nusing Common.Logging;\nusing org.apache.zookeeper;\nusing org.apache.zookeeper.data;\n\npublic class ZookeeperNodeDataChangeWatcher : Watcher, org.apache.zookeeper.AsyncCallback.DataCallback,\n    org.apache.zookeeper.AsyncCallback.StatCallback, IDisposable\n{\n    private static readonly ILog s_log = LogManager.GetLogger(typeof(ZookeeperNodeDataChangeWatcher));\n\n    private const int SessionTimeout = 2000;\n\n    private static string s_nodePath;\n\n    private readonly ZooKeeper m_zookeeper;\n\n    private readonly IObserver&lt;Maybe&lt;byte[]&gt;&gt; m_observer;\n\n    private object m_nodeExists;\n    private bool IsDisposed { get; set; }\n\n    public ZookeeperNodeDataChangeWatcher(string zookeeperName, IObserver&lt;Maybe&lt;byte[]&gt;&gt; observer, string nodePath)\n    {\n        this.m_observer = observer;\n        this.IsDisposed = false;\n        s_nodePath = nodePath;\n        this.m_zookeeper = new ZooKeeper(zookeeperName, SessionTimeout, this);\n\n        this.GetData();\n\n        s_log.Info(string.Format(\"Zookeeperwatcher: started watcher: {0},{1}\", this.m_zookeeper, s_nodePath));\n    }\n\n    private void GetData()\n    {\n        try\n        {\n            this.m_zookeeper.getData(s_nodePath, this, this, null);\n\n        }\n        catch (Exception e)\n        {\n            this.m_observer.OnError(e);\n            s_log.Info(string.Format(\"Error in getData: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n        }\n    }\n\n    private void WatchAgain()\n    {\n        try\n        {\n            this.m_zookeeper.exists(s_nodePath, this, this, null);\n            s_log.Debug(string.Format(\"Rewatching: {0},{1}\", this.m_zookeeper, s_nodePath));\n        }\n        catch (Exception e)\n        {\n            this.m_observer.OnError(e);\n            s_log.Error(string.Format(\"Error in rewatching: {0},{1},{2}\", this.m_zookeeper, s_nodePath, e.Message));\n        }\n    }\n\n    public void processResult(int i, string str, object obj, byte[] barr, Stat s)\n    {\n        var returnCode = i;\n        var data = barr;\n\n        if (this.IsDisposed)\n        {\n            return;\n        }\n\n        if (returnCode == KeeperException.Code.OK.intValue())\n        {\n            this.m_nodeExists = true;\n            this.m_observer.OnNext(Maybe.Return(data));\n            this.WatchAgain();\n        }\n        else if (returnCode == KeeperException.Code.NONODE.intValue())\n        {\n            this.m_nodeExists = false;\n            this.WatchAgain();\n            this.m_observer.OnNext(Maybe.Empty&lt;byte[]&gt;());\n        }\n        else\n        {\n            this.m_observer.OnError(\n                KeeperException.create(KeeperException.Code.get(i)));\n        }\n    }\n\n    public void processResult(int i, string str, object obj, Stat s)\n    {\n        if (this.IsDisposed)\n        {\n            return;\n        }\n\n        var returnCode = i;\n        if (returnCode == KeeperException.Code.OK.intValue() ||\n            returnCode == KeeperException.Code.NONODE.intValue() ||\n            returnCode == KeeperException.Code.NODEEXISTS.intValue())\n        {\n\n            var nodeExistsNow = s != null;\n            var oldExists = (bool?)this.m_nodeExists;\n            this.m_nodeExists = nodeExistsNow;\n            if (nodeExistsNow &amp;&amp; nodeExistsNow != oldExists)\n            {\n                this.GetData();\n            }\n        }\n        else\n        {\n           this.m_observer.OnError(KeeperException.create(\n                KeeperException.Code.get(returnCode)));\n        }\n    }\n\n    public void process(WatchedEvent e)\n    {\n        if (this.IsDisposed)\n        {\n            return;\n        }\n\n        if (s_nodePath != null &amp;&amp; (e == null || s_nodePath != e.getPath()))\n        {\n            this.WatchAgain();\n            return;\n        }\n\n        var type = e.getType();\n        if (type == Watcher.Event.EventType.NodeCreated ||\n            type == Watcher.Event.EventType.NodeDataChanged)\n        {\n            this.GetData();\n        }\n        else if (type == Watcher.Event.EventType.NodeDeleted)\n        {\n            this.m_nodeExists = false;\n            this.m_observer.OnNext(Maybe.Empty&lt;byte[]&gt;());\n            this.WatchAgain();\n        }\n        else\n        {\n            this.WatchAgain();\n        }\n    }\n\n    public void Dispose()\n    {\n        s_log.Debug(string.Format(\"Watcher is disposed: {0},{1}\", this.m_zookeeper, s_nodePath));\n        GC.SuppressFinalize(this);\n    }\n}\n</code></pre>\n\n<p>The implementation of the Clusterstate-watcher:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Reactive.Linq;\nusing System.Text;\nusing Common.Logging;\nusing DIPS.Zookeeper.Model;\n\nusing Newtonsoft.Json;\nusing Newtonsoft.Json.Linq;\n\npublic class ZooKeeperClusterstateWatcher\n{\n    public event ActiveSolrCollectionChangedDelegate ActiveSolrCollectionChanged;\n\n    private static readonly ILog s_log = LogManager.GetLogger(typeof(ZooKeeperClusterstateWatcher));\n\n    private readonly string m_solrCollectionName;\n\n    private readonly string m_zookeeperConnectionString;\n\n    private const string ClusterState = \"/clusterstate.json\";\n\n    public ZooKeeperClusterstateWatcher(string zookeeperConnectionString, string collectionName)\n    {\n        this.m_solrCollectionName = collectionName;\n        m_zookeeperConnectionString = zookeeperConnectionString;\n        Subscriber();\n    }\n\n    public IObservable&lt;Maybe&lt;byte[]&gt;&gt; WatchData(string zookeeper, string nodePath)\n    {\n        var ob = Observable.Create&lt;Maybe&lt;byte[]&gt;&gt;(\n                observer =&gt;\n                    new ZookeeperNodeDataChangeWatcher(zookeeper, observer, nodePath));\n        return ob;\n    }\n\n    public void UpdateActiveSolrCollection(SolrCollection collection)\n    {\n        ActiveSolrCollection.SolrCollection = collection;\n        if (ActiveSolrCollectionChanged != null)\n        {\n            ActiveSolrCollectionChanged(this, new ActiveSolrCollectionChangedDelegateArgs() { ActiveCollection = collection });\n        }\n    }\n\n    public void HandleException(Exception ex)\n    {\n        s_log.Error(string.Format(\"Error in clusterstate.json watcher : {0}\", ex));\n\n        if (ex is org.apache.zookeeper.KeeperException.ConnectionLossException)\n        {\n            Subscriber();\n        }\n    }\n\n    public SolrCollection ParseClusterstateJson(string content)\n    {\n        try\n        {\n            var clusterstate = JsonConvert.DeserializeObject&lt;dynamic&gt;(content);\n            var collection = new SolrCollection { Name = this.m_solrCollectionName };\n            var varShards = new List&lt;Shard&gt;();\n\n            foreach (var solrCollection in clusterstate[collection.Name])\n            {\n                collection.MaxShardsPerNode = int.Parse(clusterstate[collection.Name][\"maxShardsPerNode\"].ToString());\n                collection.ReplicationFactor = int.Parse(clusterstate[collection.Name][\"replicationFactor\"].ToString());\n                collection.AutoAddReplicas = bool.Parse(clusterstate[collection.Name][\"autoAddReplicas\"].ToString());\n                collection.Router = new Router { Name = clusterstate[collection.Name][\"router\"][\"name\"].ToString() };\n                foreach (JObject shards in solrCollection.Children&lt;JObject&gt;())\n                {\n                    foreach (var shard in shards.Properties().Where(prop =&gt; !prop.Name.Equals(\"name\")))\n                    {\n                        var varShard = new Shard();\n\n                        if (clusterstate[collection.Name][\"shards\"][shard.Name][\"state\"].ToString().Equals(\"active\"))\n                        {\n                            varShard.IsActive = true;\n                        }\n\n                        varShard.Name = shard.Name;\n                        varShard.Range = clusterstate[collection.Name][\"shards\"][shard.Name][\"range\"].ToString();\n                        var coreNodes = new List&lt;CoreNode&gt;();\n\n                        foreach (var replicas in clusterstate[collection.Name][\"shards\"][shard.Name][\"replicas\"])\n                        {\n                            var coreNode = new CoreNode();\n                            foreach (JObject replicass in replicas.Children&lt;JObject&gt;())\n                            {\n                                foreach (var coreNodeInReplica in replicass.Properties())\n                                {\n                                    switch (coreNodeInReplica.Name)\n                                    {\n                                        case \"state\":\n                                            coreNode.State = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"node_name\":\n                                            coreNode.NodeName = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"core\":\n                                            coreNode.CoreName = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"base_url\":\n                                            coreNode.BaseUrl = coreNodeInReplica.Value.ToString();\n                                            break;\n                                        case \"leader\":\n                                            coreNode.IsLeader = Convert.ToBoolean(coreNodeInReplica.Value.ToString());\n                                            break;\n                                    }\n                                }\n\n                                coreNodes.Add(coreNode);\n                            }\n\n                            varShard.ReplicaCores = coreNodes;\n                        }\n\n                        varShards.Add(varShard);\n                    }\n                }\n            }\n\n            collection.Shards = varShards;\n            s_log.Debug(string.Format(\"Registered new Solr-collection: {0}. Number of shards is {1}\", collection.Name, collection.Shards.Count()));\n            return collection;\n        }\n        catch (Exception e)\n        {\n            s_log.Error(string.Format(\"Zookeeperwatcher: error in clusterstate.json parser : {0}\", e.Message));\n        }\n\n        return new SolrCollection();\n    }\n\n    private void Subscriber()\n    {\n        var observableClusterState = this.WatchData(m_zookeeperConnectionString, ClusterState);\n        var subscription =\n            observableClusterState.Subscribe(\n                x =&gt; this.UpdateActiveSolrCollection(this.ParseClusterstateJson(Encoding.UTF8.GetString(x.Value))),\n                this.HandleException);\n    }\n}\n\npublic delegate void ActiveSolrCollectionChangedDelegate(object sender, ActiveSolrCollectionChangedDelegateArgs args);\n</code></pre>\n\n<p>From code we simply read out <code>ActiveSolrCollection.SolrCollection.LeaderNode</code>.</p>\n\n<blockquote>\n  <p>NOTE: In retrospect, we ended up rewriting and using the <a href=\"https://www.nuget.org/packages/ZooKeeper.Net/\">Apache ZooKeeper .NET Client</a> package instead, simply because IKVM makes much mess, giving us dependencies to a lot of DLLs and so on.</p>\n</blockquote>\n\n<h3 id=\"someslides\">Some slides</h3>\n\n<p>I ended up giving a short talk about the work as well as presenting extension that can be made. In the future I will look at ZooKeeper as a place to place distributed config for services, as well as service discovery.</p>\n\n<p><iframe src=\"//www.slideshare.net/slideshow/embed_code/46646103\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/mostii/zookeeper-aware\" title=\"Zookeeper-aware application server\" target=\"_blank\">Zookeeper-aware application server</a> </strong> from <strong><a href=\"//www.slideshare.net/mostii\" target=\"_blank\">Andreas Mosti</a></strong> </div></p>\n\n<p>This work is inspired by the work of the guys over at <a href=\"http://www.palladiumconsulting.com/2014/06/using-zookeeper-from-net-ikvm-task/\">Palladium Consulting</a>.</p>","amp":null,"image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-04 23:01:13","created_by":1,"updated_at":"2015-04-13 21:07:35","updated_by":1,"published_at":"2015-04-13 20:51:36","published_by":1},{"id":6,"uuid":"4b4ca266-4c08-4e59-b592-8ab2bab8b14f","title":"Updating a SolrCloud schema in a live enviroment","slug":"updating-a-solrcloud-schema-in-a-live-enviroment","markdown":"I can guarantee this will happen to you at some point: You find out that you need to add some more feilds in a Solr schema becouse you want to index some more data from you're documents. Does that mean taking down the live nodes, changing the schema and then start them up again? Thankfully, no. With the help of [Zookeeper](https://zookeeper.apache.org/) and the [Solr schema REST API](https://cwiki.apache.org/confluence/display/solr/Schema+API) we can do this live without any pain. \n\n###The API calls:\n\nFirst of, let's update the new config to the zookeeper: \n\n\t\t./zkcli.sh -zkhost myZookeeper:2181 -cmd upconfig -confname myConfig -confdir /path/to/my/conf/\n\t\t\nThen just reload the core (I do this on the shard leader out of habit): \n\n\t\tcurl http://mySolrNode:8983/solr/admin/cores?action=reload&core=myCore\n\t\t\nThis trick has saved me many times. \n","mobiledoc":null,"html":"<p>I can guarantee this will happen to you at some point: You find out that you need to add some more feilds in a Solr schema becouse you want to index some more data from you're documents. Does that mean taking down the live nodes, changing the schema and then start them up again? Thankfully, no. With the help of <a href=\"https://zookeeper.apache.org/\">Zookeeper</a> and the <a href=\"https://cwiki.apache.org/confluence/display/solr/Schema+API\">Solr schema REST API</a> we can do this live without any pain. </p>\n\n<h3 id=\"theapicalls\">The API calls:</h3>\n\n<p>First of, let's update the new config to the zookeeper: </p>\n\n<pre><code>    ./zkcli.sh -zkhost myZookeeper:2181 -cmd upconfig -confname myConfig -confdir /path/to/my/conf/\n</code></pre>\n\n<p>Then just reload the core (I do this on the shard leader out of habit): </p>\n\n<pre><code>    curl http://mySolrNode:8983/solr/admin/cores?action=reload&amp;core=myCore\n</code></pre>\n\n<p>This trick has saved me many times. </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-25 12:49:12","created_by":1,"updated_at":"2015-04-25 12:52:10","updated_by":1,"published_at":"2015-04-25 12:52:10","published_by":1},{"id":7,"uuid":"be44aad0-60f1-4ec8-9b2f-ab2f6200e9a8","title":"Easy way to upgrade Docker","slug":"easy-way-to-upgrade-docker","markdown":"If you use a system with a strickt package repository (like Debian stable, CentOS, RedHat etc.) chances are new releases of Docker won't be pushed right after release. A sweet trick I use on my CentOS machines is to just wget down the new binary like so: \n\n\t\tservice docker stop\n        wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker\n        service docker start\n        \nSince Docker has released with quite a steady pace (and I like latest and greatest) I have eaven made an alias for the job - \n\n\t\talias getLatestDockerBinary='sudo wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker'\n        \nHope this might help somebody save time in the future.","mobiledoc":null,"html":"<p>If you use a system with a strickt package repository (like Debian stable, CentOS, RedHat etc.) chances are new releases of Docker won't be pushed right after release. A sweet trick I use on my CentOS machines is to just wget down the new binary like so: </p>\n\n<pre><code>    service docker stop\n    wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker\n    service docker start\n</code></pre>\n\n<p>Since Docker has released with quite a steady pace (and I like latest and greatest) I have eaven made an alias for the job - </p>\n\n<pre><code>    alias getLatestDockerBinary='sudo wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker'\n</code></pre>\n\n<p>Hope this might help somebody save time in the future.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-04 20:11:11","created_by":1,"updated_at":"2015-05-04 20:20:21","updated_by":1,"published_at":"2015-05-04 20:20:21","published_by":1},{"id":8,"uuid":"ddd0bd10-7909-41e5-9804-f36bbff96b40","title":"Run Github's Atom editor in Docker (Aka. Containers on the desktop)","slug":"untitlrun-githubs-atom-editor-in-docker-aka-containers-on-the-desktoped","markdown":"\nBy now everybody loves [Docker](https://www.docker.com/). I mean, what is there not to love? Never have it been easier to program, pack and deploy you're applications and getting out of [dependency hell](https://www.youtube.com/watch?v=3N3n9FzebAA) while keeping them isolated from each other.\nDocker has solved the whole \"download and install this on the server, but remember to have the right version of Java (no, not that one!) and Tomcat (7, not 8)\" - problem. All servers are happy.\n\nA couple of months ago I read a [blogpost](https://blog.jessfraz.com/post/docker-containers-on-the-desktop/) by the talented Docker engineer [Jessie Frazelle](https://github.com/jfrazelle) who has made a large collection of Docker images for the [desktop](https://github.com/jfrazelle/dockerfiles).\nThe FREAKING desktop. To [quote](https://twitter.com/frazelledazzell/status/596345044912635904) Jessie:\n\n<blockquote class=\"twitter-tweet\" lang=\"no\"><p lang=\"en\" dir=\"ltr\">The worst thing someone could do if I ever left my computer unlocked (which will never happen) is install something directly on my host</p>&mdash; jessie frazelle (@frazelledazzell) <a href=\"https://twitter.com/frazelledazzell/status/596345044912635904\">7. mai 2015</a></blockquote> <script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nFeeling inspired by this crazy lady I decided to try it out myself. So here is a Dockerfile for the [Atom](https://atom.io/) editor you can run on you're Linux desktop (because, why the hell not?)\n\n### Dockerfile\n\n\n    # Based on work by Jessie Frazelle, https://github.com/jfrazelle/dockerfiles/blob/master/atom/Dockerfile\n    FROM ubuntu:14.04\n    MAINTAINER Andreas Mosti <andreas.mosti@gmail.com>\n\n    RUN apt-get update && apt-get install -y \\\n        build-essential \\\n        ca-certificates \\\n        curl \\\n        git \\\n        libasound2 \\\n        libgconf-2-4 \\\n        libgnome-keyring-dev \\\n        libgtk2.0-0 \\\n        libnss3 \\\n        libxtst6 \\\n        --no-install-recommends\n\n    RUN curl -sL https://deb.nodesource.com/setup | bash -\n    RUN apt-get install -y nodejs\n\n    RUN git clone https://github.com/atom/atom /src\n    WORKDIR /src\n    RUN git fetch && git checkout $(git describe --tags `git rev-list --tags --max-count=1`)\n    RUN script/build && script/grunt install\n\n    # set up user and permission\n    RUN export uid=1000 gid=1000 && \\\n        mkdir -p /home/developer && \\\n        echo \"developer:x:${uid}:${gid}:Developer,,,:/home/developer:/bin/bash\" >> /etc/passwd && \\\n        echo \"developer:x:${uid}:\" >> /etc/group && \\\n        echo \"developer ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/developer && \\\n        chmod 0440 /etc/sudoers.d/developer && \\\n        chown ${uid}:${gid} -R /home/developer\n\n    USER developer\n    ENV HOME /home/developer\n    \n    # get my configfiles from github\n\tRUN mkdir .atom &&  \\ \n    git clone https://github.com/andmos/dotfiles.git && \\ \n    cd dotfiles/atom; ./configureAtom\n\n    CMD /usr/local/bin/atom --foreground --log-file /var/log/atom.log && tail -f /var/log/atom.log\n\n\n### Build and run\n\n    wget https://raw.githubusercontent.com/andmos/Docker-Atom/master/Dockerfile\n    docker build -t atom\n    docker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\n\nAnother cool thing we can do is to attach a [data container](https://docs.docker.com/userguide/dockervolumes/) with the\n``--volumes-from=`` flag. Here is how to build a data container from a git-repo and attach: \n\n    docker run --name coffee -e repo=https://github.com/andmos/coffee.git -t andmos/git\n    docker run --volumes-from:coffee -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\nopen ``/var/workspace/coffee`` and there is the repo! \n\nThis image has lots of possibilities - happy hacking!\n","mobiledoc":null,"html":"<p>By now everybody loves <a href=\"https://www.docker.com/\">Docker</a>. I mean, what is there not to love? Never have it been easier to program, pack and deploy you're applications and getting out of <a href=\"https://www.youtube.com/watch?v=3N3n9FzebAA\">dependency hell</a> while keeping them isolated from each other. <br />\nDocker has solved the whole \"download and install this on the server, but remember to have the right version of Java (no, not that one!) and Tomcat (7, not 8)\" - problem. All servers are happy.</p>\n\n<p>A couple of months ago I read a <a href=\"https://blog.jessfraz.com/post/docker-containers-on-the-desktop/\">blogpost</a> by the talented Docker engineer <a href=\"https://github.com/jfrazelle\">Jessie Frazelle</a> who has made a large collection of Docker images for the <a href=\"https://github.com/jfrazelle/dockerfiles\">desktop</a>. <br />\nThe FREAKING desktop. To <a href=\"https://twitter.com/frazelledazzell/status/596345044912635904\">quote</a> Jessie:</p>\n\n<p><blockquote class=\"twitter-tweet\" lang=\"no\"><p lang=\"en\" dir=\"ltr\">The worst thing someone could do if I ever left my computer unlocked (which will never happen) is install something directly on my host</p>&mdash; jessie frazelle (@frazelledazzell) <a href=\"https://twitter.com/frazelledazzell/status/596345044912635904\">7. mai 2015</a></blockquote> <script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></p>\n\n<p>Feeling inspired by this crazy lady I decided to try it out myself. So here is a Dockerfile for the <a href=\"https://atom.io/\">Atom</a> editor you can run on you're Linux desktop (because, why the hell not?)</p>\n\n<h3 id=\"dockerfile\">Dockerfile</h3>\n\n<pre><code># Based on work by Jessie Frazelle, https://github.com/jfrazelle/dockerfiles/blob/master/atom/Dockerfile\nFROM ubuntu:14.04\nMAINTAINER Andreas Mosti &lt;andreas.mosti@gmail.com&gt;\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    ca-certificates \\\n    curl \\\n    git \\\n    libasound2 \\\n    libgconf-2-4 \\\n    libgnome-keyring-dev \\\n    libgtk2.0-0 \\\n    libnss3 \\\n    libxtst6 \\\n    --no-install-recommends\n\nRUN curl -sL https://deb.nodesource.com/setup | bash -\nRUN apt-get install -y nodejs\n\nRUN git clone https://github.com/atom/atom /src\nWORKDIR /src\nRUN git fetch &amp;&amp; git checkout $(git describe --tags `git rev-list --tags --max-count=1`)\nRUN script/build &amp;&amp; script/grunt install\n\n# set up user and permission\nRUN export uid=1000 gid=1000 &amp;&amp; \\\n    mkdir -p /home/developer &amp;&amp; \\\n    echo \"developer:x:${uid}:${gid}:Developer,,,:/home/developer:/bin/bash\" &gt;&gt; /etc/passwd &amp;&amp; \\\n    echo \"developer:x:${uid}:\" &gt;&gt; /etc/group &amp;&amp; \\\n    echo \"developer ALL=(ALL) NOPASSWD: ALL\" &gt; /etc/sudoers.d/developer &amp;&amp; \\\n    chmod 0440 /etc/sudoers.d/developer &amp;&amp; \\\n    chown ${uid}:${gid} -R /home/developer\n\nUSER developer\nENV HOME /home/developer\n\n# get my configfiles from github\nRUN mkdir .atom &amp;&amp;  \\ \ngit clone https://github.com/andmos/dotfiles.git &amp;&amp; \\ \ncd dotfiles/atom; ./configureAtom\n\nCMD /usr/local/bin/atom --foreground --log-file /var/log/atom.log &amp;&amp; tail -f /var/log/atom.log\n</code></pre>\n\n<h3 id=\"buildandrun\">Build and run</h3>\n\n<pre><code>wget https://raw.githubusercontent.com/andmos/Docker-Atom/master/Dockerfile\ndocker build -t atom\ndocker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\n</code></pre>\n\n<p>Another cool thing we can do is to attach a <a href=\"https://docs.docker.com/userguide/dockervolumes/\">data container</a> with the <br />\n<code>--volumes-from=</code> flag. Here is how to build a data container from a git-repo and attach: </p>\n\n<pre><code>docker run --name coffee -e repo=https://github.com/andmos/coffee.git -t andmos/git\ndocker run --volumes-from:coffee -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY atom\n</code></pre>\n\n<p>open <code>/var/workspace/coffee</code> and there is the repo! </p>\n\n<p>This image has lots of possibilities - happy hacking!</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-23 15:08:24","created_by":1,"updated_at":"2015-07-19 20:21:58","updated_by":1,"published_at":"2015-05-23 15:09:31","published_by":1},{"id":9,"uuid":"c9dc4ef3-ed63-4f2f-a6a4-4ed774e01cf5","title":"On Development Managers","slug":"on-development-managers","markdown":"\nA couple of days ago ex-GitHub employee and brilliant  tech-speaker [Zach Holman](http://zachholman.com/) tweeted the following:\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">There are a lot of ways to become a bad manager, but tapping me on my shoulder when I’m building shit is probably the most heinous.</p>&mdash; Zach Holman (@holman) <a href=\"https://twitter.com/holman/status/620278257506758656\">July 12, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThis statement hits the problem with bad managers spot on. Many companies struggle with this, managers who have to check in on the employees constantly - thus preventing them from being productive. In my opinion, this entire situation show a lack of trust in people and is not only bad for productivity, but also company culture. Why would a developer, a creative human by nature, want to work at a company where his boss has so little trust in him that he constantly checks in on the work he does?\n\nFortunately for me, I have only good experiences with development managers. Here is why I believe the DMs work well at my current company, [DIPS](https://www.dips.no/).\n\n### The DM's are developers themselves\nAll of our DM's have a long history as developers. They are over average interested in software development and tech, thus giving them full understanding of how developers think and how the development process work. Hell, they are one of us. Some of the DM's even rolls up his sleeves and put in some hours of coding if the project allows it. I think this is a great way for the DM's to keep on top of the project and get hands on. [Eliot Horowitz thinks Engineering Managers Should Code 30% of Their Time](http://www.drdobbs.com/architecture-and-design/engineering-managers-should-code-30-of-t/240165174).\n\nIn my opinion, DM's ***should*** have a background as developers. Putting a hot head out of business school in charge might work in the traditional office, but not in a software company.\n\n### The DM's have 100% confidence in their teams\nOn a day to day basis I rarely talk to my DM. Most of my time is spent with my teammates, being creative and building great stuff. Together. I never have my DM hanging over my shoulders asking questions or checking up on me \"to see if I actually do work\". This is because the DM have put together the team consisting of people he or she believes will work good together and get the work done in a brilliant way. The DM have full confidence in the team as a whole. If the developers need something from the DM, they contact him rather than him them. In that way both parties can focus on their work with minimal interruptions.\n\nMy team also work remote, so even if the DM wanted, he could not sneak up behind us to \"check how the work is coming along\". For remote teams trust is even more important.\n### The DM's does hiring\n\nIn the hiring process the DM's interview candidates and put them through the technical interviews. If a DM is unsure of a candidate, the answer is usually no; they only hire people they are 100% sure of. This fits well with the rule of trust. Here we also follow a golden Google rule: [Try to hire people that are smarter than yourself.](http://www.amazon.com/How-Google-Works-Eric-Schmidt/dp/1455582344)\nIt is also important for the DM to hire people who fit well with the rest of the team.\nSome will argue that hiring is the most important job the DM has.  \n","mobiledoc":null,"html":"<p>A couple of days ago ex-GitHub employee and brilliant  tech-speaker <a href=\"http://zachholman.com/\">Zach Holman</a> tweeted the following:</p>\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">There are a lot of ways to become a bad manager, but tapping me on my shoulder when I’m building shit is probably the most heinous.</p>&mdash; Zach Holman (@holman) <a href=\"https://twitter.com/holman/status/620278257506758656\">July 12, 2015</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>This statement hits the problem with bad managers spot on. Many companies struggle with this, managers who have to check in on the employees constantly - thus preventing them from being productive. In my opinion, this entire situation show a lack of trust in people and is not only bad for productivity, but also company culture. Why would a developer, a creative human by nature, want to work at a company where his boss has so little trust in him that he constantly checks in on the work he does?</p>\n\n<p>Fortunately for me, I have only good experiences with development managers. Here is why I believe the DMs work well at my current company, <a href=\"https://www.dips.no/\">DIPS</a>.</p>\n\n<h3 id=\"thedmsaredevelopersthemselves\">The DM's are developers themselves</h3>\n\n<p>All of our DM's have a long history as developers. They are over average interested in software development and tech, thus giving them full understanding of how developers think and how the development process work. Hell, they are one of us. Some of the DM's even rolls up his sleeves and put in some hours of coding if the project allows it. I think this is a great way for the DM's to keep on top of the project and get hands on. <a href=\"http://www.drdobbs.com/architecture-and-design/engineering-managers-should-code-30-of-t/240165174\">Eliot Horowitz thinks Engineering Managers Should Code 30% of Their Time</a>.</p>\n\n<p>In my opinion, DM's <strong><em>should</em></strong> have a background as developers. Putting a hot head out of business school in charge might work in the traditional office, but not in a software company.</p>\n\n<h3 id=\"thedmshave100confidenceintheirteams\">The DM's have 100% confidence in their teams</h3>\n\n<p>On a day to day basis I rarely talk to my DM. Most of my time is spent with my teammates, being creative and building great stuff. Together. I never have my DM hanging over my shoulders asking questions or checking up on me \"to see if I actually do work\". This is because the DM have put together the team consisting of people he or she believes will work good together and get the work done in a brilliant way. The DM have full confidence in the team as a whole. If the developers need something from the DM, they contact him rather than him them. In that way both parties can focus on their work with minimal interruptions.</p>\n\n<p>My team also work remote, so even if the DM wanted, he could not sneak up behind us to \"check how the work is coming along\". For remote teams trust is even more important.  </p>\n\n<h3 id=\"thedmsdoeshiring\">The DM's does hiring</h3>\n\n<p>In the hiring process the DM's interview candidates and put them through the technical interviews. If a DM is unsure of a candidate, the answer is usually no; they only hire people they are 100% sure of. This fits well with the rule of trust. Here we also follow a golden Google rule: <a href=\"http://www.amazon.com/How-Google-Works-Eric-Schmidt/dp/1455582344\">Try to hire people that are smarter than yourself.</a> <br />\nIt is also important for the DM to hire people who fit well with the rest of the team. <br />\nSome will argue that hiring is the most important job the DM has.  </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-07-17 12:49:25","created_by":1,"updated_at":"2015-07-17 12:50:36","updated_by":1,"published_at":"2015-07-17 12:50:36","published_by":1},{"id":10,"uuid":"074efc10-ac4a-442a-b668-f72b6b3ef4ff","title":"Docker Garbage Collection","slug":"docker-garbage-collection","markdown":"\nA mayor issue you might have run into when working on Docker-images is actual disk space. Over time, images and containers can consume a great deal of your precious drive, often when you are developing the actual image and having a test-run after each `ADD` or `RUN` block. The containers then tend to multiply as frequently as rabbits.\n\nTo solve this problem and the lack of Docker \"garbage collection\", the good guys over at [Spotify](https://spotify.com) has created the [docker-gc](https://github.com/spotify/docker-gc) project on GitHub. This project contains a script that removes ALL containers that has been exited over an hour ago, together with their respective images. One hour might sound extreme, but if you think about it, containers that have exited and is not running are mostly (or at least should be!) throw-away containers we have no need for anymore.\n\n### Install and run\nThe script can be run standalone or create a deb-package:\n\n    $ apt-get install git devscripts debhelper\n    $ git clone https://github.com/spotify/docker-gc.git\n    $ cd docker-gc\n    $ debuild -us -uc -b\n\n    # install:\n    $ dpkg -i ../docker-gc_0.0.3_all.deb\n\n    $ sudo docker-gc\n\nThe script is of course best used as a cron.hourly job.\n\nIf you like to run everything in Docker (and hey, why wouldn't you) they have also made a Dockerfile for you:\n\n    docker build -t spotify/docker-gc .\n    docker run --rm -v /var/run/docker.sock:/var/run/docker.sock spotify/docker-gc\n\n### Exclude images\n\nSome images are nice to actually keep on the host (maby for dev or on a production server?)\nIt would have been a real bummer if there were no support for exclusion of images from garbage collection, but thankfully there is:\n\n    touch /etc/docker-gc-exclude\n    echo \"spotify/cassandra:latest\" >> /etc/docker-gc-exclude\n\n\n### Notes\nIf you are running a docker version above 1.6 and experience an error with the date format like so:\n\n      $ sudo docker-gc\n      date: invalid date ‘\"2015-07-25 09:26:5’\nReplace the related lines in the data_parse function with these lines:\n\n    replace_t=\"${without_ms/T/ }\"\n    replace_quote=\"${replace_t#\\\"}\"\n    epoch=$(date_parse \"${replace_quote}\")\nAs of 03.08.2015 this fix has not been merged to master but I guess that will happen at some point soon.\n","mobiledoc":null,"html":"<p>A mayor issue you might have run into when working on Docker-images is actual disk space. Over time, images and containers can consume a great deal of your precious drive, often when you are developing the actual image and having a test-run after each <code>ADD</code> or <code>RUN</code> block. The containers then tend to multiply as frequently as rabbits.</p>\n\n<p>To solve this problem and the lack of Docker \"garbage collection\", the good guys over at <a href=\"https://spotify.com\">Spotify</a> has created the <a href=\"https://github.com/spotify/docker-gc\">docker-gc</a> project on GitHub. This project contains a script that removes ALL containers that has been exited over an hour ago, together with their respective images. One hour might sound extreme, but if you think about it, containers that have exited and is not running are mostly (or at least should be!) throw-away containers we have no need for anymore.</p>\n\n<h3 id=\"installandrun\">Install and run</h3>\n\n<p>The script can be run standalone or create a deb-package:</p>\n\n<pre><code>$ apt-get install git devscripts debhelper\n$ git clone https://github.com/spotify/docker-gc.git\n$ cd docker-gc\n$ debuild -us -uc -b\n\n# install:\n$ dpkg -i ../docker-gc_0.0.3_all.deb\n\n$ sudo docker-gc\n</code></pre>\n\n<p>The script is of course best used as a cron.hourly job.</p>\n\n<p>If you like to run everything in Docker (and hey, why wouldn't you) they have also made a Dockerfile for you:</p>\n\n<pre><code>docker build -t spotify/docker-gc .\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock spotify/docker-gc\n</code></pre>\n\n<h3 id=\"excludeimages\">Exclude images</h3>\n\n<p>Some images are nice to actually keep on the host (maby for dev or on a production server?) <br />\nIt would have been a real bummer if there were no support for exclusion of images from garbage collection, but thankfully there is:</p>\n\n<pre><code>touch /etc/docker-gc-exclude\necho \"spotify/cassandra:latest\" &gt;&gt; /etc/docker-gc-exclude\n</code></pre>\n\n<h3 id=\"notes\">Notes</h3>\n\n<p>If you are running a docker version above 1.6 and experience an error with the date format like so:</p>\n\n<pre><code>  $ sudo docker-gc\n  date: invalid date ‘\"2015-07-25 09:26:5’\n</code></pre>\n\n<p>Replace the related lines in the data_parse function with these lines:</p>\n\n<pre><code>replace_t=\"${without_ms/T/ }\"\nreplace_quote=\"${replace_t#\\\"}\"\nepoch=$(date_parse \"${replace_quote}\")\n</code></pre>\n\n<p>As of 03.08.2015 this fix has not been merged to master but I guess that will happen at some point soon.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-08-03 19:00:11","created_by":1,"updated_at":"2015-08-03 19:00:45","updated_by":1,"published_at":"2015-08-03 19:00:45","published_by":1},{"id":11,"uuid":"9f7b0043-5aae-422c-9ab0-bf29deafcdc4","title":"Managing NuGet dependencies with Paket","slug":"managing-nuget-dependencies-with-paket","markdown":"Since we mainly write .NET code at work, the only real choice for packing dependencies and applications is [NuGet](https://www.nuget.org/) and [Chocolatey](https://chocolatey.org/), everything stored in different feeds on the excellent [ProGet](http://inedo.com/proget/overview) server.  For CI we build our code with [TFS](https://msdn.microsoft.com/en-us/vstudio/ff637362.aspx) and [Team City](https://www.jetbrains.com/teamcity/) using some hand-made build scripts written in [scriptcs](http://scriptcs.net/), while deployment is done with good old [Octopus Deploy](https://octopusdeploy.com/).\n\nAll this works perfectly in our in-house environments, everything is just a `choco install`  away.\nThe only catch her is that we want to ship these packages to our customers as standalone installations. To do this we have  written some custom powershell-scripts that wraps chocolatey and points to a local packages-folder where the correct .nupkg-files for the applications are stored. Since we want to use this exact installation mechanism in our CI pipeline (aka eating your own dog food) we had to be creative.\n\nChocolatey is great for installing software, but the easiest way to actually grab the .nupkg-files without installing them (and get the version number as a part of the .nupkg filename, chocolatey does not include this for some reason) is to use the NuGet CLI itself:\n\n      nuget install mypackage-server -Source http://myproget -OutputDirectory \"C:\\SetupScript\\Packages\"\n\nHere we ran in to a much debated issue: NuGet's way of managing dependencies. By default NuGet resolves the **lowest** available (and legal) version of all dependencies. I understand that this is a safe choice, but for CI we always want to test and deploy the latest version of all packages and its dependencies. The worst part is, after hours of googling and trying we found no way to force NuGet to use the highest available versions of dependencies. To solve this issue I turned to [Paket](http://fsprojects.github.io/Paket/).\n\nPaket is a wonderful project that takes care of dependencies in your project for you in a much more elegant (and Ruby-like) way then what NuGet itself does. Here is an example:\n\n    $ mkdir .paket\n    $ wget https://github.com/fsprojects/Paket/releases/download/1.23.0/paket.exe -p .paket/\n    $ touch paket.dependencies\n\nIn the `paket.dependencies` file, put in your dependencies like this:\n\n    source https://nuget.org/api/v2\n\n    nuget Castle.Windsor-log4net >= 3.2\n    nuget NUnit\n\n    github forki/FsUnit FsUnit.fs\n\nTo install dependencies:\n\n    $ .paket/paket.exe install\n\nAll dependencies (including correct, transitive dependencies in **latest** versions!) are stored in the `paket.lock` file:\n\n    NUGET\n      remote: https://nuget.org/api/v2\n      specs:\n        Castle.Core (3.3.3)\n        Castle.Core-log4net (3.3.3)\n          Castle.Core (>= 3.3.3)\n          log4net (1.2.10)\n        Castle.LoggingFacility (3.3.0)\n          Castle.Core (>= 3.3.0)\n          Castle.Windsor (>= 3.3.0)\n        Castle.Windsor (3.3.0)\n          Castle.Core (>= 3.3.0)\n        Castle.Windsor-log4net (3.3.0)\n          Castle.Core-log4net (>= 3.3.0)\n          Castle.LoggingFacility (>= 3.3.0)\n        log4net (1.2.10)\n        NUnit (2.6.4)\n    GITHUB\n      remote: forki/FsUnit\n      specs:\n        FsUnit.fs (81d27fd09575a32c4ed52eadb2eeac5f365b8348)\n\nThe files end up in a folder called `Packages` with .npkg files and all, just how we like it.\n\nAs a bonus, here is the powershell-script Octopus Deploys runs:\n\n    $SetupRoot = \"C:\\SetupScript\"\n    $SetupPs1 = \"SetupRoot\\Setup.ps1\"\n    $OFS = \"`r`n\"\n\n    Write-Host \"Clearing old cache...\"\n    Remove-Item SetupRoot\\packages\\* -Recurse -Force -ExcludeSetup.psm1,Setup\n\n    if (-not $packageName) {\n        throw \"Please specify the name of a package to install.\"\n    }\n\n    if($version){\n        $versionString = \"-Version $version\"\n    }\n\n    Write-Host \"Fetching packages...\"\n\n    cd $SetupRoot\n\n    Set-Content -Value \"source $sourceFeed $OFS\" -Path $SetupRoot\\paket.dependencies\n    Add-Content -Value \"nuget $packageName\" -Path $SetupRoot\\paket.dependencies\n\n\n    if(Test-Path $SetupRoot\\paket.lock){\n        Write-Host \"Removing paket.lock file\"\n        Remove-Item $DIPSSetupRoot\\paket.lock\n    }\n\n    & .paket/paket.exe install\n\n    Write-Host \"Discovering chocolatey packages...\"\n    Copy-Item $SetupRoot\\packages\\*\\*.nupkg $SetupRoot\\packages\n\n    Write-Host \"Installing chocolatey package...\"\n    if ($myval -eq $null) { \"new value\" } else { $myval }\n\n    if (-not $version){\n        $version = [String]::Join(\".\",(Get-ChildItem $SetupRoot\\packages\\$packageName*.nupkg)[0].Name.Split('.'),1,4)\n    }\n\n    $Config = Get-Item \"$SetupRoot\\Packages.config\"\n    $ConfigContent = [xml]@\"\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    <packages>\n      <package id=\"$packageName\" version=\"$version\" />\n    </packages>\n    \"@\n    $ConfigContent.Save($Config)\n\n    & $SetupRoot\\Setup.ps1\n","mobiledoc":null,"html":"<p>Since we mainly write .NET code at work, the only real choice for packing dependencies and applications is <a href=\"https://www.nuget.org/\">NuGet</a> and <a href=\"https://chocolatey.org/\">Chocolatey</a>, everything stored in different feeds on the excellent <a href=\"http://inedo.com/proget/overview\">ProGet</a> server.  For CI we build our code with <a href=\"https://msdn.microsoft.com/en-us/vstudio/ff637362.aspx\">TFS</a> and <a href=\"https://www.jetbrains.com/teamcity/\">Team City</a> using some hand-made build scripts written in <a href=\"http://scriptcs.net/\">scriptcs</a>, while deployment is done with good old <a href=\"https://octopusdeploy.com/\">Octopus Deploy</a>.</p>\n\n<p>All this works perfectly in our in-house environments, everything is just a <code>choco install</code>  away. <br />\nThe only catch her is that we want to ship these packages to our customers as standalone installations. To do this we have  written some custom powershell-scripts that wraps chocolatey and points to a local packages-folder where the correct .nupkg-files for the applications are stored. Since we want to use this exact installation mechanism in our CI pipeline (aka eating your own dog food) we had to be creative.</p>\n\n<p>Chocolatey is great for installing software, but the easiest way to actually grab the .nupkg-files without installing them (and get the version number as a part of the .nupkg filename, chocolatey does not include this for some reason) is to use the NuGet CLI itself:</p>\n\n<pre><code>  nuget install mypackage-server -Source http://myproget -OutputDirectory \"C:\\SetupScript\\Packages\"\n</code></pre>\n\n<p>Here we ran in to a much debated issue: NuGet's way of managing dependencies. By default NuGet resolves the <strong>lowest</strong> available (and legal) version of all dependencies. I understand that this is a safe choice, but for CI we always want to test and deploy the latest version of all packages and its dependencies. The worst part is, after hours of googling and trying we found no way to force NuGet to use the highest available versions of dependencies. To solve this issue I turned to <a href=\"http://fsprojects.github.io/Paket/\">Paket</a>.</p>\n\n<p>Paket is a wonderful project that takes care of dependencies in your project for you in a much more elegant (and Ruby-like) way then what NuGet itself does. Here is an example:</p>\n\n<pre><code>$ mkdir .paket\n$ wget https://github.com/fsprojects/Paket/releases/download/1.23.0/paket.exe -p .paket/\n$ touch paket.dependencies\n</code></pre>\n\n<p>In the <code>paket.dependencies</code> file, put in your dependencies like this:</p>\n\n<pre><code>source https://nuget.org/api/v2\n\nnuget Castle.Windsor-log4net &gt;= 3.2\nnuget NUnit\n\ngithub forki/FsUnit FsUnit.fs\n</code></pre>\n\n<p>To install dependencies:</p>\n\n<pre><code>$ .paket/paket.exe install\n</code></pre>\n\n<p>All dependencies (including correct, transitive dependencies in <strong>latest</strong> versions!) are stored in the <code>paket.lock</code> file:</p>\n\n<pre><code>NUGET\n  remote: https://nuget.org/api/v2\n  specs:\n    Castle.Core (3.3.3)\n    Castle.Core-log4net (3.3.3)\n      Castle.Core (&gt;= 3.3.3)\n      log4net (1.2.10)\n    Castle.LoggingFacility (3.3.0)\n      Castle.Core (&gt;= 3.3.0)\n      Castle.Windsor (&gt;= 3.3.0)\n    Castle.Windsor (3.3.0)\n      Castle.Core (&gt;= 3.3.0)\n    Castle.Windsor-log4net (3.3.0)\n      Castle.Core-log4net (&gt;= 3.3.0)\n      Castle.LoggingFacility (&gt;= 3.3.0)\n    log4net (1.2.10)\n    NUnit (2.6.4)\nGITHUB\n  remote: forki/FsUnit\n  specs:\n    FsUnit.fs (81d27fd09575a32c4ed52eadb2eeac5f365b8348)\n</code></pre>\n\n<p>The files end up in a folder called <code>Packages</code> with .npkg files and all, just how we like it.</p>\n\n<p>As a bonus, here is the powershell-script Octopus Deploys runs:</p>\n\n<pre><code>$SetupRoot = \"C:\\SetupScript\"\n$SetupPs1 = \"SetupRoot\\Setup.ps1\"\n$OFS = \"`r`n\"\n\nWrite-Host \"Clearing old cache...\"\nRemove-Item SetupRoot\\packages\\* -Recurse -Force -ExcludeSetup.psm1,Setup\n\nif (-not $packageName) {\n    throw \"Please specify the name of a package to install.\"\n}\n\nif($version){\n    $versionString = \"-Version $version\"\n}\n\nWrite-Host \"Fetching packages...\"\n\ncd $SetupRoot\n\nSet-Content -Value \"source $sourceFeed $OFS\" -Path $SetupRoot\\paket.dependencies\nAdd-Content -Value \"nuget $packageName\" -Path $SetupRoot\\paket.dependencies\n\n\nif(Test-Path $SetupRoot\\paket.lock){\n    Write-Host \"Removing paket.lock file\"\n    Remove-Item $DIPSSetupRoot\\paket.lock\n}\n\n&amp; .paket/paket.exe install\n\nWrite-Host \"Discovering chocolatey packages...\"\nCopy-Item $SetupRoot\\packages\\*\\*.nupkg $SetupRoot\\packages\n\nWrite-Host \"Installing chocolatey package...\"\nif ($myval -eq $null) { \"new value\" } else { $myval }\n\nif (-not $version){\n    $version = [String]::Join(\".\",(Get-ChildItem $SetupRoot\\packages\\$packageName*.nupkg)[0].Name.Split('.'),1,4)\n}\n\n$Config = Get-Item \"$SetupRoot\\Packages.config\"\n$ConfigContent = [xml]@\"\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;packages&gt;\n  &lt;package id=\"$packageName\" version=\"$version\" /&gt;\n&lt;/packages&gt;\n\"@\n$ConfigContent.Save($Config)\n\n&amp; $SetupRoot\\Setup.ps1\n</code></pre>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-08-07 16:53:47","created_by":1,"updated_at":"2015-08-07 16:55:05","updated_by":1,"published_at":"2015-08-07 16:54:46","published_by":1},{"id":12,"uuid":"82db10bc-c2ce-4874-963d-f35f8dfed994","title":"Build and publish AsciiDoc with Docker and nginx","slug":"build-and-publish-asciidoc-with-docker-and-nginx","markdown":"One of the teams at work is piloting the usage of [AsciiDoc](http://www.methods.co.nz/asciidoc/) for documentation of their product. AsciiDoc is a markup-format just like Gruber's [Markdown](http://daringfireball.net/projects/markdown/), but is more advanced and offers more possibilities. Since the documentation is located alongside the source code in Git (as it should be!) I created a simple build step for easy build and deploy of the documentation with my favorite tool, [Docker](https://www.docker.com/) with public images directly from the [Docker Hub](https://hub.docker.com/). \n\nThese steps gets triggered on each build:\n\n    if [[ -z \"$(sudo docker ps | grep webserver)\" ]]; then\n        sudo docker run -dt --name webserver -p 80:80 -v /usr/share/nginx/html nginx\n    fi\n\n    sudo docker run -it -v /var/buildDropLocation/build/docs:/documents/ --volumes-from webserver asciidoctor/docker-asciidoctor asciidoctor -a stylesheet=dips.css -a toc-left technical_document.adoc -D /usr/share/nginx/html\n\nIn short, we check if the [nginx](http://nginx.org/)-container is running and starts it if needed. The HTML-folder is exposed.\nNext we pull down and run the [asciidoctor-image](https://github.com/asciidoctor/docker-asciidoctor) from the Docker Hub and link in the docs-folder from the build. We use [TeamCity](https://www.jetbrains.com/teamcity/), so customize these variables to fit your build system. asciidoctor then runs on the files and puts the output HTML in the volume from the nginx-container. Easy as that, live documentation directly from the latest build. ","mobiledoc":null,"html":"<p>One of the teams at work is piloting the usage of <a href=\"http://www.methods.co.nz/asciidoc/\">AsciiDoc</a> for documentation of their product. AsciiDoc is a markup-format just like Gruber's <a href=\"http://daringfireball.net/projects/markdown/\">Markdown</a>, but is more advanced and offers more possibilities. Since the documentation is located alongside the source code in Git (as it should be!) I created a simple build step for easy build and deploy of the documentation with my favorite tool, <a href=\"https://www.docker.com/\">Docker</a> with public images directly from the <a href=\"https://hub.docker.com/\">Docker Hub</a>. </p>\n\n<p>These steps gets triggered on each build:</p>\n\n<pre><code>if [[ -z \"$(sudo docker ps | grep webserver)\" ]]; then\n    sudo docker run -dt --name webserver -p 80:80 -v /usr/share/nginx/html nginx\nfi\n\nsudo docker run -it -v /var/buildDropLocation/build/docs:/documents/ --volumes-from webserver asciidoctor/docker-asciidoctor asciidoctor -a stylesheet=dips.css -a toc-left technical_document.adoc -D /usr/share/nginx/html\n</code></pre>\n\n<p>In short, we check if the <a href=\"http://nginx.org/\">nginx</a>-container is running and starts it if needed. The HTML-folder is exposed. <br />\nNext we pull down and run the <a href=\"https://github.com/asciidoctor/docker-asciidoctor\">asciidoctor-image</a> from the Docker Hub and link in the docs-folder from the build. We use <a href=\"https://www.jetbrains.com/teamcity/\">TeamCity</a>, so customize these variables to fit your build system. asciidoctor then runs on the files and puts the output HTML in the volume from the nginx-container. Easy as that, live documentation directly from the latest build. </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-09-03 19:02:18","created_by":1,"updated_at":"2015-09-03 19:04:43","updated_by":1,"published_at":"2015-09-03 19:02:39","published_by":1},{"id":13,"uuid":"2db8bce6-fc22-462d-8c39-fcc8f665df93","title":"Fun with Travis CI","slug":"fun-with-travis-ci","markdown":"Ok, I need to swallow my own words on this one. Yesterday I stated the following on twitter:\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">I was thinking of writing a blogpost about building with <a href=\"https://twitter.com/travisci\">@travisci</a>, but it is so easy to use that I don&#39;t see the point. <a href=\"https://twitter.com/hashtag/greatProject?src=hash\">#greatProject</a> <a href=\"https://twitter.com/hashtag/CI?src=hash\">#CI</a></p>&mdash; Andreas Mosti (@amostii) <a href=\"https://twitter.com/amostii/status/656205920922390528\">October 19, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nSo, yeah.\n\nRecently I have been playing a lot with [Travis CI](https://travis-ci.com/) for building my open source projects directly from [GitHub](https://github.com/). All you need to get it working is to add a simple ``.travis.yml`` file to the root folder of the project you want to build, where the steps required to build the project is described. Typically it will look something like this:\n\n    language: csharp\n    mono:\n      - latest\n      - 3.12.0\n      - 3.10.0\n    solution: solution-name.sln\n    install:\n      - nuget restore solution-name.sln\n      - nuget install NUnit.Runners -Version 2.6.4 -OutputDirectory testrunner\n    script:\n      - xbuild /p:Configuration=Release solution-name.sln\n      - mono ./testrunner/NUnit.Runners.2.6.4/tools/nunit-console.exe ./MyPoject.Tests/bin/Release/MyProject.Tests.dll\n\nAs my tweet stated, this is really easy to understand just from looking at the file. We select a language, pick out some runtime versions to build on, select a solution, grab some dependencies, run the build and run some tests. commit some code and the build starts. Simple and powerful, as it should be.\n\nPersonally, I prefer to add a `build.sh` file to all my project, which makes it all even easier:  \n\n\n    language: csharp\n    mono:\n      - latest\n      - 3.12.0\n      - 3.10.0\n    before_script:\n        - chmod +x build.sh\n    script:\n      - ./build.sh compile\n      - ./build.sh test\nNormally Travis CI uses [container technology to spawn these builds fast](http://docs.travis-ci.com/user/migrating-from-legacy/#How-can-I-use-container-based-infrastructure%3F), but there is nothing wrong with using containers for yourself in the build:\n\n    language: csharp\n    sudo: required\n    services:\n      - docker\n    before_script:\n      - chmod +x buildServer\n      - ./build.sh build-local\n      - ./build.sh build-docker\n      - ./build.sh run-docker\n    script:\n      - ./build.sh unit-local\n      - ./build.sh integration-local\n\nNow this solution I like a lot. Here we allow the use of [Docker](https://www.docker.com/) thanks to the `sudo: required` and the `-docker` service. Next up we run a regular compile-build both directly on the host and in a Docker-container, before starting this container. The container allows us to run integration-tests against newly built code fast, without the need to deploy it on another server, making the build more efficient and keeps complexity low. The build test the entire solution from compile to deploy, all in a few lines of code.  ","mobiledoc":null,"html":"<p>Ok, I need to swallow my own words on this one. Yesterday I stated the following on twitter:</p>\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">I was thinking of writing a blogpost about building with <a href=\"https://twitter.com/travisci\">@travisci</a>, but it is so easy to use that I don&#39;t see the point. <a href=\"https://twitter.com/hashtag/greatProject?src=hash\">#greatProject</a> <a href=\"https://twitter.com/hashtag/CI?src=hash\">#CI</a></p>&mdash; Andreas Mosti (@amostii) <a href=\"https://twitter.com/amostii/status/656205920922390528\">October 19, 2015</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>So, yeah.</p>\n\n<p>Recently I have been playing a lot with <a href=\"https://travis-ci.com/\">Travis CI</a> for building my open source projects directly from <a href=\"https://github.com/\">GitHub</a>. All you need to get it working is to add a simple <code>.travis.yml</code> file to the root folder of the project you want to build, where the steps required to build the project is described. Typically it will look something like this:</p>\n\n<pre><code>language: csharp\nmono:\n  - latest\n  - 3.12.0\n  - 3.10.0\nsolution: solution-name.sln\ninstall:\n  - nuget restore solution-name.sln\n  - nuget install NUnit.Runners -Version 2.6.4 -OutputDirectory testrunner\nscript:\n  - xbuild /p:Configuration=Release solution-name.sln\n  - mono ./testrunner/NUnit.Runners.2.6.4/tools/nunit-console.exe ./MyPoject.Tests/bin/Release/MyProject.Tests.dll\n</code></pre>\n\n<p>As my tweet stated, this is really easy to understand just from looking at the file. We select a language, pick out some runtime versions to build on, select a solution, grab some dependencies, run the build and run some tests. commit some code and the build starts. Simple and powerful, as it should be.</p>\n\n<p>Personally, I prefer to add a <code>build.sh</code> file to all my project, which makes it all even easier:  </p>\n\n<pre><code>language: csharp\nmono:\n  - latest\n  - 3.12.0\n  - 3.10.0\nbefore_script:\n    - chmod +x build.sh\nscript:\n  - ./build.sh compile\n  - ./build.sh test\n</code></pre>\n\n<p>Normally Travis CI uses <a href=\"http://docs.travis-ci.com/user/migrating-from-legacy/#How-can-I-use-container-based-infrastructure%3F\">container technology to spawn these builds fast</a>, but there is nothing wrong with using containers for yourself in the build:</p>\n\n<pre><code>language: csharp\nsudo: required\nservices:\n  - docker\nbefore_script:\n  - chmod +x buildServer\n  - ./build.sh build-local\n  - ./build.sh build-docker\n  - ./build.sh run-docker\nscript:\n  - ./build.sh unit-local\n  - ./build.sh integration-local\n</code></pre>\n\n<p>Now this solution I like a lot. Here we allow the use of <a href=\"https://www.docker.com/\">Docker</a> thanks to the <code>sudo: required</code> and the <code>-docker</code> service. Next up we run a regular compile-build both directly on the host and in a Docker-container, before starting this container. The container allows us to run integration-tests against newly built code fast, without the need to deploy it on another server, making the build more efficient and keeps complexity low. The build test the entire solution from compile to deploy, all in a few lines of code.  </p>","amp":null,"image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-10-20 20:22:10","created_by":1,"updated_at":"2015-10-20 20:27:27","updated_by":1,"published_at":"2015-10-20 20:23:32","published_by":1},{"id":14,"uuid":"35ef7d68-8c7e-4b3b-9c2e-6d7504274554","title":"Trondheim Developer Conference 2015","slug":"trondheim-developer-conference","markdown":"\nThis year I had the pleasure of being one of the talkers at the fourth annual [TDC](http://2015.trondheimdc.no/) in Trondhiem.\nThe lineup included big international tech-names like [Scott Hanselman](http://www.hanselman.com/), [Sahil Malik](http://blah.winsmarts.com/), [Seb Lee-Delisle](http://seb.ly/) and [Scott Allen](http://odetocode.com/about/scott-allen), and I had a lot of fun making my debut as a conference speaker along side these people.\n\nThe talk i brought was **Simple crossplatform REST-Service with .NET, Vagrant and Docker**, a walkthrough on how to make crossplatform server components in .NET with [Vagrant](https://www.vagrantup.com/) and [Docker](https://www.docker.com/) as key tools, helping us focus on integration testing and production-like deployment from the first lines of code written. This is also a subject I have [blogged](http://blog.amosti.net/build-test-and-deploy-net-apps-with-vagrant-and-docker/) some about before. The essence of the talk is that .NET and C# now is all you need to know to write the entire stack of your applications, including the mobile client code for iOS and Android via [Xamarin](https://xamarin.com/) to the server side part with [Mono](http://www.mono-project.com/), letting you choose what platform to run on.\n\n![](http://i.imgur.com/bJeyynv.jpg)\n\nI gave some tips on frameworks and libraries to use when writing a simple REST-Service, including [NancyFX](http://nancyfx.org/), [Topshelf](http://topshelf-project.com/) and [Dapper](https://github.com/StackExchange/dapper-dot-net).\n\nMy session will be released by the TDC people shortly, until then - here are the slides.\n\n<script async class=\"speakerdeck-embed\" data-id=\"3191aeafb0bf493b8be90abe01639bce\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nEDIT: Here is also the talk (in norwegian): \n\n<iframe src=\"https://player.vimeo.com/video/144964559\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n\nAfter the talk I walked back stage and helped Scott Hanselman out with his upcoming [ASP.NET 5](http://www.asp.net/vnext) talk, where he showed me the future of open source Microsoft, running on all platforms without anything installed thanks to the [CoreCLR](https://github.com/dotnet/coreclr). He told me he planned on asking some random people in the audience for a Mac to show how easy it was to run a sample ASP.NET 5 app with the CoreCLR, so I placed my co-worker [@hjerpbakk](http://hjerpbakk.com/) on the front raw. Sure enough, he had to take the stage with his Mac.\n\n![](http://i.imgur.com/6Ba2BF7.jpg)\n\nThe tricky bit proved to not be getting asp.net to run on a Mac, but tackling the Norwegian keyboard. Hanselman joked as always, and his sessions ended up, not surprisingly, to be some of the best the entire conference had to offer.   ","mobiledoc":null,"html":"<p>This year I had the pleasure of being one of the talkers at the fourth annual <a href=\"http://2015.trondheimdc.no/\">TDC</a> in Trondhiem. <br />\nThe lineup included big international tech-names like <a href=\"http://www.hanselman.com/\">Scott Hanselman</a>, <a href=\"http://blah.winsmarts.com/\">Sahil Malik</a>, <a href=\"http://seb.ly/\">Seb Lee-Delisle</a> and <a href=\"http://odetocode.com/about/scott-allen\">Scott Allen</a>, and I had a lot of fun making my debut as a conference speaker along side these people.</p>\n\n<p>The talk i brought was <strong>Simple crossplatform REST-Service with .NET, Vagrant and Docker</strong>, a walkthrough on how to make crossplatform server components in .NET with <a href=\"https://www.vagrantup.com/\">Vagrant</a> and <a href=\"https://www.docker.com/\">Docker</a> as key tools, helping us focus on integration testing and production-like deployment from the first lines of code written. This is also a subject I have <a href=\"http://blog.amosti.net/build-test-and-deploy-net-apps-with-vagrant-and-docker/\">blogged</a> some about before. The essence of the talk is that .NET and C# now is all you need to know to write the entire stack of your applications, including the mobile client code for iOS and Android via <a href=\"https://xamarin.com/\">Xamarin</a> to the server side part with <a href=\"http://www.mono-project.com/\">Mono</a>, letting you choose what platform to run on.</p>\n\n<p><img src=\"http://i.imgur.com/bJeyynv.jpg\" alt=\"\" /></p>\n\n<p>I gave some tips on frameworks and libraries to use when writing a simple REST-Service, including <a href=\"http://nancyfx.org/\">NancyFX</a>, <a href=\"http://topshelf-project.com/\">Topshelf</a> and <a href=\"https://github.com/StackExchange/dapper-dot-net\">Dapper</a>.</p>\n\n<p>My session will be released by the TDC people shortly, until then - here are the slides.</p>\n\n<script async class=\"speakerdeck-embed\" data-id=\"3191aeafb0bf493b8be90abe01639bce\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n<p>EDIT: Here is also the talk (in norwegian): </p>\n\n<iframe src=\"https://player.vimeo.com/video/144964559\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n<p>After the talk I walked back stage and helped Scott Hanselman out with his upcoming <a href=\"http://www.asp.net/vnext\">ASP.NET 5</a> talk, where he showed me the future of open source Microsoft, running on all platforms without anything installed thanks to the <a href=\"https://github.com/dotnet/coreclr\">CoreCLR</a>. He told me he planned on asking some random people in the audience for a Mac to show how easy it was to run a sample ASP.NET 5 app with the CoreCLR, so I placed my co-worker <a href=\"http://hjerpbakk.com/\">@hjerpbakk</a> on the front raw. Sure enough, he had to take the stage with his Mac.</p>\n\n<p><img src=\"http://i.imgur.com/6Ba2BF7.jpg\" alt=\"\" /></p>\n\n<p>The tricky bit proved to not be getting asp.net to run on a Mac, but tackling the Norwegian keyboard. Hanselman joked as always, and his sessions ended up, not surprisingly, to be some of the best the entire conference had to offer.   </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-10-31 16:00:50","created_by":1,"updated_at":"2015-11-24 14:12:09","updated_by":1,"published_at":"2015-10-31 16:01:59","published_by":1},{"id":15,"uuid":"fb35d433-6955-4fdf-9e1a-a92b161f374f","title":"Dumbing down the build server","slug":"dumbing-down-the-build-server","markdown":"### Motivation\nA common mistake I see many do (yes, i call it *mistake* ) is a build system too tightly linked with the build server they end up choosing. We made that mistake ourself. Being a typical .NET shop, we started out with [Microsoft Team Foundation server](https://www.visualstudio.com/en-us/products/tfs-overview-vs.aspx) back in 2007. Building with TFS in those days relied heavily on workflow scripts and build templates that had to be compiled and could *only* be run on the build server. The result? We could not build our code in exactly the same way locally as we did on the build server. As long as the codebase is small this might be OK, but when you start getting integration tests deep down failing on the server and not locally you are in trouble. Another clear problem we found with the TFS approach was how people treated the build server: like a black box. Of all the developers working in the company, only 4 knew how the build system was working. The whole thing got far too complex and switching over to something like [Team City](https://www.jetbrains.com/teamcity/) would force us to make the entire build system from scratch. We tried this too - and without thinking about it ended up using the [Team City \nenvironment variabes](https://confluence.jetbrains.com/display/TCD9/Predefined+Build+Parameters) far too much. The result? The new build system also got tied strongly against Team City. If you ever find yourself in some of these situation, press the big red button and think.\n\n### What is a build server? \nA build server is nothing more than a *worker*. You send it tasks to do, and it does it. Simple as that. Hell, I don't like the words **build server** or **CI server**, it makes it sound more complex than it really is. The important part is not the build server but the *build system*. With a good build system the server itself can be dumbed down to just running i cron job, nothing more. The key point here is to not use all the fancy configuration options the build server has - that should be a part of the build system itself. With this in mind - let's do it right.\n\n### Do it right \nWhen we started out working with the new build system, we had some clear rules to follow: \n\n* The system must do *exactly*  the same things locally and on the server.\n* The system should be made in an easy to understand scripting language.\n* Each team owns its own build, but common code should be shared.\n* Use as much [Vanilla](https://en.wikipedia.org/wiki/Vanilla_software) methods as possible.\n* The system should be simple without too much lock-in to the build server we choose to use. \n\nBuilding for .NET has become much better the recent years. Where we before called MSBuild directly from PowerShell we now have [psake](https://github.com/psake/psake) inspired by Ruby's  [rake](https://github.com/ruby/rake). We have some mixed experience with Powershell so we early turned this down. If you don't mind Powershell, psake seems to be a good choice.\n\nNext up we looked at [FAKE](https://fsharp.github.io/FAKE/), a popular buildsystem written in F#. NRK has [written about switching to FAKE](https://nrkbeta.no/2015/11/10/how-i-learned-to-stop-worrying-and-love-the-ci-server/). Some teams ended up using FAKE on some independent modules, but we ended up not using it because of the learning curve F# would bring to most of our developers. \n\nThanks to Microsofts [Roslyn compiler](https://roslyn.codeplex.com/) we discovered [ScriptCS](http://scriptcs.net/). ScriptCS let's you use C# as a scripting language, offering REPL and everything else you would expect from a scripting language. Since the language is C# every developer working can read and modify the scripts, no learning needed. Perfect. ScriptCS can also reference DLLs from GAC or a binary folder. \n\nWe ended up creating a `tools\\` folder with a `build.csx` and a `common.csx` file. The `common.csx` file is centrally shared in source control and acts like our library with all methods needed for building tasks, like MSBuild, file management and calls to the NuGet API and triggers for [Octopus Deploy](https://octopus.com/). The `build.csx`  file is the build file itself specified for the solution(s) to build, using methods from `common.csx`. The build system itself is distributed as a NuGet package to be installed installed to the root of the project we want to build. A `build.bat` file wrapps the build system and can be triggered like so:\n\t\t\n\t./build.bat clean init compile unit integration package deploy\n\nThis flow cleans, pulls down dependencies, compiles, runs unit- and integrationtests, packages NuGet or Chocolatey packages and deploys to ProGet and Octopus Deploy. These steps are exactly alike locally and on the build server, just a command line call to build.bat. If ScriptCS is not installed on the system, the bat-file will do it for you. \n\n### Conclusion\n\nIt took some time, but after a while we had a build system we were very satisfied with. The shared code is constantly changing as teams finds new needs. ScriptCS has been a success and have removed a lot of the magic from the build process. The teams have taken a lot more ownership over building than they had before. By pushing logic from the build server and down to the build system itself, everything works perfectly with Team City, TFS2015 and Jenkins, just as it should be. \n\n### Note\n\nIf you don't want to write a ScriptCS-based build system from scratch, take a look at Cake. Cake is pretty much a ScriptCS based framework for building with lots of extension possibilities.","mobiledoc":null,"html":"<h3 id=\"motivation\">Motivation</h3>\n\n<p>A common mistake I see many do (yes, i call it <em>mistake</em> ) is a build system too tightly linked with the build server they end up choosing. We made that mistake ourself. Being a typical .NET shop, we started out with <a href=\"https://www.visualstudio.com/en-us/products/tfs-overview-vs.aspx\">Microsoft Team Foundation server</a> back in 2007. Building with TFS in those days relied heavily on workflow scripts and build templates that had to be compiled and could <em>only</em> be run on the build server. The result? We could not build our code in exactly the same way locally as we did on the build server. As long as the codebase is small this might be OK, but when you start getting integration tests deep down failing on the server and not locally you are in trouble. Another clear problem we found with the TFS approach was how people treated the build server: like a black box. Of all the developers working in the company, only 4 knew how the build system was working. The whole thing got far too complex and switching over to something like <a href=\"https://www.jetbrains.com/teamcity/\">Team City</a> would force us to make the entire build system from scratch. We tried this too - and without thinking about it ended up using the <a href=\"https://confluence.jetbrains.com/display/TCD9/Predefined+Build+Parameters\">Team City <br />\nenvironment variabes</a> far too much. The result? The new build system also got tied strongly against Team City. If you ever find yourself in some of these situation, press the big red button and think.</p>\n\n<h3 id=\"whatisabuildserver\">What is a build server?</h3>\n\n<p>A build server is nothing more than a <em>worker</em>. You send it tasks to do, and it does it. Simple as that. Hell, I don't like the words <strong>build server</strong> or <strong>CI server</strong>, it makes it sound more complex than it really is. The important part is not the build server but the <em>build system</em>. With a good build system the server itself can be dumbed down to just running i cron job, nothing more. The key point here is to not use all the fancy configuration options the build server has - that should be a part of the build system itself. With this in mind - let's do it right.</p>\n\n<h3 id=\"doitright\">Do it right</h3>\n\n<p>When we started out working with the new build system, we had some clear rules to follow: </p>\n\n<ul>\n<li>The system must do <em>exactly</em>  the same things locally and on the server.</li>\n<li>The system should be made in an easy to understand scripting language.</li>\n<li>Each team owns its own build, but common code should be shared.</li>\n<li>Use as much <a href=\"https://en.wikipedia.org/wiki/Vanilla_software\">Vanilla</a> methods as possible.</li>\n<li>The system should be simple without too much lock-in to the build server we choose to use. </li>\n</ul>\n\n<p>Building for .NET has become much better the recent years. Where we before called MSBuild directly from PowerShell we now have <a href=\"https://github.com/psake/psake\">psake</a> inspired by Ruby's  <a href=\"https://github.com/ruby/rake\">rake</a>. We have some mixed experience with Powershell so we early turned this down. If you don't mind Powershell, psake seems to be a good choice.</p>\n\n<p>Next up we looked at <a href=\"https://fsharp.github.io/FAKE/\">FAKE</a>, a popular buildsystem written in F#. NRK has <a href=\"https://nrkbeta.no/2015/11/10/how-i-learned-to-stop-worrying-and-love-the-ci-server/\">written about switching to FAKE</a>. Some teams ended up using FAKE on some independent modules, but we ended up not using it because of the learning curve F# would bring to most of our developers. </p>\n\n<p>Thanks to Microsofts <a href=\"https://roslyn.codeplex.com/\">Roslyn compiler</a> we discovered <a href=\"http://scriptcs.net/\">ScriptCS</a>. ScriptCS let's you use C# as a scripting language, offering REPL and everything else you would expect from a scripting language. Since the language is C# every developer working can read and modify the scripts, no learning needed. Perfect. ScriptCS can also reference DLLs from GAC or a binary folder. </p>\n\n<p>We ended up creating a <code>tools\\</code> folder with a <code>build.csx</code> and a <code>common.csx</code> file. The <code>common.csx</code> file is centrally shared in source control and acts like our library with all methods needed for building tasks, like MSBuild, file management and calls to the NuGet API and triggers for <a href=\"https://octopus.com/\">Octopus Deploy</a>. The <code>build.csx</code>  file is the build file itself specified for the solution(s) to build, using methods from <code>common.csx</code>. The build system itself is distributed as a NuGet package to be installed installed to the root of the project we want to build. A <code>build.bat</code> file wrapps the build system and can be triggered like so:</p>\n\n<pre><code>./build.bat clean init compile unit integration package deploy\n</code></pre>\n\n<p>This flow cleans, pulls down dependencies, compiles, runs unit- and integrationtests, packages NuGet or Chocolatey packages and deploys to ProGet and Octopus Deploy. These steps are exactly alike locally and on the build server, just a command line call to build.bat. If ScriptCS is not installed on the system, the bat-file will do it for you. </p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>It took some time, but after a while we had a build system we were very satisfied with. The shared code is constantly changing as teams finds new needs. ScriptCS has been a success and have removed a lot of the magic from the build process. The teams have taken a lot more ownership over building than they had before. By pushing logic from the build server and down to the build system itself, everything works perfectly with Team City, TFS2015 and Jenkins, just as it should be. </p>\n\n<h3 id=\"note\">Note</h3>\n\n<p>If you don't want to write a ScriptCS-based build system from scratch, take a look at Cake. Cake is pretty much a ScriptCS based framework for building with lots of extension possibilities.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-01-02 12:31:27","created_by":1,"updated_at":"2016-02-14 09:32:48","updated_by":1,"published_at":"2016-01-02 12:32:42","published_by":1},{"id":16,"uuid":"3e7b9961-f2b1-4f09-bba9-17206e7055a4","title":"We created a tech blog!","slug":"we-created-a-tech-blog","markdown":"At [DIPS](https://www.dips.com/) Trondheim, we like to think about new ways to connect with developers and giving back to the community. A lot of us do Open Source development and attend meetups on a regular basis. We talk at [conferences](http://blog.amosti.net/trondheim-developer-conference/) and [universities](http://www.slideshare.net/hjerpbakk/kryssplatform-mobilutvikling-i-c-vha-xamarinforms). As geeks we also read blogs. A lot of blogs. Being Norwegian developers, we found that the amount of people and companies in the tech scene actually *blogging* in Norwegian was rather slim, so we decided to do something about it. DIPS ASA has over 150 developers, making us one of Norway's largest software houses. It's time for us to share some of our knowledge. That's how the [DIPS Tech Blog](http://dipsasa.github.io) was born. On this blog we will share tips and tricks from our every-day work, including solid patterns and practices, libraries and tools we use, how we design products and so on. Everything will be in Norwegian as well. We hope you like it!   \n\nVisit us at [tech.dips.no](http://dipsasa.github.io)! ","mobiledoc":null,"html":"<p>At <a href=\"https://www.dips.com/\">DIPS</a> Trondheim, we like to think about new ways to connect with developers and giving back to the community. A lot of us do Open Source development and attend meetups on a regular basis. We talk at <a href=\"http://blog.amosti.net/trondheim-developer-conference/\">conferences</a> and <a href=\"http://www.slideshare.net/hjerpbakk/kryssplatform-mobilutvikling-i-c-vha-xamarinforms\">universities</a>. As geeks we also read blogs. A lot of blogs. Being Norwegian developers, we found that the amount of people and companies in the tech scene actually <em>blogging</em> in Norwegian was rather slim, so we decided to do something about it. DIPS ASA has over 150 developers, making us one of Norway's largest software houses. It's time for us to share some of our knowledge. That's how the <a href=\"http://dipsasa.github.io\">DIPS Tech Blog</a> was born. On this blog we will share tips and tricks from our every-day work, including solid patterns and practices, libraries and tools we use, how we design products and so on. Everything will be in Norwegian as well. We hope you like it!   </p>\n\n<p>Visit us at <a href=\"http://dipsasa.github.io\">tech.dips.no</a>! </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-24 11:58:30","created_by":1,"updated_at":"2016-03-24 21:25:25","updated_by":1,"published_at":"2016-03-24 12:00:06","published_by":1},{"id":17,"uuid":"eaa33f8c-9176-4d5c-95a9-2e40591f73db","title":"DockerCon 2016: Docker nails iterative open source development","slug":"dockercon-2016-docker-nails-iterative-open-source-development","markdown":"Yesterday [DockerCon 2016](http://2016.dockercon.com/) kicked off from rainy Seattle with a brilliant keynote lead by CEO [Ben Golub](https://twitter.com/golubbe) and CTO [Solomon Hykes](https://twitter.com/solomonstre). Hykes talked about how Docker's goal is to make [tools of mass innovation](https://www.youtube.com/watch?v=apOEYhmskvQ), to remove as much friction as possible from the development workflow. One such example is [Docker for Mac and Windows](https://blog.docker.com/2016/03/docker-for-mac-windows-beta/). [Anand Prasad](https://twitter.com/AanandPrasad) came on stage as an \"First day on the job\" developer to demonstrate how Docker for Mac could help him get up and running, debugging and committing code - 10 minutes in on the new job.\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Live de-bugging inside a container with <a href=\"https://twitter.com/docker\">@docker</a> for Mac and Windows by <a href=\"https://twitter.com/AanandPrasad\">@AanandPrasad</a> <a href=\"https://twitter.com/hashtag/dockercon?src=hash\">#dockercon</a> <a href=\"https://t.co/zAUvVUOEc5\">pic.twitter.com/zAUvVUOEc5</a></p>&mdash; Betty Junod (@BettyJunod) <a href=\"https://twitter.com/BettyJunod/status/744935919673708544\">June 20, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n>As a beta user of Docker for Mac, I am really happy with how simple Docker has made this use case. With bringing Native Docker (or at least [close to native](https://blog.docker.com/2016/03/docker-for-mac-windows-beta/)) I have been able to bypass the Vagrant machine I have been using for running Docker - saving me a lot of overhead. Docker for Mac also gives me the option to run apps and throw them away when done. Great for my laptop. No more clutter with different MySQL databases in the same instance, just run the entire database in it's own container.\n\nThe big \"wow\" experience came with the introduction of the [new built-in orchestration](https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/). It is now possible to add hosts to a cluster and deploy containers to this cluster - with as little as 3 CLI commands, with just the Docker-Engine installed. Switch out the familiar ``run`` command with ``service`` and you are all set. This cluster is secure, self-healing and load balanced out of the box. This is in many ways what [Docker Swarm](https://docs.docker.com/swarm/overview/) should have been from the start. The new orchestration tool is also a great example of the mantra to remove friction from the development process. Docker has become really good at fixing the really hard parts and hiding them behind a simple CLI.\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">on stage: <a href=\"https://twitter.com/mikegoelzer\">@mikegoelzer</a> and <a href=\"https://twitter.com/aluzzardi\">@aluzzardi</a> launching service live with <a href=\"https://twitter.com/docker\">@docker</a> 1.12 <a href=\"https://twitter.com/hashtag/DockerCon?src=hash\">#DockerCon</a> <a href=\"https://t.co/dvItYBDeI0\">pic.twitter.com/dvItYBDeI0</a></p>&mdash; Docker (@docker) <a href=\"https://twitter.com/docker/status/744943959177199616\">June 20, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\nWhat the keynote shows is how good Docker is at doing Iterative Open Source development out in the open. When Docker first released it was a much simpler way to use ``namespacing`` and ``cgroups`` to isolate processes from each other. Great for testing and Linux based development, making it possible to set up a reproducible application. With the containers (or building blocks) came the need for orchestration. What use is a hundred containerized apps if it is a pain to orchestrate them? Since Docker has been Open Source from day one, many problems has been addressed by others. The [Fig Project](http://www.fig.sh/) (now Docker Compose) solved orchestration on the development level. Docker [acquired the team behind Fig](http://www.informationweek.com/cloud/infrastructure-as-a-service/docker-acquires-devops-flavor-with-fig/d/d-id/1297523) and put them in charge for development of Docker Compose.\n\nNext up came the split of the Docker application into the Docker Engine, the Client and the Machine. The Engine is the part that runs Docker, the client talks to it via the CLI. Docker Machine lets you install the Docker-Engine on multiple hosts and controls them with the client, making deployment on remote hosts much easier. The final product to come out of last year's DockerCon was Docker Swarm. Swarm enables multiple Docker Machines to function together as a cluster.\n\nThese products are results of the Docker iteration process. By attacking different challenges one at the time, Docker gives us tools that are adapted to specific needs. These tools can be built upon in the next iteration to tackle another challenge by another group of people. There would not have been a Docker-Engine or Docker-Compose without Docker itself. Docker Machine could not have been made without the Docker Engine. No Machine, no Swarm. Without Swarm? Certainly no [Docker-Cloud](https://www.docker.com/products/docker-cloud).\n","mobiledoc":null,"html":"<p>Yesterday <a href=\"http://2016.dockercon.com/\">DockerCon 2016</a> kicked off from rainy Seattle with a brilliant keynote lead by CEO <a href=\"https://twitter.com/golubbe\">Ben Golub</a> and CTO <a href=\"https://twitter.com/solomonstre\">Solomon Hykes</a>. Hykes talked about how Docker's goal is to make <a href=\"https://www.youtube.com/watch?v=apOEYhmskvQ\">tools of mass innovation</a>, to remove as much friction as possible from the development workflow. One such example is <a href=\"https://blog.docker.com/2016/03/docker-for-mac-windows-beta/\">Docker for Mac and Windows</a>. <a href=\"https://twitter.com/AanandPrasad\">Anand Prasad</a> came on stage as an \"First day on the job\" developer to demonstrate how Docker for Mac could help him get up and running, debugging and committing code - 10 minutes in on the new job.  </p>\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Live de-bugging inside a container with <a href=\"https://twitter.com/docker\">@docker</a> for Mac and Windows by <a href=\"https://twitter.com/AanandPrasad\">@AanandPrasad</a> <a href=\"https://twitter.com/hashtag/dockercon?src=hash\">#dockercon</a> <a href=\"https://t.co/zAUvVUOEc5\">pic.twitter.com/zAUvVUOEc5</a></p>&mdash; Betty Junod (@BettyJunod) <a href=\"https://twitter.com/BettyJunod/status/744935919673708544\">June 20, 2016</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<blockquote>\n  <p>As a beta user of Docker for Mac, I am really happy with how simple Docker has made this use case. With bringing Native Docker (or at least <a href=\"https://blog.docker.com/2016/03/docker-for-mac-windows-beta/\">close to native</a>) I have been able to bypass the Vagrant machine I have been using for running Docker - saving me a lot of overhead. Docker for Mac also gives me the option to run apps and throw them away when done. Great for my laptop. No more clutter with different MySQL databases in the same instance, just run the entire database in it's own container.</p>\n</blockquote>\n\n<p>The big \"wow\" experience came with the introduction of the <a href=\"https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/\">new built-in orchestration</a>. It is now possible to add hosts to a cluster and deploy containers to this cluster - with as little as 3 CLI commands, with just the Docker-Engine installed. Switch out the familiar <code>run</code> command with <code>service</code> and you are all set. This cluster is secure, self-healing and load balanced out of the box. This is in many ways what <a href=\"https://docs.docker.com/swarm/overview/\">Docker Swarm</a> should have been from the start. The new orchestration tool is also a great example of the mantra to remove friction from the development process. Docker has become really good at fixing the really hard parts and hiding them behind a simple CLI.  </p>\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">on stage: <a href=\"https://twitter.com/mikegoelzer\">@mikegoelzer</a> and <a href=\"https://twitter.com/aluzzardi\">@aluzzardi</a> launching service live with <a href=\"https://twitter.com/docker\">@docker</a> 1.12 <a href=\"https://twitter.com/hashtag/DockerCon?src=hash\">#DockerCon</a> <a href=\"https://t.co/dvItYBDeI0\">pic.twitter.com/dvItYBDeI0</a></p>&mdash; Docker (@docker) <a href=\"https://twitter.com/docker/status/744943959177199616\">June 20, 2016</a></blockquote>  \n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>What the keynote shows is how good Docker is at doing Iterative Open Source development out in the open. When Docker first released it was a much simpler way to use <code>namespacing</code> and <code>cgroups</code> to isolate processes from each other. Great for testing and Linux based development, making it possible to set up a reproducible application. With the containers (or building blocks) came the need for orchestration. What use is a hundred containerized apps if it is a pain to orchestrate them? Since Docker has been Open Source from day one, many problems has been addressed by others. The <a href=\"http://www.fig.sh/\">Fig Project</a> (now Docker Compose) solved orchestration on the development level. Docker <a href=\"http://www.informationweek.com/cloud/infrastructure-as-a-service/docker-acquires-devops-flavor-with-fig/d/d-id/1297523\">acquired the team behind Fig</a> and put them in charge for development of Docker Compose.</p>\n\n<p>Next up came the split of the Docker application into the Docker Engine, the Client and the Machine. The Engine is the part that runs Docker, the client talks to it via the CLI. Docker Machine lets you install the Docker-Engine on multiple hosts and controls them with the client, making deployment on remote hosts much easier. The final product to come out of last year's DockerCon was Docker Swarm. Swarm enables multiple Docker Machines to function together as a cluster.</p>\n\n<p>These products are results of the Docker iteration process. By attacking different challenges one at the time, Docker gives us tools that are adapted to specific needs. These tools can be built upon in the next iteration to tackle another challenge by another group of people. There would not have been a Docker-Engine or Docker-Compose without Docker itself. Docker Machine could not have been made without the Docker Engine. No Machine, no Swarm. Without Swarm? Certainly no <a href=\"https://www.docker.com/products/docker-cloud\">Docker-Cloud</a>.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-21 10:51:52","created_by":1,"updated_at":"2016-06-21 10:58:28","updated_by":1,"published_at":"2016-06-21 10:52:47","published_by":1},{"id":18,"uuid":"5bbd4d49-2056-44c9-bc35-0700cb1e8dfc","title":"Spawning TFS2015 Build Agents with Docker","slug":"spawning-tfs2015-build-agents-with-docker","markdown":"The Microsoft Open Source train keeps rolling fast forward with more and more projects popping up on GitHub. When we upgraded to TFS2015 late last year, the support for Linux build agents was so-so. \nA couple of weeks ago I decided to check in again. \n\nTo my delight I discovered that the \"old\" [build agent had been depricated](https://github.com/Microsoft/vso-agent) and the [new one](https://github.com/Microsoft/vsts-agent) has been implementet using [dotnet core](https://www.microsoft.com/net/core#macos).\nThat means cross-platform awesomeness!\n\nIt is now easy to install the agent on OSX, Ubuntu, the Red Hat family and so on.\nThat makes TFS much more accessible for people building other stuff than .NET. TFS has become a great product over the years, so this realy neat.\n\nThis also means that Docker can be used to spawn build agents on the fly. Somethimes we don't need to have build agents standing idle hours on end. Docker is great for spawning the agent when it actually is needed. It also allows us to install the relevant build-environment for each case. If you want to build Java, we can create a build agent with Java. Or Node. Or Mono.\n\nThe following ``Dockerfile`` gives you the idea:\n\n```\nFROM java\n\nRUN useradd -ms /bin/bash builder\nWORKDIR /home/builder\n\nRUN apt-get update && apt-get install -y libunwind8 libcurl3 libicu52 && apt-get install wget\n\nRUN mkdir buildAgent && cd buildAgent\nRUN wget https://github.com/Microsoft/vsts-agent/releases/download/v2.107.0/vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz\nRUN tar xzf vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz\n\nUSER builder\nCMD ./config.sh && ./run.sh\n```\n\nThe `Dockerfile` itself installs the build agent and starts an interactive configuration before starting the agent. The neat part is the `FROM` keyword. In this example, I want to build some Java code, so I base the file on the Java image.\n\nAnother tip is the scriptable setup of the agent. The `config.sh` script can be triggered noninteractive:\n\n```\ndocker run -it andmos/vsts-agent ./config.sh --unattended --acceptteeeula --url http://mylocaltfsserver:8080/tfs --auth Negotiate --username DOMAIN\\USER_NAME --password MyPassword --pool default --agent myagent && ./run.sh\n```","mobiledoc":null,"html":"<p>The Microsoft Open Source train keeps rolling fast forward with more and more projects popping up on GitHub. When we upgraded to TFS2015 late last year, the support for Linux build agents was so-so. <br />\nA couple of weeks ago I decided to check in again. </p>\n\n<p>To my delight I discovered that the \"old\" <a href=\"https://github.com/Microsoft/vso-agent\">build agent had been depricated</a> and the <a href=\"https://github.com/Microsoft/vsts-agent\">new one</a> has been implementet using <a href=\"https://www.microsoft.com/net/core#macos\">dotnet core</a>. <br />\nThat means cross-platform awesomeness!</p>\n\n<p>It is now easy to install the agent on OSX, Ubuntu, the Red Hat family and so on. <br />\nThat makes TFS much more accessible for people building other stuff than .NET. TFS has become a great product over the years, so this realy neat.</p>\n\n<p>This also means that Docker can be used to spawn build agents on the fly. Somethimes we don't need to have build agents standing idle hours on end. Docker is great for spawning the agent when it actually is needed. It also allows us to install the relevant build-environment for each case. If you want to build Java, we can create a build agent with Java. Or Node. Or Mono.</p>\n\n<p>The following <code>Dockerfile</code> gives you the idea:</p>\n\n<pre><code>FROM java\n\nRUN useradd -ms /bin/bash builder  \nWORKDIR /home/builder\n\nRUN apt-get update &amp;&amp; apt-get install -y libunwind8 libcurl3 libicu52 &amp;&amp; apt-get install wget\n\nRUN mkdir buildAgent &amp;&amp; cd buildAgent  \nRUN wget https://github.com/Microsoft/vsts-agent/releases/download/v2.107.0/vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz  \nRUN tar xzf vsts-agent-ubuntu.14.04-x64-2.107.0.tar.gz\n\nUSER builder  \nCMD ./config.sh &amp;&amp; ./run.sh  \n</code></pre>\n\n<p>The <code>Dockerfile</code> itself installs the build agent and starts an interactive configuration before starting the agent. The neat part is the <code>FROM</code> keyword. In this example, I want to build some Java code, so I base the file on the Java image.</p>\n\n<p>Another tip is the scriptable setup of the agent. The <code>config.sh</code> script can be triggered noninteractive:</p>\n\n<pre><code>docker run -it andmos/vsts-agent ./config.sh --unattended --acceptteeeula --url http://mylocaltfsserver:8080/tfs --auth Negotiate --username DOMAIN\\USER_NAME --password MyPassword --pool default --agent myagent &amp;&amp; ./run.sh  \n</code></pre>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-09-24 12:47:39","created_by":1,"updated_at":"2016-09-24 12:52:40","updated_by":1,"published_at":"2016-09-24 12:48:56","published_by":1},{"id":19,"uuid":"ca71c7aa-a0dc-4cb6-8486-895019f44f2e","title":"Rolling out web services with Topshelf, Chocolatey and Ansible","slug":"rolling-out-web-services-with-topshelf-chocolatey-and-ansible","markdown":"I have been doing a lot of automation on the Windows platform at work lately. That sentence would have been associated with pain some years ago - but things have changed. [Ansible](https://www.ansible.com/) is nothing less than the perfect provisioning tool for both Linux and Windows, providing (in my opinion) the best level of abstraction when managing systems and state. A lot of modules are there also for Windows, so you have to put minimal of effort in the details of the provisioning steps, making it easy for people who are not developers or scripting guys to catch the gist of it. The next tool that has changed the game is [Chocolatey](https://chocolatey.org/). One of the traditional advantages Linux has had  over the Windows platform has been package managers - or a standardized interface for finding and installing software. Chocolatey is is an ambitious attempt at the same thing for Windows. Scott Hanselman [wrote about it ](http://www.hanselman.com/blog/IsTheWindowsUserReadyForAptget.aspx) back in 2013, and since then the project has catched on and kept growing. At DIPS we [package our software with Chocolatey](http://tech.dips.no/2016/08/17/Deployment-av-servere.html), giving us more scriptable flexibility over the old MSI regime. Chocolatey shines on it's own, but combined with Ansible it is _pure_ magic. Ansible provides a [Chocolatey module](https://docs.ansible.com/ansible/win_chocolatey_module.html) that let's us install Chocolatey packages as a part of the provisioning. Take the following examples from our Playbooks:\n\n\n\n``` python\n - name: Install DotNet Framework 4.6.1\n   win_chocolatey: name=dotnet4.6.1\n```\n\n```python\n- name: Install Octopus Tentacle Files\n  win_chocolatey: name=octopusdeploy.tentacle\n```\n\n```python\n- name: Install Java JDK 8\n  win_chocolatey: name=jdk8\n```\n\n\n\nYou catch the point. Almost looks like `apt-get` right there.\n\nNext up I like to show how a HTTP service can be rolled out with Ansible and Chocolatey. Since we roll out our own software as Chocolatey packages in a Continuous Delivery pipeline, the need to monitor exactly which packages are deployed to a given server at _this time_ came up. To deal with it I wrote [Stratos](https://github.com/andmos/Stratos), a simple HTTP API to report what Chocolatey packages are installed on the server. A `GET` on `/api/chocoPackages` will returns some JSON:\n\n```json\n[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 3,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.5.2\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 5,\n        \"build\": 2,\n        \"revision\": 20140902,\n        \"majorRevision\": 307,\n        \"minorRevision\": 21350\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.6.1\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 6,\n        \"build\": 1055,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  }\n]\n```\n\nSimple, but enough to provide data for a simple dashboard.\n\n\n\nStratos is written with [Nancy](http://nancyfx.org/) and [Topshelf](http://topshelf-project.com/). Topshelf is great because it allows you to install Console applications as Windows Services. Together with Nancy's self-hostable package this means that the HTTP service can be deployed without IIS, which is a good thing for simple applications like this one. The main method for Stratos looks like this:\n\n\n\n```c#\nusing Topshelf.Nancy;\nusing Topshelf;\n\nnamespace Stratos\n{\n\tpublic class Program\n\t{\n\t\tstatic void Main(string[] args)\n\t\t{\n\t\t\tvar host = HostFactory.New(x =>\n\t\t\t{\n\t\t\t\tx.UseLinuxIfAvailable();\n\t\t\t\tx.Service<StratosSelfHost>(s =>\n\t\t\t\t{\n\t\t\t\t\ts.ConstructUsing(settings => new StratosSelfHost());\n\t\t\t\t\ts.WhenStarted(service => service.Start());\n\t\t\t\t\ts.WhenStopped(service => service.Stop());\n\t\t\t\t\ts.WithNancyEndpoint(x, c =>\n\t\t\t\t\t{\n\t\t\t\t\t\tc.AddHost(port: 1337);\n\t\t\t\t\t\tc.CreateUrlReservationsOnInstall();\n\t\t\t\t\t\tc.OpenFirewallPortsOnInstall(firewallRuleName: \"StratosService\");\n\t\t\t\t\t});\n\t\t\t\t});\n\n\t\t\t\tx.StartAutomatically();\n\t\t\t\tx.SetServiceName(\"StratosService\");\n\t\t\t\tx.SetDisplayName(\"StratosService\");\n\t\t\t\tx.SetDescription(\"StratosService\");\n\t\t\t\tx.RunAsNetworkService();\n\n\t\t\t});\n\t\t\thost.Run();\n\t\t}\n\t}\n}\n```\n\nAll the configuration in one method. Sweet.\n\n\n\nThe application is packaged up as a Chocolatey package, AKA NuGet with a `ChocolateyInstall.ps1` script for the installation of the service:\n\n\n\n```powershell\nWrite-Host \"Installing Stratos as as windows service...\"\n\ntry {\n\n    $service_name = \"StratosService\"\n    $process_name = \"StratosService\"\n    $serviceFileName = \"Stratos.exe\"\n\n    $PSScriptRoot = Split-Path -parent $MyInvocation.MyCommand.Definition\n    $packageDir = $PSScriptRoot | Split-Path;\n    $srcDir = \"$($PSScriptRoot)\\..\\bin\"\n    $destDir = \"$srcDir\"\n\n    $service = Get-Service | Where-Object {$_.Name -eq $service_name}\n\n    if($service){\n        Stop-Service $service_name\n        $service.WaitForStatus(\"Stopped\")\n        kill -processname $process_name -force -ErrorAction SilentlyContinue\n        Wait-Process -Name $process_name -ErrorAction SilentlyContinue\n        Write-Host \"Uninstalling $service_name...\"\n\n        $fileToUninstall = Join-Path \"$srcDir\\\" $serviceFileName\n        . $fileToUninstall uninstall\n    }\n\n    $fileToInstall = Join-Path \"$destDir\\\" $serviceFileName\n    . $fileToInstall install\n}\ncatch {\n    throw $_.Exception\n}\ntry{\n    . $fileToInstall start\n}\ncatch{\n    Write-Host \"$process_name was successfully installed, but could not be started. This is most likely because of a configuration error. Please check the Windows Event Log.\"\n}\n```\n\nThe next part is to deploy the service. That is the easy part thanks to Ansible:\n\n\n\n```powershell\n- name: Install Stratos Chocolatey service\n  win_chocolatey: name=stratos source=http://dips-nuget/nuget/InternalSoftware state=present upgrade=True\n```\n\nThe `upgrade=True` flag will make sure that any new versions of the service get's rolled out.\n","mobiledoc":null,"html":"<p>I have been doing a lot of automation on the Windows platform at work lately. That sentence would have been associated with pain some years ago - but things have changed. <a href=\"https://www.ansible.com/\">Ansible</a> is nothing less than the perfect provisioning tool for both Linux and Windows, providing (in my opinion) the best level of abstraction when managing systems and state. A lot of modules are there also for Windows, so you have to put minimal of effort in the details of the provisioning steps, making it easy for people who are not developers or scripting guys to catch the gist of it. The next tool that has changed the game is <a href=\"https://chocolatey.org/\">Chocolatey</a>. One of the traditional advantages Linux has had  over the Windows platform has been package managers - or a standardized interface for finding and installing software. Chocolatey is is an ambitious attempt at the same thing for Windows. Scott Hanselman <a href=\"http://www.hanselman.com/blog/IsTheWindowsUserReadyForAptget.aspx\">wrote about it </a> back in 2013, and since then the project has catched on and kept growing. At DIPS we <a href=\"http://tech.dips.no/2016/08/17/Deployment-av-servere.html\">package our software with Chocolatey</a>, giving us more scriptable flexibility over the old MSI regime. Chocolatey shines on it's own, but combined with Ansible it is <em>pure</em> magic. Ansible provides a <a href=\"https://docs.ansible.com/ansible/win_chocolatey_module.html\">Chocolatey module</a> that let's us install Chocolatey packages as a part of the provisioning. Take the following examples from our Playbooks:</p>\n\n<pre><code class=\"language- python\"> - name: Install DotNet Framework 4.6.1\n   win_chocolatey: name=dotnet4.6.1\n</code></pre>\n\n<pre><code class=\"language-python\">- name: Install Octopus Tentacle Files\n  win_chocolatey: name=octopusdeploy.tentacle\n</code></pre>\n\n<pre><code class=\"language-python\">- name: Install Java JDK 8\n  win_chocolatey: name=jdk8\n</code></pre>\n\n<p>You catch the point. Almost looks like <code>apt-get</code> right there.</p>\n\n<p>Next up I like to show how a HTTP service can be rolled out with Ansible and Chocolatey. Since we roll out our own software as Chocolatey packages in a Continuous Delivery pipeline, the need to monitor exactly which packages are deployed to a given server at <em>this time</em> came up. To deal with it I wrote <a href=\"https://github.com/andmos/Stratos\">Stratos</a>, a simple HTTP API to report what Chocolatey packages are installed on the server. A <code>GET</code> on <code>/api/chocoPackages</code> will returns some JSON:</p>\n\n<pre><code class=\"language-json\">[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 3,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.5.2\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 5,\n        \"build\": 2,\n        \"revision\": 20140902,\n        \"majorRevision\": 307,\n        \"minorRevision\": 21350\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"DotNet4.6.1\",\n    \"version\": {\n      \"version\": {\n        \"major\": 4,\n        \"minor\": 6,\n        \"build\": 1055,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  }\n]\n</code></pre>\n\n<p>Simple, but enough to provide data for a simple dashboard.</p>\n\n<p>Stratos is written with <a href=\"http://nancyfx.org/\">Nancy</a> and <a href=\"http://topshelf-project.com/\">Topshelf</a>. Topshelf is great because it allows you to install Console applications as Windows Services. Together with Nancy's self-hostable package this means that the HTTP service can be deployed without IIS, which is a good thing for simple applications like this one. The main method for Stratos looks like this:</p>\n\n<pre><code class=\"language-c#\">using Topshelf.Nancy;  \nusing Topshelf;\n\nnamespace Stratos  \n{\n    public class Program\n    {\n        static void Main(string[] args)\n        {\n            var host = HostFactory.New(x =&gt;\n            {\n                x.UseLinuxIfAvailable();\n                x.Service&lt;StratosSelfHost&gt;(s =&gt;\n                {\n                    s.ConstructUsing(settings =&gt; new StratosSelfHost());\n                    s.WhenStarted(service =&gt; service.Start());\n                    s.WhenStopped(service =&gt; service.Stop());\n                    s.WithNancyEndpoint(x, c =&gt;\n                    {\n                        c.AddHost(port: 1337);\n                        c.CreateUrlReservationsOnInstall();\n                        c.OpenFirewallPortsOnInstall(firewallRuleName: \"StratosService\");\n                    });\n                });\n\n                x.StartAutomatically();\n                x.SetServiceName(\"StratosService\");\n                x.SetDisplayName(\"StratosService\");\n                x.SetDescription(\"StratosService\");\n                x.RunAsNetworkService();\n\n            });\n            host.Run();\n        }\n    }\n}\n</code></pre>\n\n<p>All the configuration in one method. Sweet.</p>\n\n<p>The application is packaged up as a Chocolatey package, AKA NuGet with a <code>ChocolateyInstall.ps1</code> script for the installation of the service:</p>\n\n<pre><code class=\"language-powershell\">Write-Host \"Installing Stratos as as windows service...\"\n\ntry {\n\n    $service_name = \"StratosService\"\n    $process_name = \"StratosService\"\n    $serviceFileName = \"Stratos.exe\"\n\n    $PSScriptRoot = Split-Path -parent $MyInvocation.MyCommand.Definition\n    $packageDir = $PSScriptRoot | Split-Path;\n    $srcDir = \"$($PSScriptRoot)\\..\\bin\"\n    $destDir = \"$srcDir\"\n\n    $service = Get-Service | Where-Object {$_.Name -eq $service_name}\n\n    if($service){\n        Stop-Service $service_name\n        $service.WaitForStatus(\"Stopped\")\n        kill -processname $process_name -force -ErrorAction SilentlyContinue\n        Wait-Process -Name $process_name -ErrorAction SilentlyContinue\n        Write-Host \"Uninstalling $service_name...\"\n\n        $fileToUninstall = Join-Path \"$srcDir\\\" $serviceFileName\n        . $fileToUninstall uninstall\n    }\n\n    $fileToInstall = Join-Path \"$destDir\\\" $serviceFileName\n    . $fileToInstall install\n}\ncatch {  \n    throw $_.Exception\n}\ntry{  \n    . $fileToInstall start\n}\ncatch{  \n    Write-Host \"$process_name was successfully installed, but could not be started. This is most likely because of a configuration error. Please check the Windows Event Log.\"\n}\n</code></pre>\n\n<p>The next part is to deploy the service. That is the easy part thanks to Ansible:</p>\n\n<pre><code class=\"language-powershell\">- name: Install Stratos Chocolatey service\n  win_chocolatey: name=stratos source=http://dips-nuget/nuget/InternalSoftware state=present upgrade=True\n</code></pre>\n\n<p>The <code>upgrade=True</code> flag will make sure that any new versions of the service get's rolled out.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-11-20 16:34:55","created_by":1,"updated_at":"2016-11-20 16:42:02","updated_by":1,"published_at":"2016-11-20 16:35:37","published_by":1},{"id":20,"uuid":"5aaa023e-2bb0-4824-8ea6-de60a6bd0682","title":"Docker Garbage Collection version 1.13.0 update!","slug":"docker-garbage-collection-version-1-13-0-update","markdown":"Back in 2015 I blogged about how the missing garbage collection in Docker could seriously fill up the drive on the computer you\ndevelop Docker Images on. This was a serioust problem, and the solution (then) was the [Spotify docker-gc\nimage](https://github.com/spotify/docker-gc) image. Now, with the 1.13 release we *finally* got support for just this in the\ndocker-client: `docker system prune`. \n\n`docker system prune` will remove all unused data - that is hanging images (images not used by any existing container) as well as\nold volumes and networks. With the cleanup command in place, what suprises me is how long it took before we finally got this quite\nbasic functionality in the client.  ","mobiledoc":null,"html":"<p>Back in 2015 I blogged about how the missing garbage collection in Docker could seriously fill up the drive on the computer you <br />\ndevelop Docker Images on. This was a serioust problem, and the solution (then) was the <a href=\"https://github.com/spotify/docker-gc\">Spotify docker-gc <br />\nimage</a> image. Now, with the 1.13 release we <em>finally</em> got support for just this in the <br />\ndocker-client: <code>docker system prune</code>. </p>\n\n<p><code>docker system prune</code> will remove all unused data - that is hanging images (images not used by any existing container) as well as\nold volumes and networks. With the cleanup command in place, what suprises me is how long it took before we finally got this quite <br />\nbasic functionality in the client.  </p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-15 18:24:40","created_by":1,"updated_at":"2017-06-15 18:25:42","updated_by":1,"published_at":"2017-03-01 18:24:00","published_by":1},{"id":21,"uuid":"2e6c861e-97f2-4a9c-a63b-f84819ed053c","title":"My Favorite Podcasts - 2017 edition","slug":"my-favorite-podcasts-2017-edition-2","markdown":"I am a huge fan of podcasts. They are a great way to learn about new subjects, both from the tech world, science and academia. Here is a list of some of my favorite podcasts so far in 2017.\n\n\n## Tech and Design\n\n### [Arrested DevOps](https://www.arresteddevops.com/)\n\nProbably the leading podcast about the DevOps movement. Matt Stratton (Chef), Trevor Hess (10th Magnitude) and Bridget Kromhout (Pivotal) guides the listener through the ever-changing world of DevOps - including tools, patterns and practices. What makes the show good is the amount of guests that are well known in the DevOps community, including [Julia Evens](https://www.arresteddevops.com/discovery/), [James Turnbull](https://www.arresteddevops.com/art-of-monitoring-james-turnbull/), [Ben Hughes](https://www.arresteddevops.com/containers-security/) and [Jessie Frazelle](https://www.arresteddevops.com/containers-security/) to name a few. You don't have to work with DevOps related subjects to enjoy this show, all software developers or ops people will get something out of it.\n\n\n\n### [Bread Time](https://breadtime.simplecast.fm/)\n\nBread Time is a micro-podcast, which means episodes are between 10 - 30 minutes. Hosted by Gabriel Valdivia (Facebook) and Charlie Deets (WhatsApp), the show touches in to a wide spectrum of topics, ranging from design and software development to soft skills and [how to work well with others](https://breadtime.simplecast.fm/episodes/63809-working-well-with-others), or [Developing an effective workflow](https://breadtime.simplecast.fm/episodes/61202-developing-an-effective-workflow). The podcast is a nice fit for the short commute to work. I would recommend it for software developers, but especially for designers.\n\n\n\n### [No Fluff Just Stuff](https://www.nofluffjuststuff.com/podcast)\n\nOriginally a conference series about Java and software architecture, No Fluff Just Stuff also provides a kick ass podcast about software development, architecture, agile practices and in-depth interviews with developers. NFJS delivers rich content about topics every developer needs to know about, like Git, microservices and REST API's, critical thinking and software evangelism. Hosted by [Michael Carducci](https://www.nofluffjuststuff.com/conference/speaker/michael_carducci).  \n\n\n\n### [99% Invisible](http://99percentinvisible.org/)\n\nOh you already know about this one. 99% Invisible is the holy grail for all things architecture and design. In their own words:\n\n> 99% Invisible is about all the thought that goes into the things we don’t think about — the unnoticed architecture and design that shape our world.\n\nHosted by Roman Mars, the man with the silk voice, 99% Invisible is guaranteed to teach you something new each week, both about design, architecture and history. My favorite episodes include [Ten Letters for the Presiden](http://99percentinvisible.org/episode/ten-letters-president/), [Project Cybersyn](http://99percentinvisible.org/episode/project-cybersyn/) and [Unpleasant Design & Hostile Urban Architecture](http://99percentinvisible.org/episode/unpleasant-design-hostile-urban-architecture/). No matter who you are and what your interests are - you will love 99% Invisible.\n\n\n\n### [Accidental Tech Podcast](http://atp.fm/)\n\nAnother famous one, at least for the Apple fanboys. Marco Arment (Tumblr, Instapaper, Overcast), John Siracusa (Hypercritical, AnandTech) and Casey Liss (Macdown, Analog(ue) and other projects, sorry Casey) were making a car show, but ended up talking about tech - hence the name. These guys are famous in the Apple community, and every week they talk critically about all things Apple. If this sounds boring or you are not an Apple guy, fear not. All of the guys are developers too, and often touches into software development related topics, or random stuff like the [Nintento Switch](http://atp.fm/episodes/215) or [the American school system](the American school system). Also worth listening to just for the ending song.\n\n\n\n## Norwegian\n\n\n\n### [NRK Beta](https://nrkbeta.no/)\n\nNRK Beta is Norsk Rikskringkastings sandbox for technology and the new media. In a world where the traditional media like newspapers and TV struggle, NRK Beta is on a mission to see how modern technology can fit in with how journalists work and how we as users of their service consume content. NRK Beta has worked along side the mothership NRK as advisors on new media experiences, like the popular web-only series [SKAM](http://skam.p3.no/) and the now famous [Slow television](https://en.wikipedia.org/wiki/Slow_television) concept. The podcast talks us through  these solutions and how they came to be.  \n\n\n\n### [BEKK Open Podcast](https://open.bekk.no/)\n\nBEKK consulting is one of Norway's leading consultant houses and strive to be ahead of the technological curve. This means trying out the newest technology in customer projects and talk about the experience afterwards. A typical podcast episode revolves around a specific kind of technology, like [the functional paradigm](https://open.bekk.no/podcast-om-det-funksjonelle-paradigmet), [machine learning](https://open.bekk.no/podcast-om-maskinlering) and [creativity](https://open.bekk.no/podcast-om-kreativitet). The production is top notch and the hosts are developers and scientists working for BEKK. If you understand some Norwegian and love to write code, this show is for you.\n\n\n\n### [Giæver og Joffen](http://www.vg.no/podcast/giaever-og-joffen/)\n\nA political podcast!? Yes. As a matter of fact, it might be Norway's best about domestic and foreign political news. Anders Giæver and Frithjof Jacobsen are some of VG's heaviest political commentators. The podcast is recommended if you want to catch up on this weeks headlines.\n\n\n\n### [Hva er greia med?](http://www.rubicontv.no/radio/13/hva-er-greia-med)\n\nThe last podcast on the list is probably my favorite. In Hva er greia med? (What's The Deal With?) the hosts, Dr. Jonas Bergland and Dr. Ole Elvebakk do homework about a specific topic that interest them and talk about it for about an hour. The result is great entertainment where you also learn something from each episode. Topics so far include the Waldorf school, fingerprints, Ayn Rand, The Golden Ratio, Zombies, hearing and many many more subjects.\n","mobiledoc":null,"html":"<p>I am a huge fan of podcasts. They are a great way to learn about new subjects, both from the tech world, science and academia. Here is a list of some of my favorite podcasts so far in 2017.</p>\n\n<h2 id=\"techanddesign\">Tech and Design</h2>\n\n<h3 id=\"arresteddevopshttpswwwarresteddevopscom\"><a href=\"https://www.arresteddevops.com/\">Arrested DevOps</a></h3>\n\n<p>Probably the leading podcast about the DevOps movement. Matt Stratton (Chef), Trevor Hess (10th Magnitude) and Bridget Kromhout (Pivotal) guides the listener through the ever-changing world of DevOps - including tools, patterns and practices. What makes the show good is the amount of guests that are well known in the DevOps community, including <a href=\"https://www.arresteddevops.com/discovery/\">Julia Evens</a>, <a href=\"https://www.arresteddevops.com/art-of-monitoring-james-turnbull/\">James Turnbull</a>, <a href=\"https://www.arresteddevops.com/containers-security/\">Ben Hughes</a> and <a href=\"https://www.arresteddevops.com/containers-security/\">Jessie Frazelle</a> to name a few. You don't have to work with DevOps related subjects to enjoy this show, all software developers or ops people will get something out of it.</p>\n\n<h3 id=\"breadtimehttpsbreadtimesimplecastfm\"><a href=\"https://breadtime.simplecast.fm/\">Bread Time</a></h3>\n\n<p>Bread Time is a micro-podcast, which means episodes are between 10 - 30 minutes. Hosted by Gabriel Valdivia (Facebook) and Charlie Deets (WhatsApp), the show touches in to a wide spectrum of topics, ranging from design and software development to soft skills and <a href=\"https://breadtime.simplecast.fm/episodes/63809-working-well-with-others\">how to work well with others</a>, or <a href=\"https://breadtime.simplecast.fm/episodes/61202-developing-an-effective-workflow\">Developing an effective workflow</a>. The podcast is a nice fit for the short commute to work. I would recommend it for software developers, but especially for designers.</p>\n\n<h3 id=\"nofluffjuststuffhttpswwwnofluffjuststuffcompodcast\"><a href=\"https://www.nofluffjuststuff.com/podcast\">No Fluff Just Stuff</a></h3>\n\n<p>Originally a conference series about Java and software architecture, No Fluff Just Stuff also provides a kick ass podcast about software development, architecture, agile practices and in-depth interviews with developers. NFJS delivers rich content about topics every developer needs to know about, like Git, microservices and REST API's, critical thinking and software evangelism. Hosted by <a href=\"https://www.nofluffjuststuff.com/conference/speaker/michael_carducci\">Michael Carducci</a>.  </p>\n\n<h3 id=\"99invisiblehttp99percentinvisibleorg\"><a href=\"http://99percentinvisible.org/\">99% Invisible</a></h3>\n\n<p>Oh you already know about this one. 99% Invisible is the holy grail for all things architecture and design. In their own words:</p>\n\n<blockquote>\n  <p>99% Invisible is about all the thought that goes into the things we don’t think about — the unnoticed architecture and design that shape our world.</p>\n</blockquote>\n\n<p>Hosted by Roman Mars, the man with the silk voice, 99% Invisible is guaranteed to teach you something new each week, both about design, architecture and history. My favorite episodes include <a href=\"http://99percentinvisible.org/episode/ten-letters-president/\">Ten Letters for the Presiden</a>, <a href=\"http://99percentinvisible.org/episode/project-cybersyn/\">Project Cybersyn</a> and <a href=\"http://99percentinvisible.org/episode/unpleasant-design-hostile-urban-architecture/\">Unpleasant Design &amp; Hostile Urban Architecture</a>. No matter who you are and what your interests are - you will love 99% Invisible.</p>\n\n<h3 id=\"accidentaltechpodcasthttpatpfm\"><a href=\"http://atp.fm/\">Accidental Tech Podcast</a></h3>\n\n<p>Another famous one, at least for the Apple fanboys. Marco Arment (Tumblr, Instapaper, Overcast), John Siracusa (Hypercritical, AnandTech) and Casey Liss (Macdown, Analog(ue) and other projects, sorry Casey) were making a car show, but ended up talking about tech - hence the name. These guys are famous in the Apple community, and every week they talk critically about all things Apple. If this sounds boring or you are not an Apple guy, fear not. All of the guys are developers too, and often touches into software development related topics, or random stuff like the <a href=\"http://atp.fm/episodes/215\">Nintento Switch</a> or <a href=\"the American school system\">the American school system</a>. Also worth listening to just for the ending song.</p>\n\n<h2 id=\"norwegian\">Norwegian</h2>\n\n<h3 id=\"nrkbetahttpsnrkbetano\"><a href=\"https://nrkbeta.no/\">NRK Beta</a></h3>\n\n<p>NRK Beta is Norsk Rikskringkastings sandbox for technology and the new media. In a world where the traditional media like newspapers and TV struggle, NRK Beta is on a mission to see how modern technology can fit in with how journalists work and how we as users of their service consume content. NRK Beta has worked along side the mothership NRK as advisors on new media experiences, like the popular web-only series <a href=\"http://skam.p3.no/\">SKAM</a> and the now famous <a href=\"https://en.wikipedia.org/wiki/Slow_television\">Slow television</a> concept. The podcast talks us through  these solutions and how they came to be.  </p>\n\n<h3 id=\"bekkopenpodcasthttpsopenbekkno\"><a href=\"https://open.bekk.no/\">BEKK Open Podcast</a></h3>\n\n<p>BEKK consulting is one of Norway's leading consultant houses and strive to be ahead of the technological curve. This means trying out the newest technology in customer projects and talk about the experience afterwards. A typical podcast episode revolves around a specific kind of technology, like <a href=\"https://open.bekk.no/podcast-om-det-funksjonelle-paradigmet\">the functional paradigm</a>, <a href=\"https://open.bekk.no/podcast-om-maskinlering\">machine learning</a> and <a href=\"https://open.bekk.no/podcast-om-kreativitet\">creativity</a>. The production is top notch and the hosts are developers and scientists working for BEKK. If you understand some Norwegian and love to write code, this show is for you.</p>\n\n<h3 id=\"giverogjoffenhttpwwwvgnopodcastgiaeverogjoffen\"><a href=\"http://www.vg.no/podcast/giaever-og-joffen/\">Giæver og Joffen</a></h3>\n\n<p>A political podcast!? Yes. As a matter of fact, it might be Norway's best about domestic and foreign political news. Anders Giæver and Frithjof Jacobsen are some of VG's heaviest political commentators. The podcast is recommended if you want to catch up on this weeks headlines.</p>\n\n<h3 id=\"hvaergreiamedhttpwwwrubicontvnoradio13hvaergreiamed\"><a href=\"http://www.rubicontv.no/radio/13/hva-er-greia-med\">Hva er greia med?</a></h3>\n\n<p>The last podcast on the list is probably my favorite. In Hva er greia med? (What's The Deal With?) the hosts, Dr. Jonas Bergland and Dr. Ole Elvebakk do homework about a specific topic that interest them and talk about it for about an hour. The result is great entertainment where you also learn something from each episode. Topics so far include the Waldorf school, fingerprints, Ayn Rand, The Golden Ratio, Zombies, hearing and many many more subjects.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-15 18:27:17","created_by":1,"updated_at":"2017-06-15 18:28:04","updated_by":1,"published_at":"2017-04-14 18:27:00","published_by":1},{"id":22,"uuid":"f6e2255c-c3b7-49c8-8fa7-f9714612b55f","title":"How I broke an API (and how you can do it too!)","slug":"how-i-broke-an-api-and-how-you-can-do-it-too","markdown":"> Disclaimer: The content of this post is not nearly as dramatic as the title will have it, and for a lot of developers it will sound obvious. Based on code I have seen I nonetheless think it can learn a lot of people a valid lesson as well, and hopefully help other avoid the mistakes laziness can lead to.\n\nIn november last year [I wrote about Stratos,](http://blog.amosti.net/rolling-out-web-services-with-topshelf-chocolatey-and-ansible/) a simple web API based on Nancy with the sole purpose of listing out installed Chocolatey packages on a server. With this convenient web-process installed on our servers, the team figured out that it would be nice if it also supported [plugins](https://github.com/andmos/Stratos/blob/master/doc/Plugin.md).\n\nWith plugin support we could serve other useful info from the servers via the same API host. So far so good. The story begins when one of my colleagues had to serialize some rather complex objects to JSON. Out of the box Nancy comes with it's own JSON serializer - not using [Json.NET](http://www.newtonsoft.com/json) as the rest of the world uses. This serializer did some funny things with his object, so he wanted to use Json.NET. No problem, the Nancy guys offer [Nancy.Serialization.JsonNet](https://github.com/NancyFx/Nancy.Serialization.JsonNet) as a NuGet package.\nJust slide the DLL in and Nancy grabs hold of it. God stuff.\n\nAfter checking in the updated code the CI build failed on the original Stratos API tests. We expect a JSON on the following format when asking for installed Chocolatey packages:\n\n```\n[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 6,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"chocolatey-core.extension\",\n    \"version\": {\n      \"version\": {\n        \"major\": 1,\n        \"minor\": 1,\n        \"build\": 0,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  }]\n```\n\nAnd what we got with Json.Net was\n\n```\n[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": \"0.10.6\\r\"\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": \"1.1.0\\r\"\n  }\n]\n```\n\nThis was quite a surprise. The two JSON serializers were acting quite differently.\n\nThe original object should be quite simple:\n\n```\nusing NuGet;\n\nnamespace Stratos.Model\n{\n    public class NuGetPackage\n    {\n        public string PackageName { get; set;}\n        public SemanticVersion Version { get; set; }\n    }\n}\n```\n\nRight of the bat this should be no problem right?\nTrying to debug this a thought struck me: the only place I need `NuGet.Core` is in this object, to have a `SemanticVersion` object. [If we look at the SemanticVersion class](https://github.com/NuGet/NuGet2/blob/2.13/src/Core/SemanticVersion.cs) there are a lot of stuff there that could mess up the JSON serializer. Not owning the class ourself also prevent us from using things like DataMember attributes to control what parts of the object should be serialized. Another quite lazy choice here is to use the `SemanticVersion` object itself as a model property. There is a lot of dead weight on the object we don't need. A better choice is to wrap the parts we need in it's own object:\n\n```\nnamespace Stratos.Model\n{\n    public class PackageVersion\n    {\n        public Version Version { get; set; }\n        public string SpecialVersion { get; set; }\n\n    }\n}\n```\n\nAnd use it in the original object:\n\n```\nnamespace Stratos.Model\n{\n\tpublic class NuGetPackage\n\t{\n\t\tpublic string PackageName { get; set;}\n\t\tpublic PackageVersion Version { get; set; }\n\t}\n}\n```\n\nTo get rid of the NuGet reference, the `SemanticVersion` class got duplicated in. It is much better to own that logic ourself.\n\nWith this refactoring the JSON response looked a lot better:\n\n```\n[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 0,\n        \"Minor\": 10,\n        \"Build\": 6,\n        \"Revision\": 1,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 1\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 1,\n        \"Minor\": 1,\n        \"Build\": 0,\n        \"Revision\": 0,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 0\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n```\n\nCool, let's ship it!\n\nAn hour later, we had 25 systems that consumed this API die. What had gone wrong? The observant reader have seen it already:\n\n`packageName` vs. `PackageName`. Lowercase, uppercase. The Nancy serializer lowercases the keys by default, while Json.Net don't.\n\nWhy didn't the testes go red you ask? Let's look at the asserts in the test:\n\n```\nvar result = browser.Get(\"/api/chocoPackages\", with =>\n\t\t\t{\n\t\t\t\twith.HttpRequest();\t\t\t\n\t\t\t});\n\n\t\t\tvar resultJson = result.Body.AsString().ToLower();\n\n\t\t\tAssert.Equal(HttpStatusCode.OK, result.StatusCode);\n\t\t\tAssert.True(resultJson.Contains(\"major\"));\nAssert.True(resultJson.Contains(\"minor\"));\n```\n\n`ToLower()`. Yeah. The consumer of the API did *not* use `ToLower()`, obviously.\n\nSo what can we learn from this story?\n\n* Don't take on dependencies for a single and simple usecase\n\nI referenced `Nuget.Core` as a NuGet package to grab hold of the `SemanticVersion` class for parsing semantic version strings to a `Version` object. That usecase is so slim that just duplicating `SemanticVersion` from GitHub to my project is a much better approach - it is just stupid to take on a NuGet dependency.\n\n* Allways unit test as far out as possible\n\nBy having unit tests that call the actual API and assert on the expected response, the chances of breaking the API minimizes substantially. With Nancy [this is no problem](https://github.com/NancyFx/Nancy/wiki/Testing-your-application). Also, *test the types*. A simple `Contains()` on the JSON response is not enough. Always deserialize the object if possible.\n\n* You don't know what consumers have done with your API\n\nThe last and most important lesson is that if you have published your API and you have users on it, you have no idea how the client consume it. Even the smallest changes (like going from lowercase to uppsercase keys in this example) can break the consumer. That is the last thing we want. If you have put the API out there, you should respect the consumer and allways think about what effect your changes can have on them.\n","mobiledoc":null,"html":"<blockquote>\n  <p>Disclaimer: The content of this post is not nearly as dramatic as the title will have it, and for a lot of developers it will sound obvious. Based on code I have seen I nonetheless think it can learn a lot of people a valid lesson as well, and hopefully help other avoid the mistakes laziness can lead to.</p>\n</blockquote>\n\n<p>In november last year <a href=\"http://blog.amosti.net/rolling-out-web-services-with-topshelf-chocolatey-and-ansible/\">I wrote about Stratos,</a> a simple web API based on Nancy with the sole purpose of listing out installed Chocolatey packages on a server. With this convenient web-process installed on our servers, the team figured out that it would be nice if it also supported <a href=\"https://github.com/andmos/Stratos/blob/master/doc/Plugin.md\">plugins</a>.</p>\n\n<p>With plugin support we could serve other useful info from the servers via the same API host. So far so good. The story begins when one of my colleagues had to serialize some rather complex objects to JSON. Out of the box Nancy comes with it's own JSON serializer - not using <a href=\"http://www.newtonsoft.com/json\">Json.NET</a> as the rest of the world uses. This serializer did some funny things with his object, so he wanted to use Json.NET. No problem, the Nancy guys offer <a href=\"https://github.com/NancyFx/Nancy.Serialization.JsonNet\">Nancy.Serialization.JsonNet</a> as a NuGet package. <br />\nJust slide the DLL in and Nancy grabs hold of it. God stuff.</p>\n\n<p>After checking in the updated code the CI build failed on the original Stratos API tests. We expect a JSON on the following format when asking for installed Chocolatey packages:</p>\n\n<pre><code>[\n  {\n    \"packageName\": \"chocolatey\",\n    \"version\": {\n      \"version\": {\n        \"major\": 0,\n        \"minor\": 10,\n        \"build\": 6,\n        \"revision\": 1,\n        \"majorRevision\": 0,\n        \"minorRevision\": 1\n      },\n      \"specialVersion\": \"\"\n    }\n  },\n  {\n    \"packageName\": \"chocolatey-core.extension\",\n    \"version\": {\n      \"version\": {\n        \"major\": 1,\n        \"minor\": 1,\n        \"build\": 0,\n        \"revision\": 0,\n        \"majorRevision\": 0,\n        \"minorRevision\": 0\n      },\n      \"specialVersion\": \"\"\n    }\n  }]\n</code></pre>\n\n<p>And what we got with Json.Net was</p>\n\n<pre><code>[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": \"0.10.6\\r\"\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": \"1.1.0\\r\"\n  }\n]\n</code></pre>\n\n<p>This was quite a surprise. The two JSON serializers were acting quite differently.</p>\n\n<p>The original object should be quite simple:</p>\n\n<pre><code>using NuGet;\n\nnamespace Stratos.Model  \n{\n    public class NuGetPackage\n    {\n        public string PackageName { get; set;}\n        public SemanticVersion Version { get; set; }\n    }\n}\n</code></pre>\n\n<p>Right of the bat this should be no problem right? <br />\nTrying to debug this a thought struck me: the only place I need <code>NuGet.Core</code> is in this object, to have a <code>SemanticVersion</code> object. <a href=\"https://github.com/NuGet/NuGet2/blob/2.13/src/Core/SemanticVersion.cs\">If we look at the SemanticVersion class</a> there are a lot of stuff there that could mess up the JSON serializer. Not owning the class ourself also prevent us from using things like DataMember attributes to control what parts of the object should be serialized. Another quite lazy choice here is to use the <code>SemanticVersion</code> object itself as a model property. There is a lot of dead weight on the object we don't need. A better choice is to wrap the parts we need in it's own object:</p>\n\n<pre><code>namespace Stratos.Model  \n{\n    public class PackageVersion\n    {\n        public Version Version { get; set; }\n        public string SpecialVersion { get; set; }\n\n    }\n}\n</code></pre>\n\n<p>And use it in the original object:</p>\n\n<pre><code>namespace Stratos.Model  \n{\n    public class NuGetPackage\n    {\n        public string PackageName { get; set;}\n        public PackageVersion Version { get; set; }\n    }\n}\n</code></pre>\n\n<p>To get rid of the NuGet reference, the <code>SemanticVersion</code> class got duplicated in. It is much better to own that logic ourself.</p>\n\n<p>With this refactoring the JSON response looked a lot better:</p>\n\n<pre><code>[\n  {\n    \"PackageName\": \"chocolatey\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 0,\n        \"Minor\": 10,\n        \"Build\": 6,\n        \"Revision\": 1,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 1\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n  {\n    \"PackageName\": \"chocolatey-core.extension\",\n    \"Version\": {\n      \"Version\": {\n        \"Major\": 1,\n        \"Minor\": 1,\n        \"Build\": 0,\n        \"Revision\": 0,\n        \"MajorRevision\": 0,\n        \"MinorRevision\": 0\n      },\n      \"SpecialVersion\": \"\"\n    }\n  },\n</code></pre>\n\n<p>Cool, let's ship it!</p>\n\n<p>An hour later, we had 25 systems that consumed this API die. What had gone wrong? The observant reader have seen it already:</p>\n\n<p><code>packageName</code> vs. <code>PackageName</code>. Lowercase, uppercase. The Nancy serializer lowercases the keys by default, while Json.Net don't.</p>\n\n<p>Why didn't the testes go red you ask? Let's look at the asserts in the test:</p>\n\n<pre><code>var result = browser.Get(\"/api/chocoPackages\", with =&gt;  \n            {\n                with.HttpRequest();         \n            });\n\n            var resultJson = result.Body.AsString().ToLower();\n\n            Assert.Equal(HttpStatusCode.OK, result.StatusCode);\n            Assert.True(resultJson.Contains(\"major\"));\nAssert.True(resultJson.Contains(\"minor\"));  \n</code></pre>\n\n<p><code>ToLower()</code>. Yeah. The consumer of the API did <em>not</em> use <code>ToLower()</code>, obviously.</p>\n\n<p>So what can we learn from this story?</p>\n\n<ul>\n<li>Don't take on dependencies for a single and simple usecase</li>\n</ul>\n\n<p>I referenced <code>Nuget.Core</code> as a NuGet package to grab hold of the <code>SemanticVersion</code> class for parsing semantic version strings to a <code>Version</code> object. That usecase is so slim that just duplicating <code>SemanticVersion</code> from GitHub to my project is a much better approach - it is just stupid to take on a NuGet dependency.</p>\n\n<ul>\n<li>Allways unit test as far out as possible</li>\n</ul>\n\n<p>By having unit tests that call the actual API and assert on the expected response, the chances of breaking the API minimizes substantially. With Nancy <a href=\"https://github.com/NancyFx/Nancy/wiki/Testing-your-application\">this is no problem</a>. Also, <em>test the types</em>. A simple <code>Contains()</code> on the JSON response is not enough. Always deserialize the object if possible.</p>\n\n<ul>\n<li>You don't know what consumers have done with your API</li>\n</ul>\n\n<p>The last and most important lesson is that if you have published your API and you have users on it, you have no idea how the client consume it. Even the smallest changes (like going from lowercase to uppsercase keys in this example) can break the consumer. That is the last thing we want. If you have put the API out there, you should respect the consumer and allways think about what effect your changes can have on them.</p>","amp":null,"image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-15 18:29:41","created_by":1,"updated_at":"2017-06-15 18:55:42","updated_by":1,"published_at":"2017-06-08 18:29:00","published_by":1}],"users":[{"id":1,"uuid":"bda5739e-5fec-4fef-a602-f0587a031875","name":"Andreas Mosti","slug":"andreas","password":"$2a$10$SUWV5avN.sg3sRsUziM92OohCr67OiAQgRGfLydkCS3iFEm9aKYLy","email":"andreas.mosti@gmail.com","image":"//www.gravatar.com/avatar/99f481c5ef312c5f3d299b5957a22a07?s=250&d=mm&r=x","cover":null,"bio":"Software developer at DIPS ASA. I like shiny things that automate infrastructure.","website":null,"location":"Trondheim, Norway","facebook":null,"twitter":"@amostii","accessibility":null,"status":"active","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_login":"2017-06-15 19:02:01","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 19:02:01","updated_by":1}],"roles":[{"id":1,"uuid":"8209e092-4bb9-4dc4-a0f6-bd7fb23cc0f3","name":"Administrator","description":"Administrators","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":2,"uuid":"ae01657b-5e2e-4292-883d-7ab8a7c7986a","name":"Editor","description":"Editors","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":3,"uuid":"2ac0b974-98c8-437d-b12f-28fbd0946595","name":"Author","description":"Authors","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":4,"uuid":"9a227e34-91d9-4e71-a769-6ae3ff26efc5","name":"Owner","description":"Blog Owner","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1}],"roles_users":[{"id":1,"role_id":4,"user_id":1}],"permissions":[{"id":1,"uuid":"3609fb00-89d9-4bfb-9bca-f298eaebab03","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":2,"uuid":"f6f904b5-05e8-43db-a6dc-056e9c6c952b","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":3,"uuid":"daa7afc6-81b2-4632-bded-6e3e569a7acb","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":4,"uuid":"891c6845-eeba-49a0-abcf-ebe3ac412501","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":5,"uuid":"642a10b4-3e8f-413e-b6ff-e538a185b003","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":6,"uuid":"a77933d4-9ceb-4a05-92cd-84c355d8edec","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":7,"uuid":"1a200086-6b5e-44ad-b348-1e1eda6046dc","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":8,"uuid":"1a95efcd-ee50-4f06-8bf9-698dfab551e6","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":9,"uuid":"938c84ee-f087-4cd4-aefb-26ee5ab6126d","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":10,"uuid":"e413bfc4-50c5-4384-976e-157cf7678ab7","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":11,"uuid":"1a550544-2ffc-4db7-9437-9e7879485f8d","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":12,"uuid":"03de42b6-8223-4f10-a859-94b1553bad09","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":13,"uuid":"054db0e1-9fa8-4e05-8ea4-5f4f44ccb45c","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":14,"uuid":"3289c179-d8b2-46b5-92db-c94be9bf05bf","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":15,"uuid":"cd11086a-fb3e-4d13-b07c-36f2e7fa0fb5","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":16,"uuid":"2f1f7f32-e074-45ff-8ac4-1bfb0f07f068","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":17,"uuid":"203fe97b-dd5a-4d9a-9e48-9a30535d5536","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":18,"uuid":"5f7f2029-4ad0-4322-a3d7-f9237f06f39f","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":19,"uuid":"ff658ee3-a6e9-47ef-90f7-dc2297a768ff","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":20,"uuid":"438472a7-3996-4dbe-98f8-d92b3513d40f","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":21,"uuid":"adaa0897-b53b-4999-b48c-80850ddbb989","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":22,"uuid":"d6c012ec-f36a-4f74-8375-7d2d0922c6f1","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":23,"uuid":"d2397cb9-320a-4b5f-899c-951d1e00884c","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":24,"uuid":"a761dd30-9052-4b19-819f-8115f71d179d","name":"Upload themes","object_type":"theme","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":25,"uuid":"ab71aa24-f21c-4637-9f5a-9f2d6b24f4f1","name":"Download themes","object_type":"theme","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":26,"uuid":"1f3c35cd-2adf-489b-82eb-6deb8c92419b","name":"Delete themes","object_type":"theme","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":27,"uuid":"14b840dd-3f3d-4b14-97be-be9a7642ca3a","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":28,"uuid":"19eca015-7a0b-4d96-a81d-c539de45d7e5","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":29,"uuid":"1111bb3a-17d1-475d-96ad-1b00899680d9","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":30,"uuid":"625d4b94-6371-444e-b324-2bed04a97439","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":31,"uuid":"ecd059da-919a-424b-b1d5-eb0f7e761e1a","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":32,"uuid":"dfc4a66d-de52-4a22-a293-dd672a53f2d6","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":33,"uuid":"559dd253-6f96-4eab-8aa7-7942ce597a90","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":34,"uuid":"0dbcba25-9ba6-4dfc-837c-c9108c3ed6c6","name":"Browse clients","object_type":"client","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":35,"uuid":"d44a9143-d6b0-461e-ba5f-86edb55f1bd0","name":"Read clients","object_type":"client","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":36,"uuid":"706f4340-a4c3-4662-b643-67d39f2849aa","name":"Edit clients","object_type":"client","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":37,"uuid":"48935317-38af-4319-a948-2b8f4d258703","name":"Add clients","object_type":"client","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":38,"uuid":"9dc66c29-bf78-43a0-8d31-2c06010cab24","name":"Delete clients","object_type":"client","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":39,"uuid":"0427fa13-a921-49ef-8c5f-e53ee23b7f1e","name":"Browse subscribers","object_type":"subscriber","action_type":"browse","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":40,"uuid":"5abe8380-1f52-4117-9d6d-0b810507f2c1","name":"Read subscribers","object_type":"subscriber","action_type":"read","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":41,"uuid":"1acb21a0-78ef-4941-821e-55d4ea705599","name":"Edit subscribers","object_type":"subscriber","action_type":"edit","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":42,"uuid":"e2ca950f-c630-41d3-a063-535011874fcc","name":"Add subscribers","object_type":"subscriber","action_type":"add","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":43,"uuid":"0be529bb-9c4d-45d4-8c71-98e9e9a173b0","name":"Delete subscribers","object_type":"subscriber","action_type":"destroy","object_id":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1}],"permissions_users":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3},{"id":4,"role_id":1,"permission_id":4},{"id":5,"role_id":1,"permission_id":5},{"id":6,"role_id":1,"permission_id":6},{"id":7,"role_id":1,"permission_id":7},{"id":8,"role_id":1,"permission_id":8},{"id":9,"role_id":1,"permission_id":9},{"id":10,"role_id":1,"permission_id":10},{"id":11,"role_id":1,"permission_id":11},{"id":12,"role_id":1,"permission_id":12},{"id":13,"role_id":1,"permission_id":13},{"id":14,"role_id":1,"permission_id":14},{"id":15,"role_id":1,"permission_id":15},{"id":16,"role_id":1,"permission_id":16},{"id":17,"role_id":1,"permission_id":17},{"id":18,"role_id":1,"permission_id":18},{"id":19,"role_id":1,"permission_id":19},{"id":20,"role_id":1,"permission_id":20},{"id":21,"role_id":1,"permission_id":21},{"id":22,"role_id":1,"permission_id":22},{"id":23,"role_id":1,"permission_id":23},{"id":24,"role_id":1,"permission_id":24},{"id":25,"role_id":1,"permission_id":25},{"id":26,"role_id":1,"permission_id":26},{"id":27,"role_id":1,"permission_id":27},{"id":28,"role_id":1,"permission_id":28},{"id":29,"role_id":1,"permission_id":29},{"id":30,"role_id":1,"permission_id":30},{"id":31,"role_id":1,"permission_id":31},{"id":32,"role_id":1,"permission_id":32},{"id":33,"role_id":1,"permission_id":33},{"id":34,"role_id":1,"permission_id":34},{"id":35,"role_id":1,"permission_id":35},{"id":36,"role_id":1,"permission_id":36},{"id":37,"role_id":1,"permission_id":37},{"id":38,"role_id":1,"permission_id":38},{"id":39,"role_id":1,"permission_id":39},{"id":40,"role_id":1,"permission_id":40},{"id":41,"role_id":1,"permission_id":41},{"id":42,"role_id":1,"permission_id":42},{"id":43,"role_id":1,"permission_id":43},{"id":44,"role_id":2,"permission_id":8},{"id":45,"role_id":2,"permission_id":9},{"id":46,"role_id":2,"permission_id":10},{"id":47,"role_id":2,"permission_id":11},{"id":48,"role_id":2,"permission_id":12},{"id":49,"role_id":2,"permission_id":13},{"id":50,"role_id":2,"permission_id":14},{"id":51,"role_id":2,"permission_id":16},{"id":52,"role_id":2,"permission_id":17},{"id":53,"role_id":2,"permission_id":18},{"id":54,"role_id":2,"permission_id":19},{"id":55,"role_id":2,"permission_id":20},{"id":56,"role_id":2,"permission_id":21},{"id":57,"role_id":2,"permission_id":27},{"id":58,"role_id":2,"permission_id":28},{"id":59,"role_id":2,"permission_id":29},{"id":60,"role_id":2,"permission_id":30},{"id":61,"role_id":2,"permission_id":31},{"id":62,"role_id":2,"permission_id":32},{"id":63,"role_id":2,"permission_id":33},{"id":64,"role_id":2,"permission_id":34},{"id":65,"role_id":2,"permission_id":35},{"id":66,"role_id":2,"permission_id":36},{"id":67,"role_id":2,"permission_id":37},{"id":68,"role_id":2,"permission_id":38},{"id":69,"role_id":2,"permission_id":42},{"id":70,"role_id":3,"permission_id":8},{"id":71,"role_id":3,"permission_id":9},{"id":72,"role_id":3,"permission_id":11},{"id":73,"role_id":3,"permission_id":13},{"id":74,"role_id":3,"permission_id":14},{"id":75,"role_id":3,"permission_id":16},{"id":76,"role_id":3,"permission_id":17},{"id":77,"role_id":3,"permission_id":18},{"id":78,"role_id":3,"permission_id":20},{"id":79,"role_id":3,"permission_id":27},{"id":80,"role_id":3,"permission_id":28},{"id":81,"role_id":3,"permission_id":33},{"id":82,"role_id":3,"permission_id":34},{"id":83,"role_id":3,"permission_id":35},{"id":84,"role_id":3,"permission_id":36},{"id":85,"role_id":3,"permission_id":37},{"id":86,"role_id":3,"permission_id":38},{"id":87,"role_id":3,"permission_id":42}],"permissions_apps":[],"settings":[{"id":1,"uuid":"fbcea552-3f0e-4ce5-bf8e-89c765cdc263","key":"databaseVersion","value":"009","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":2,"uuid":"e134ed6e-309a-49d6-8900-92c1fe0cc0b7","key":"dbHash","value":"0915185b-175d-4182-8481-01f164edbb9f","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:44","updated_by":1},{"id":3,"uuid":"a25a9ea3-f26a-4e04-8b27-1276a9eac143","key":"nextUpdateCheck","value":"1497636251","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:04:11","updated_by":1},{"id":4,"uuid":"3ac97335-8843-45f6-b82c-60446904af39","key":"displayUpdateNotification","value":"0.11.9","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:04:11","updated_by":1},{"id":5,"uuid":"4fb0885d-1fcd-4791-a0ba-9cb9abaa75e8","key":"seenNotifications","value":"[]","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":6,"uuid":"74a7806e-82aa-48e5-b40c-9008f3f87568","key":"migrations","value":"{}","type":"core","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":7,"uuid":"12d7fbc2-37c9-4994-ac95-24b49e1c40db","key":"title","value":"Dev&Ops","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":8,"uuid":"486db554-2a42-4cfb-a4f1-8a6aaabd399b","key":"description","value":"Personal blog of Andreas Mosti. Thoughts, ideas and crazy hacks from my life as a developer and operations guy.","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":9,"uuid":"e1710282-950c-4aad-a844-6735a802210f","key":"logo","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":10,"uuid":"9e9204d2-dce9-4a71-b882-1dc3760b36d7","key":"cover","value":"/content/images/2017/06/Fil-17-10-2015--10-53-59.jpeg","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":11,"uuid":"f4e1c721-81e2-43d1-b746-2c9ff58fded1","key":"defaultLang","value":"en_US","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":12,"uuid":"fc613b33-0161-4ba9-8649-e8aa44afb08d","key":"postsPerPage","value":"10","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":13,"uuid":"f4bd3202-921e-4709-b34d-ad13390b8828","key":"activeTimezone","value":"Etc/UTC","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":14,"uuid":"a63cdd2c-c136-4d3b-88d8-e7c9bc63d6c3","key":"forceI18n","value":"true","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":15,"uuid":"a7bfc2d3-810e-4523-8696-dc6066682117","key":"permalinks","value":"/:slug/","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":16,"uuid":"a04de4ae-2a4b-4c99-9ee0-f01757e8c4df","key":"amp","value":"true","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":17,"uuid":"f23c183f-8738-43b4-a045-88b18f3e3b0c","key":"ghost_head","value":"<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-79783580-1', 'auto');\n  ga('send', 'pageview');\n\n</script>","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":18,"uuid":"8ebee5bf-a952-48d8-8daf-ea75228955d6","key":"ghost_foot","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":19,"uuid":"c4cb0321-9fc0-4ab7-9bf5-f257c54f4cc6","key":"facebook","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":20,"uuid":"5a54bc28-cf66-4876-88ae-259d12f16b3b","key":"twitter","value":"","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":21,"uuid":"e6a34f47-8a33-433a-90f6-457552ee7710","key":"labs","value":"{}","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":22,"uuid":"7442e48b-2226-4bc4-9ad5-3b420d380932","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"}]","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":23,"uuid":"ae9a1f17-f1be-47ff-85d7-0d4cdfe91efe","key":"slack","value":"[{\"url\":\"\"}]","type":"blog","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":24,"uuid":"548cb79b-c158-47e0-8868-adba2b4288f3","key":"activeApps","value":"[]","type":"app","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1},{"id":25,"uuid":"82bc810f-9ac9-4d43-8e89-c7c65aba0f72","key":"installedApps","value":"[]","type":"app","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:10:49","updated_by":1},{"id":26,"uuid":"b007dbba-5f7e-424e-87e6-5fd4daa6d4ae","key":"isPrivate","value":"false","type":"private","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":27,"uuid":"2bc9432a-b8a7-4bd1-bb4a-7842a6c46ef8","key":"password","value":"","type":"private","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1},{"id":28,"uuid":"b325fff2-0925-45a3-aea0-d7f6c60098ca","key":"activeTheme","value":"casper","type":"theme","created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:44:09","updated_by":1}],"tags":[{"id":1,"uuid":"a25b49cf-e284-4103-a69f-ad6f6ca9aef7","name":"Getting Started","slug":"getting-started","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-15 18:03:43","created_by":1,"updated_at":"2017-06-15 18:03:43","updated_by":1}],"posts_tags":[],"apps":[],"app_settings":[],"app_fields":[],"subscribers":[]}}]}